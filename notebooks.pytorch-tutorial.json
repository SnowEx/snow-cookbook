{"version":2,"kind":"Notebook","sha256":"01f4d8fc291599d82686ca3a8c17e20fc258edbcfb559b44e81a2b4ff6a72d23","slug":"notebooks.pytorch-tutorial","location":"/notebooks/pytorch_tutorial.ipynb","dependencies":[],"frontmatter":{"title":"Neural Networks with PyTorch","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"authors":[{"nameParsed":{"literal":"The SnowPit Community","given":"The SnowPit","family":"Community"},"name":"The SnowPit Community","id":"contributors-myst-generated-uid-0"}],"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"Apache-2.0","url":"https://opensource.org/licenses/Apache-2.0","name":"Apache License 2.0","free":true,"osi":true}},"github":"https://github.com/SnowEx/snow-cookbook","copyright":"2025","affiliations":[{"id":"UAlbany","name":"University at Albany (SUNY)","department":"Atmospheric and Environmental Sciences","url":"https://www.albany.edu/daes"},{"id":"CISL","name":"NSF National Center for Atmospheric Research","department":"Computational and Information Systems Lab","url":"https://www.cisl.ucar.edu"},{"id":"Unidata","name":"NSF Unidata","url":"https://www.unidata.ucar.edu"},{"id":"Argonne","name":"Argonne National Laboratory","department":"Environmental Science Division","url":"https://www.anl.gov/evs"},{"id":"CarbonPlan","name":"CarbonPlan","url":"https://carbonplan.org"},{"id":"NVIDIA","name":"NVIDIA Corporation","url":"https://www.nvidia.com/"}],"numbering":{"title":{"offset":1}},"source_url":"https://github.com/SnowEx/snow-cookbook/blob/main/notebooks/pytorch_tutorial.ipynb","edit_url":"https://github.com/SnowEx/snow-cookbook/edit/main/notebooks/pytorch_tutorial.ipynb","exports":[{"format":"ipynb","filename":"pytorch_tutorial.ipynb","url":"/snow-cookbook/build/pytorch_tutorial-fc53693aa751b3200e31313e9f041488.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This is a notebook designed to introduce users to machine learning using ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Gvs7Su74r0"},{"type":"inlineCode","value":"PyTorch","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"LRyjJkHdZA"},{"type":"text","value":" and station data. The ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"P0MdgiObOF"},{"type":"inlineCode","value":"metloom","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YyGdZMXP2q"},{"type":"text","value":" package developed by M3Works is needed to run this script.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"yteUwbEXWt"}],"key":"dcKNKWrBLv"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"This notebook is adapted from a SnowEx Hackweek tutorial developed by Ibrahim Alabi. The full tutorial may be found here: ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"i0BPwvEQI7"},{"type":"link","url":"https://snowex-2024.hackweek.io/tutorials/NN_with_Pytorch/intro.html","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"https://​snowex​-2024​.hackweek​.io​/tutorials​/NN​_with​_Pytorch​/intro​.html","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"fPk65ytpD7"}],"urlSource":"https://snowex-2024.hackweek.io/tutorials/NN_with_Pytorch/intro.html","key":"mo2uAI3pm0"}],"key":"VJKSzCwt5E"}],"key":"wE0VMDIMNd"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What is Machine Learning?","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QKQ3OUeo5G"}],"identifier":"what-is-machine-learning","label":"What is Machine Learning?","html_id":"what-is-machine-learning","implicit":true,"key":"M4XKL2Q432"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Machine learning (ML) is a field of artificial intelligence (AI) that focuses on developing algorithms or computer models using data. The goal is to use these “trained” compuer models to make decisions. Unlike traditional programming, where we write explicit rules for every situation, ML models learn patterns from data to perform tasks.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"XJP9CDzPTv"}],"key":"ybVtJwVsw7"}],"key":"XkgxZfNurf"},{"type":"block","kind":"notebook-content","children":[{"type":"admonition","kind":"warning","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Important","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DJGZduVfjH"}],"key":"D9yyDxlX1f"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Machine learning is useful when the function (","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"tPQtsQAIoz"},{"type":"inlineMath","value":"f","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span></span></span></span>","key":"EQ6LtT0FFV"},{"type":"text","value":") cannot be explicitly programmed, or when the relationship between the feature(s) and outcome is unknown.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"nte2g5JYRm"}],"key":"VIZ1ljGKBo"}],"key":"rUhtnJKRNZ"}],"key":"dSV7RhooDY"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Data Download and Cleaning","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"F9CaWmCZ2u"}],"identifier":"data-download-and-cleaning","label":"Data Download and Cleaning","html_id":"data-download-and-cleaning","implicit":true,"key":"XOga7zzNVV"}],"key":"RZrXMEfU5G"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"To begin with this tutorial, we will download SNOTEL data using the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k2bHWrK8Ax"},{"type":"inlineCode","value":"metloom","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SvdJUmi1Jq"},{"type":"text","value":" package. Users that don’t have it installed can run the cell below.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AWckcK2YuQ"}],"key":"UTjtOnofW3"}],"key":"eUboMpAFkl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"!pip install -q metloom torch torchvision torchaudio","key":"b7ZuOLzjYh"},{"type":"output","id":"p_AMulO35leVVrWCM6BHG","data":[],"key":"d3rI814S6G"}],"key":"k6z0xpVr8E"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"For a more detailed explanation on ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RfwfmagWkw"},{"type":"inlineCode","value":"metloom","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pieYl4HpfV"},{"type":"text","value":", check out the tutorial on SNOTEL data access.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OOQahFk6S6"}],"key":"AhqWEM3e4e"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"In this notebook, we will grab the following variables: SWE (","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"v129thpiVD"},{"type":"inlineCode","value":"SWE","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"o42WfqNFbv"},{"type":"text","value":"), average temperature (","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"lzAvol2yjw"},{"type":"inlineCode","value":"TEMPAVG","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"sETmBYbQee"},{"type":"text","value":"), snow depth (","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"FtaFuJCdmd"},{"type":"inlineCode","value":"SNOWDEPTH","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"p4BrFrkevP"},{"type":"text","value":"), and precipitation (","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"XDLe4st53u"},{"type":"inlineCode","value":"PRECIPITATION","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Kpc4M7fdio"},{"type":"text","value":"). These variables will be obtained from the SNOTEL station at Green Lake, WA.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"A6XbFhg8ut"}],"key":"yr7fSvUNLp"}],"key":"QKdjB9XSnX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# Define variables of interest\nALLOWED_VARIABLES = [\n    SnotelPointData.ALLOWED_VARIABLES.SWE,\n    SnotelPointData.ALLOWED_VARIABLES.TEMPAVG,\n    SnotelPointData.ALLOWED_VARIABLES.SNOWDEPTH,\n    SnotelPointData.ALLOWED_VARIABLES.PRECIPITATION,\n]\n\n# Define SNOTEL station\nsnotel_point = SnotelPointData(station_id=\"502:WA:SNTL\", name=\"Green Lake\")\n\n# Get daily SNOTEL data from Green Lake, WA\ndata = snotel_point.get_daily_data(\n    start_date=datetime(*(2010, 1, 1)),\n    end_date=datetime(*(2023, 1, 1)),\n    variables=ALLOWED_VARIABLES\n)\n\ndata.head()","key":"BNAeezmFV5"},{"type":"output","id":"h2WYyfoDz17QROYBcdzNL","data":[{"output_type":"execute_result","execution_count":2,"metadata":{},"data":{"text/plain":{"content":"                                                                 geometry  \\\ndatetime                  site                                              \n2010-01-01 08:00:00+00:00 502:WA:SNTL  POINT Z (-121.17093 46.54741 5930)   \n2010-01-02 08:00:00+00:00 502:WA:SNTL  POINT Z (-121.17093 46.54741 5930)   \n2010-01-03 08:00:00+00:00 502:WA:SNTL  POINT Z (-121.17093 46.54741 5930)   \n2010-01-04 08:00:00+00:00 502:WA:SNTL  POINT Z (-121.17093 46.54741 5930)   \n2010-01-05 08:00:00+00:00 502:WA:SNTL  POINT Z (-121.17093 46.54741 5930)   \n\n                                        SWE SWE_units  AVG AIR TEMP  \\\ndatetime                  site                                        \n2010-01-01 08:00:00+00:00 502:WA:SNTL   9.2        in         32.18   \n2010-01-02 08:00:00+00:00 502:WA:SNTL   9.7        in         29.30   \n2010-01-03 08:00:00+00:00 502:WA:SNTL  10.0        in         28.94   \n2010-01-04 08:00:00+00:00 502:WA:SNTL  10.1        in         33.80   \n2010-01-05 08:00:00+00:00 502:WA:SNTL  10.8        in         36.86   \n\n                                      AVG AIR TEMP_units  SNOWDEPTH  \\\ndatetime                  site                                        \n2010-01-01 08:00:00+00:00 502:WA:SNTL               degF       34.0   \n2010-01-02 08:00:00+00:00 502:WA:SNTL               degF       37.0   \n2010-01-03 08:00:00+00:00 502:WA:SNTL               degF       38.0   \n2010-01-04 08:00:00+00:00 502:WA:SNTL               degF       38.0   \n2010-01-05 08:00:00+00:00 502:WA:SNTL               degF       38.0   \n\n                                      SNOWDEPTH_units  PRECIPITATION  \\\ndatetime                  site                                         \n2010-01-01 08:00:00+00:00 502:WA:SNTL              in            0.5   \n2010-01-02 08:00:00+00:00 502:WA:SNTL              in            0.3   \n2010-01-03 08:00:00+00:00 502:WA:SNTL              in            0.1   \n2010-01-04 08:00:00+00:00 502:WA:SNTL              in            0.7   \n2010-01-05 08:00:00+00:00 502:WA:SNTL              in            0.1   \n\n                                      PRECIPITATION_units datasource  \ndatetime                  site                                        \n2010-01-01 08:00:00+00:00 502:WA:SNTL                  in       NRCS  \n2010-01-02 08:00:00+00:00 502:WA:SNTL                  in       NRCS  \n2010-01-03 08:00:00+00:00 502:WA:SNTL                  in       NRCS  \n2010-01-04 08:00:00+00:00 502:WA:SNTL                  in       NRCS  \n2010-01-05 08:00:00+00:00 502:WA:SNTL                  in       NRCS  ","content_type":"text/plain"},"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>geometry</th>\n      <th>SWE</th>\n      <th>SWE_units</th>\n      <th>AVG AIR TEMP</th>\n      <th>AVG AIR TEMP_units</th>\n      <th>SNOWDEPTH</th>\n      <th>SNOWDEPTH_units</th>\n      <th>PRECIPITATION</th>\n      <th>PRECIPITATION_units</th>\n      <th>datasource</th>\n    </tr>\n    <tr>\n      <th>datetime</th>\n      <th>site</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010-01-01 08:00:00+00:00</th>\n      <th>502:WA:SNTL</th>\n      <td>POINT Z (-121.17093 46.54741 5930)</td>\n      <td>9.2</td>\n      <td>in</td>\n      <td>32.18</td>\n      <td>degF</td>\n      <td>34.0</td>\n      <td>in</td>\n      <td>0.5</td>\n      <td>in</td>\n      <td>NRCS</td>\n    </tr>\n    <tr>\n      <th>2010-01-02 08:00:00+00:00</th>\n      <th>502:WA:SNTL</th>\n      <td>POINT Z (-121.17093 46.54741 5930)</td>\n      <td>9.7</td>\n      <td>in</td>\n      <td>29.30</td>\n      <td>degF</td>\n      <td>37.0</td>\n      <td>in</td>\n      <td>0.3</td>\n      <td>in</td>\n      <td>NRCS</td>\n    </tr>\n    <tr>\n      <th>2010-01-03 08:00:00+00:00</th>\n      <th>502:WA:SNTL</th>\n      <td>POINT Z (-121.17093 46.54741 5930)</td>\n      <td>10.0</td>\n      <td>in</td>\n      <td>28.94</td>\n      <td>degF</td>\n      <td>38.0</td>\n      <td>in</td>\n      <td>0.1</td>\n      <td>in</td>\n      <td>NRCS</td>\n    </tr>\n    <tr>\n      <th>2010-01-04 08:00:00+00:00</th>\n      <th>502:WA:SNTL</th>\n      <td>POINT Z (-121.17093 46.54741 5930)</td>\n      <td>10.1</td>\n      <td>in</td>\n      <td>33.80</td>\n      <td>degF</td>\n      <td>38.0</td>\n      <td>in</td>\n      <td>0.7</td>\n      <td>in</td>\n      <td>NRCS</td>\n    </tr>\n    <tr>\n      <th>2010-01-05 08:00:00+00:00</th>\n      <th>502:WA:SNTL</th>\n      <td>POINT Z (-121.17093 46.54741 5930)</td>\n      <td>10.8</td>\n      <td>in</td>\n      <td>36.86</td>\n      <td>degF</td>\n      <td>38.0</td>\n      <td>in</td>\n      <td>0.1</td>\n      <td>in</td>\n      <td>NRCS</td>\n    </tr>\n  </tbody>\n</table>\n</div>","content_type":"text/html"}}}],"key":"eiJyVwrkoW"}],"key":"dnEwADmyTG"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s take a look at the data that we just collected.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TLIJAlLuqU"}],"key":"HfywXIrNrn"}],"key":"W9amJzMO8X"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Reset the index of the DataFrame for plotting purposes\nfor_plotting=data.reset_index()\n\n# Define the units for each variable\nunits={\n    \"SWE\": \"in\",\n    \"SNOWDEPTH\": \"in\",\n    \"AVG AIR TEMP\": \"degF\",\n    \"PRECIPITATION\": \"in\"\n}\n\n# List the variables for plotting\nvariables_to_plot = [\n    \"SWE\", \"SNOWDEPTH\", \"AVG AIR TEMP\", \"PRECIPITATION\"\n]\n\n# Make the plot\nplt.figure(figsize=(12, 8))\n\nfor variable in variables_to_plot:\n    plt.subplot(2, 2, variables_to_plot.index(variable) + 1)\n    plt.plot(for_plotting[\"datetime\"], for_plotting[variable], label=variable)\n    plt.ylabel(f\"{variable} ({units[variable]})\", fontsize=14)\n    plt.xlabel(\"Date\", fontsize=14)\n\nplt.tight_layout()\nplt.show()","key":"bBXDs6ilZi"},{"type":"output","id":"OqDR0HJDrXFp--PbAKAEq","data":[{"output_type":"error","traceback":"\u001b[31m---------------------------------------------------------------------------\u001b[39m\n\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Reset the index of the DataFrame for plotting purposes\u001b[39;00m\n\u001b[32m      6\u001b[39m for_plotting=data.reset_index()\n\n\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'","ename":"ModuleNotFoundError","evalue":"No module named 'matplotlib'"}],"key":"wQ4mn0nJvq"}],"key":"mc4OvXvDmN"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We will now process this data so it’s easier to work with. First, we convert from imperial to metric units for easier interpretation. We then set the dates as the index, so that we can derive weekly rolling averages of precipitation and tempoerature.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FJ8YWtk9kJ"}],"key":"txuOKTZiUC"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"The dataframe is then cleaned so that only snow depth, SWE, weekly temperature, and weekly precipitation are left. The dataframe is then filtered  for any NaN values, and for any zero/unrealistic snow depth and SWE values. Finally, we derive snow density from the SWE and depth data.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"P9trkMgkNk"}],"key":"TJr0hrFLjQ"}],"key":"EGjEHYvKP4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"clean_df=(\n    for_plotting\n    .assign(\n        swe=lambda x: x.SWE.map(lambda y: y*2.54 if y is not None else None),\n        snowdepth=lambda x: x.SNOWDEPTH.map(lambda y: y*2.54 if y is not None else None),\n        precipitation=lambda x: x.PRECIPITATION.map(lambda y: y*2.54 if y is not None else None),\n        tempavg=lambda x: x['AVG AIR TEMP'].map(lambda y: (y-32)*5/9 if y is not None else None)\n    )\n    .set_index('datetime')\n    .assign(\n        precip_7_days_avg=lambda x: x.precipitation.shift().rolling(window=\"7D\", min_periods=7).mean(),\n        tempavg_7_days_avg=lambda x: x.tempavg.shift().rolling(window=\"7D\", min_periods=7).mean(),\n    )\n    .filter([\"datetime\", \"swe\", \"snowdepth\", \"tempavg_7_days_avg\", \"precip_7_days_avg\"])\n    .dropna()\n    .query(\n        \"snowdepth != 0 and swe != 0 and \"\n        \"snowdepth > 5 and swe > 3\"\n    )\n    .assign(snowdensity=lambda x: x.swe / x.snowdepth)\n)","key":"UoHLHGCg0L"},{"type":"output","id":"5es6a-tqUGyHQ2ImNq9Nb","data":[],"key":"u07EwsEIvU"}],"key":"eeEtpuqL97"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"clean_df","key":"QBA6XqPYwQ"},{"type":"output","id":"-KG4aIZ3gnqLCQZO-YHdd","data":[],"key":"zcn8VY6dq9"}],"key":"TBSJePR0X8"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Building and Training a Neural Network","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lBHt9b0TNK"}],"identifier":"building-and-training-a-neural-network","label":"Building and Training a Neural Network","html_id":"building-and-training-a-neural-network","implicit":true,"key":"DD1NRIdcEd"}],"key":"bRZ3QeB3sY"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now that we have SNOTEL data configured in a desirable format, we can build a simple neural network using PyTorch.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fJyA32DFCo"}],"key":"ic6vTIxQID"}],"key":"CTLLg5xBRS"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Basic analysis libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# PyTorch libraries\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\n# Find the best available computing resource\navailable_device = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\n\nprint(f\"Available device: {available_device}\")","key":"yHLNXAeFXg"},{"type":"output","id":"GWa_yijN8838sGGl_RqH_","data":[],"key":"K7GBqwPwTf"}],"key":"ViiHDyQpnW"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The above cell identifies the most appropriate computing resource, based on what is available. If NVIDIA GPU or Apple’s Metal Performance Shaders are available, then they are prioritized. Otherwise, the code defaults to CPU. The former options offer faster GPU support, though the CPU option is universally available.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M30vNlbdPo"}],"key":"AnCqrUhW5l"}],"key":"PBIvybcGlZ"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Data Splitting","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JEsggunPeA"}],"identifier":"data-splitting","label":"Data Splitting","html_id":"data-splitting","implicit":true,"key":"HIkY0A5s5E"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"With our current SNOTEL data, we are going to split it into training, validation, and testing sets. A typical split is 70% training data, 15% validation data, and 15% testing data.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"jkGecIlLNQ"}],"key":"JI9ylbdy1l"}],"key":"BPkDHrQkwz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Non-snow density data\nfeatures = clean_df.drop('snowdensity', axis=1).values\n\n# Snow density data only\ntargets = clean_df['snowdensity'].values\n\n# Split the data into training and temporary sets (70% training, 30% temporary)\nfeatures_train,features_temp,targets_train,targets_temp = train_test_split(features, targets, \n                                                                           test_size=0.3, random_state=0)\n\n# Further split the temp set into validation and test sets (15% each)\nfeatures_val,features_test,targets_val,targets_test = train_test_split(features_temp, targets_temp, \n                                                                       test_size=0.5, random_state=0)","key":"EkwTP0eEJp"},{"type":"output","id":"p6h0o73QsmaWdCX2mo_KX","data":[],"key":"mjy67EZ81G"}],"key":"N8HwtK1c4c"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Here is a breakdown of the above cell:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WDYA2QRqD6"}],"key":"c5WNr1RyXo"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"We identify our “features”, or datasets that will be used to predict snow density. These include SWE, snow depth, temperature, and precipitation.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"bRGs3VbBFS"}],"key":"u1MGyci6xx"}],"key":"SRRMhztFgS"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"We identify our “target”, which is snow density in this example.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"OkuzGom5L8"}],"key":"PuRf9zn4t8"}],"key":"ZN1ofAykk1"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The data is split into model training data (","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"nZHvTt8iEo"},{"type":"inlineCode","value":"features_train","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"iJkAAtiEcI"},{"type":"text","value":", ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"uhdIrz8J1J"},{"type":"inlineCode","value":"targets_train","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"UC4osIUeqH"},{"type":"text","value":") and validation/testing data (","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"VlL6LvGuyt"},{"type":"inlineCode","value":"features_temp","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"dByRslIwPT"},{"type":"text","value":", ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"Hy3ckZGbyO"},{"type":"inlineCode","value":"targets_temp","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"bGg4kOZUxo"},{"type":"text","value":").","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"CnXbrTGkmi"}],"key":"AO3ebyadOm"}],"key":"zH2E9sGfyO"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The temporary data sets are further split in half to validation data (","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"q0r7oLdKho"},{"type":"inlineCode","value":"features_val","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"rea34vGrvd"},{"type":"text","value":", ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ozRQghEaXC"},{"type":"inlineCode","value":"targets_val","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"KHY4Ey6EHh"},{"type":"text","value":") and test data (","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"E39d5dzPTv"},{"type":"inlineCode","value":"features_test","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"DYd04SVhDq"},{"type":"text","value":", ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ZDrwrbz6Nj"},{"type":"inlineCode","value":"targets_test","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"aErC0qXSa6"},{"type":"text","value":").","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"nH7zP7U9An"}],"key":"oGCtS6wr3T"}],"key":"UuuIhIoZFZ"}],"key":"ESJqRq6RbR"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"In both splitting operations, we also set ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"HQXTpJ8Prb"},{"type":"inlineCode","value":"random_state=0","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"ulPj2Jzm6F"},{"type":"text","value":". This means that the same training/validation/testing split occurs every time the code is run, to ensure reproducibility.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"qm8KFhornM"}],"key":"MLWS8EyYId"}],"key":"yKuvEt38zE"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Data Scaling","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Q4xjxJZuvB"}],"identifier":"data-scaling","label":"Data Scaling","html_id":"data-scaling","implicit":true,"key":"Ki6dGjvul1"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Now that we’ve split the data, we can apply scaling. The scaler should be fit on the training data and then used to transform the training, validation, and test sets.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"WbLokOzDQ6"}],"key":"m0FDrtndus"}],"key":"QMOZOQKAfU"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Generate scaler\nscaler = StandardScaler()\n\n# Fit scaler to training data\nscaler.fit(features_train)\n\n# Transform the training, validation, and test sets\nfeatures_train = scaler.transform(features_train)\nfeatures_val = scaler.transform(features_val)\nfeatures_test = scaler.transform(features_test)","key":"bHVRaoLNsh"},{"type":"output","id":"tSAMxGSOpecum_EYzaegr","data":[],"key":"BTbrieCq21"}],"key":"OhkZi9UKE2"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Create Dataset Classes","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I6LKe0IPWT"}],"identifier":"create-dataset-classes","label":"Create Dataset Classes","html_id":"create-dataset-classes","implicit":true,"key":"SlU4oTOqsG"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Next, we define custom ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"OABnexfb6L"},{"type":"inlineCode","value":"Dataset","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"qjotyEV2Wu"},{"type":"text","value":" classes for each of the three sets: training, validation, and testing.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"KWMvwIQs4u"}],"key":"dGYfj1vLSO"}],"key":"hZZUoBWB4E"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Create class for available data\nclass SNOTELDataset(Dataset):\n    def __init__(self, data, targets):\n        self.data = torch.tensor(data, dtype=torch.float32)\n        self.targets = torch.tensor(targets, dtype=torch.float32).view(-1, 1)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        target = self.targets[idx]\n        return sample, target\n\n# Create instances of the custom datasets for training, validation, and testing sets\ntrain_dataset = SNOTELDataset(data=features_train, targets=targets_train)\nval_dataset = SNOTELDataset(data=features_val, targets=targets_val)\ntest_dataset = SNOTELDataset(data=features_test, targets=targets_test)","key":"K7feqJYiww"},{"type":"output","id":"F22FaUEmczF8OVTunYQFI","data":[],"key":"kPFGKUgqD1"}],"key":"ykJWVgZ0c9"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, we use DataLoader to manage our data in mini-batches during training, validation, and testing.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JAEo9S7wTc"}],"key":"yvbtIX2Ufq"}],"key":"Gv1QE7nSH2"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Create DataLoaders for training, validation, and testing sets\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","key":"zGUcacwY8E"},{"type":"output","id":"YfxzNActgFVImybahB9IG","data":[],"key":"YSindBe0tk"}],"key":"ui1n8EQS5c"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Define the neural network","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QTJjQF190w"}],"identifier":"define-the-neural-network","label":"Define the neural network","html_id":"define-the-neural-network","implicit":true,"key":"pZ3SSkzbLu"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"To set up our model, we begin by generating a simple feedforward neural network using ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"UcGwXIWCVW"},{"type":"inlineCode","value":"torch.nn.Module","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"XjlMwh2Nlz"},{"type":"text","value":".","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"HPAS9gylCB"}],"key":"XklRWlF8eB"}],"key":"W0u2qcehDk"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Define new class for neural network\nclass SNOTELNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SNOTELNN, self).__init__() # super class to inherit from nn.Module\n        # Define the layers\n        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer 1\n        self.relu = nn.ReLU()  # ReLU activation function\n        self.fc2 = nn.Linear(hidden_size, output_size)  # Fully connected layer 2\n    \n    def forward(self, x): # x is the batch of input\n        # Define the forward pass\n        out = self.fc1(x)  # Pass input through first layer\n        out = self.relu(out)  # Apply ReLU activation\n        out = self.fc2(out)  # Pass through second layer to get output\n        return out\n\n# Instantiate the model and move it to the device (GPU or CPU)\nmodel = SNOTELNN(input_size=features_train.shape[1], hidden_size=128, output_size=1).to(available_device)","key":"coY2FGCT4k"},{"type":"output","id":"vIIhmXZnWk7Cb-SOQI1xz","data":[],"key":"Ksvy1xQQ3u"}],"key":"XwFVFn4zog"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q4o5ngEnGA"},{"type":"inlineCode","value":"forward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JPpuMvVBFP"},{"type":"text","value":" method defines how the input data flows through the network layers. It specifies the sequence of operations that the data undergoes as it moves from the input layer to the output layer. This method is automatically called when you pass data through the model (e.g., ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kBY8C0QVXP"},{"type":"inlineCode","value":"outputs = model(inputs)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GMS5CYmgI7"},{"type":"text","value":").","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IcJ08bQxGb"}],"key":"Ac3kpKUp7T"}],"key":"RqwNaFjgqe"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"For any ML application, we need to define a loss function and an optimizer. Here, we’ll use Mean Squared Error Loss since we’re dealing with a regression problem. We’ll use the Adam optimizer, which is a good default choice due to its adaptive learning rates.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jFG8s9XRie"}],"key":"XjwbFiiHBD"}],"key":"FhyjrPAr1X"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Loss function\ncriterion = nn.MSELoss()\n\n# Optimizer function\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)","key":"HutH7rwOY8"},{"type":"output","id":"4KW09-AZLM_WGkykEG2wd","data":[],"key":"DEx1se9jZX"}],"key":"RXZfV2fdx6"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Training the Neural Network","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AkVyBUup17"}],"identifier":"training-the-neural-network","label":"Training the Neural Network","html_id":"training-the-neural-network","implicit":true,"key":"pVuDxutmtf"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"We now write the training loop, which includes zeroing the gradients, performing the forward pass, computing the loss, backpropagating, and updating the model parameters. We will also validate the model on the validation set after each epoch.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"U50Osq8Mse"}],"key":"dbw2GNSXMN"}],"key":"ayLi8cMob3"},{"type":"block","kind":"notebook-content","children":[{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"BNHTBfNu24"}],"key":"L9qyzPuIMA"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"An ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"pX54rgLwIs"},{"type":"strong","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Epoch","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"A5vHZFJ5hW"}],"key":"igOh063ZGh"},{"type":"text","value":" refers to one complete pass through the entire training dataset. During each epoch, the model sees every example in the dataset once.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"fPdvCWgFVL"}],"key":"vKog9oaQdb"}],"key":"WNyTdonwtg"}],"key":"jrEBIEBNQw"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"num_epochs = 5\n\n# Lists to store the training and validation losses\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    train_loss = 0.0  # Initialize cumulative training loss\n    \n    for inputs, labels in train_loader:\n        # Move data to the appropriate device\n        inputs, labels = inputs.to(available_device), labels.to(available_device)\n        \n        # Zero the gradients from the previous iteration\n        optimizer.zero_grad()\n        \n        # Perform forward pass\n        outputs = model(inputs)\n        \n        # Compute the loss\n        loss = criterion(outputs, labels)\n        \n        # Perform backward pass (compute gradients)\n        loss.backward()\n        \n        # Update the model parameters\n        optimizer.step()\n        \n        # Accumulate training loss\n        train_loss += loss.item()\n    \n    # Average training loss\n    train_loss /= len(train_loader)\n    train_losses.append(train_loss)  # Store the training loss for this epoch\n    \n    # Validation phase\n    model.eval()  # Set model to evaluation mode\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(available_device), labels.to(available_device)  # Move to device\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n    \n    # Average validation loss\n    val_loss /= len(val_loader)\n    val_losses.append(val_loss)  # Store the validation loss for this epoch\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')","key":"lJ19KV6fhp"},{"type":"output","id":"iHNrvPw2qocTzqBEFoXbn","data":[],"key":"L3sm9SmNI6"}],"key":"pWRSaQZXQz"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s check out the loss from both the training data and the validation data.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tQ4Zv3obNc"}],"key":"unm5VJc1Ja"}],"key":"Wtf4vtqecy"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Plotting the training and validation losses\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\nplt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Over Epochs')\nplt.legend()\nplt.show()","key":"HY71qLx7CF"},{"type":"output","id":"ReZjpwVgnaxnKXZy4v31Q","data":[],"key":"uoV05hQHZe"}],"key":"TfXuP66u4N"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Testing the Model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ND4aIktpDh"}],"identifier":"testing-the-model","label":"Testing the Model","html_id":"testing-the-model","implicit":true,"key":"SgDlS46NvN"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"We’ve done a lot of work to prepare the machine learning model for our applications. Now, it’s finally time to test it against our observations.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Prun3DYj2U"}],"key":"mP9Mj6rFl5"}],"key":"gi8neQKlYK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Evaluate the model on the test set and collect predictions\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0  # Initialize cumulative test loss\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():  # Disable gradient computation for inference\n    for inputs, labels in test_loader:\n        # Move data to the appropriate device\n        inputs, labels = inputs.to(available_device), labels.to(available_device)\n        \n        # Perform forward pass\n        outputs = model(inputs)\n        \n        # Compute the loss\n        loss = criterion(outputs, labels)\n        \n        # Accumulate test loss\n        test_loss += loss.item()\n        \n        # Store the predictions and the corresponding labels\n        all_preds.extend(outputs.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Calculate the average test loss\ntest_loss /= len(test_loader)\nprint(f'Test Loss: {test_loss:.4f}')\n\n# Convert lists to numpy arrays for plotting\nall_preds = np.array(all_preds)\nall_labels = np.array(all_labels)\n\n# Plot observed vs predicted\nplt.figure(figsize=(8, 8))\nplt.scatter(all_labels, all_preds, alpha=0.7)\nplt.xlabel('Observed (Actual) Values')\nplt.ylabel('Predicted Values')\nplt.title('Observed vs. Predicted Values')\nplt.grid(True)\nplt.show()","key":"vF5E178PHW"},{"type":"output","id":"uGXEfJqH289jP917HmKa_","data":[],"key":"pDaA70tgCx"}],"key":"kpZ8MZnoGb"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Save the Model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yvxGZembaX"}],"identifier":"save-the-model","label":"Save the Model","html_id":"save-the-model","implicit":true,"key":"xs82nTFcKT"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"We now have some pretty good results for predicting snow density. Now it is essential to save our trained model, as doing so allows you to reuse the model for predictions, further training, or sharing with others without having to retrain it from scratch. In PyTorch, saving and loading models is straightforward and can be done using the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"I8tRKVQCZz"},{"type":"inlineCode","value":"torch.save","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"AnxM72ZiQs"},{"type":"text","value":" and ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"pl7Ntd33m2"},{"type":"inlineCode","value":"torch.load","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"YPqUM2e84g"},{"type":"text","value":" functions.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"SJBnuOAnTH"}],"key":"YOIP1jWREY"}],"key":"qfxCzA4cxE"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Save the model's state dictionary\ntorch.save(model.state_dict(), 'snotel_nn_model.pth')\n\n\n# Initialize the model architecture\nmodel = SNOTELNN(input_size=features_train.shape[1], hidden_size=128, output_size=1)\n\n# Load the model's state dictionary\nmodel.load_state_dict(torch.load('snotel_nn_model.pth', weights_only=True))\n\n# Set the model to evaluation mode before inference\nmodel.eval()","key":"dZSbJ6qSBo"},{"type":"output","id":"Mr3pfNnRh6uTyUh_vWVeT","data":[],"key":"U4E76pUOaA"}],"key":"rlzlmZ6l6A"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Further Information","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cxuhQWwWSA"}],"identifier":"further-information","label":"Further Information","html_id":"further-information","implicit":true,"key":"jC78MVlmww"},{"type":"heading","depth":4,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Hyperparameter Tuning","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"scAA3YhjPH"}],"identifier":"hyperparameter-tuning","label":"Hyperparameter Tuning","html_id":"hyperparameter-tuning","implicit":true,"key":"FoLcq5VDvn"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Hyperparameter tuning is a critical step in building machine learning models. Unlike model parameters (like weights and biases), which are learned from the data during training, hyperparameters are the settings you choose before the training process begins. These include:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"bIkrehp0Mj"}],"key":"xbN06MQBeW"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Learning Rate","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Ufp7AaTJqq"}],"key":"Nz8DrPTaFe"},{"type":"text","value":": Controls how much to adjust the model’s weights with respect to the loss gradient.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"TSxBGSksW1"}],"key":"WlfuGCBhj0"}],"key":"j2IQ9uCbcd"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Batch Size","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"fryzoiJfA9"}],"key":"WFyMtZ0wmZ"},{"type":"text","value":": Determines the number of training examples utilized in one iteration.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"BHmG7JpeKO"}],"key":"iUZSEVz7Yi"}],"key":"wNKhoO9xHr"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Number of Hidden Layers and Neurons","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"c1iUPZ0NL5"}],"key":"op79etFbwQ"},{"type":"text","value":": Specifies the architecture of the neural network.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"LIe7jJLWQ4"}],"key":"qPv6eIn4Fw"}],"key":"Z3LV9HJZTM"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Optimizer","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"VpfnPh4uuc"}],"key":"QdjJCaITiK"},{"type":"text","value":": The algorithm used to update model weights based on the computed gradients (e.g., Adam, SGD).","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"d4K4rB2q3P"}],"key":"eBn550o7On"}],"key":"JqhTIH8yPm"}],"key":"Mz8qkm8Hev"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Tuning these hyperparameters can significantly affect the performance of your model. However, finding the optimal set of hyperparameters can be a challenging and time-consuming process, often requiring experimentation.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"kA5uNGvh7l"}],"key":"yXtJN2NCRr"},{"type":"heading","depth":4,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Manual vs. Automated Tuning","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"vnP47A2aID"}],"identifier":"manual-vs-automated-tuning","label":"Manual vs. Automated Tuning","html_id":"manual-vs-automated-tuning","implicit":true,"key":"eP3ciMkhHL"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":13,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Manual Tuning","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"EReVs0vJmx"}],"key":"i1ZM4txbHb"},{"type":"text","value":": Involves adjusting hyperparameters based on intuition, experience, or trial and error. While straightforward, this approach can be inefficient and might not always yield the best results.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"XwJe8D4iJ1"}],"key":"SScLy4684G"}],"key":"MNXNzqpeVx"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"strong","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Automated Tuning","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"OFpQfhl13a"}],"key":"vzUoWJ5n8r"},{"type":"text","value":": Tools like Optuna can help automate the search for the best hyperparameters. These tools explore the hyperparameter space more systematically and can save a lot of time compared to manual tuning.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"seTcCVlGEo"}],"key":"dbvHnF6VhX"}],"key":"ImsIbZ30Lq"}],"key":"ZfvTByGwbc"}],"key":"fKR1lCiQMI"}],"key":"hk84QzTZka"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Terrestrial Laser Scanning","url":"/notebooks/tls-data-access","group":"Observations"},"next":{"title":"Snow Modeling","url":"/notebooks/snowmodeling-tutorial-pt1","group":"Analysis and Machine Learning"}}},"domain":"http://localhost:3000"}