{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPR and Lidar\n",
    "\n",
    "## Author: Randall Bonnell\n",
    "\n",
    "## Outline:\n",
    "1. GPR Methods for the Retrieval of Snow Depth and SWE\n",
    "2. Lidar Methods for Snow Depth Retrieval and SWE Estimation\n",
    "3. Leveraging Coincident GPR and Lidar Data Sets to Derive Snow Density\n",
    "4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska\n",
    "5. Discussion: Improving Density Estimation\n",
    "6. GPR SnowEx Analysis-Ready Datasets\n",
    "7. References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Title Card](./images/gpr_lidar/HackweekGraphicalAbstract_GPR_Lidar_v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPR Methods for the Retrieval of Snow Depth and SWE\n",
    "\n",
    "### What is GPR?\n",
    "* GPR transmits a radar signal into the snowpack, which then reflects off objects/interfaces with contrasting dielectric permittivity. The GPR records the **amplitude** and **two-way travel time (twt)** of the reflections.\n",
    "* **Dielectric permittivity** refers to the dielectric properties of the snowpack that define how EM energy trasmits through the medium.\n",
    "* Usually, we are interested in the snow-ground interface, and we measure the snowpack thickness (depth) in two-way travel time (in nanoseconds).\n",
    "* Most analysis-ready GPR products have twt, snow depth, and SWE variables. Some have also been updated to include derived snow density\n",
    "\n",
    "For this notebook, we will start from the two-way travel time data to derive our snow properties of interest, and compare it to airborne lidar data. The `snowexsql` package will be needed.\n",
    "\n",
    "Note that the results derived here are for the user's reference - most of the GPR products posted on NSIDC already have snow depth and SWE available as variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Radargram Examples](./images/gpr_lidar/radargrams.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lidar Methods for Snow Depth Retrieval and SWE Estimation\n",
    "### A (Very) General Review of Lidar\n",
    "- Lidar emits photons and measures the twt of the returned photons\n",
    "- These twt are converted to elevation surfaces (e.g., DEM, DTM, DSM).\n",
    "- Lidar can be collected from a variety of platforms:\n",
    "    - Terrestrial\n",
    "    - UAV\n",
    "    - Airborne\n",
    "    - Satellite\n",
    "- Two acquisitions are required for snow, a snow-on acquisition and a snow-off acquisition. Snow depth can be calculated in two general ways:\n",
    "    - Raster-based approaches (see figure below, credit Airborne Snow Observatories Inc.)\n",
    "    - Point cloud approaches\n",
    "\n",
    "### How is SWE calculated from lidar snow depths?\n",
    "- At larger scales, SWE is calculated via modeled densities (e.g., M3 Works and ASO).\n",
    "- At smaller field sites, it may be appropriate to use representative in situ measurements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ASO Lidar Figure](./images/gpr_lidar/aso_figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska\n",
    "\n",
    "## Deriving Snow Density at Farmer's Loop/Creamer's Field\n",
    "For this first step, we will use:\n",
    "1. Airborne lidar data collected on 11 March, 2023.\n",
    "2. GPR data collected on 7, 11, 13, and 16 March, 2023.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary packages\n",
    "import os\n",
    "import numpy as np \n",
    "from datetime import date\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "\n",
    "# Geospatial packages\n",
    "import geopandas as gpd #for vector data\n",
    "import xarray as xr\n",
    "import rioxarray #for raster data\n",
    "import pandas as pd\n",
    "from shapely.geometry import box, Point\n",
    "import rasterio as rio\n",
    "\n",
    "# Import SnowEx database\n",
    "from snowexsql.api import PointMeasurements, LayerMeasurements, RasterMeasurements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GPR Data from the SnowEx Database\n",
    "Thanks goes to Micah Johnson and Micah Sandusky from M3Works for their development of the SnowEx Database.\n",
    "\n",
    "We will focus on a single date for the GPR data to limit the memory usage needed for data accessing and processing. The GPR data is in CSV format, so we will use `pandas` to load the data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a date of interest\n",
    "dt = date(2023, 3, 11)\n",
    "\n",
    "# Define site of interest\n",
    "site = LayerMeasurements.from_filter(date=dt,\n",
    "                                     site_name='Fairbanks',\n",
    "                                     site_id='FLCF',\n",
    "                                     limit=1\n",
    "                                    )\n",
    "\n",
    "# Use pandas to read in GPR data\n",
    "gpr_df = PointMeasurements.from_area(pt=site.geometry[0],\n",
    "                                     crs=26906,\n",
    "                                     buffer=10000,\n",
    "                                     type='two_way_travel',\n",
    "                                     observers='Randall Bonnell',\n",
    "                                     date=dt,\n",
    "                                     site_name='farmers-creamers',\n",
    "                                     limit=20213 # Expected number of measurements\n",
    "                                    )\n",
    "\n",
    "gpr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of GPR two-way travel time. We will also estimate snow depth from twt and plot that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate snow depth from the two-way travel time\n",
    "gpr_df['Depth_estimated'] = 0.25 * (gpr_df['value']/2)\n",
    "\n",
    "# Make histograms of two-way travel time and snow depth\n",
    "fig, (ax1,ax2) = plt.subplots(2,1, figsize=(6, 9))\n",
    "gpr_df.plot.hist(ax=ax1, column='value', bins=20, edgecolor='black', legend=False)\n",
    "ax1.set_xlabel(' ')\n",
    "ax1.set_title('Two-way travel time [ns]')\n",
    "\n",
    "gpr_df.plot.hist(ax=ax2, column='Depth_estimated', bins=20, edgecolor='black', legend=False)\n",
    "ax2.set_xlabel(' ')\n",
    "ax2.set_title('Snow depth [m]')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Lidar-Derived Canopy Height and Snow Depth\n",
    "To compare against the GPR data, we will load airborne lidar data products, specifically canopy height and snow depths. To facilitate our analysis, we will create a bounding box for the lidar results using our GPR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x/y limit from GPR data\n",
    "bounds = gpr_df.total_bounds\n",
    "\n",
    "# Create a bounding box\n",
    "gpr_limits = box(*bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from this bounding box, accessing the lidar data is very similar to the GPR data. However, because the lidar data is much larger, loading it through the database may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lidar canopy heights\n",
    "flcf_ch = RasterMeasurements.from_area(shp=gpr_limits,\n",
    "                                      crs=26906,\n",
    "                                      buffer=None,\n",
    "                                      type='canopy_height',\n",
    "                                      site_name='farmers-creamers',\n",
    "                                      observers='chris larsen'\n",
    "                                      )\n",
    "print(flcf_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lidar snow depths\n",
    "flcf_sd = RasterMeasurements.from_area(shp=gpr_limits,\n",
    "                                       crs=26906,\n",
    "                                       buffer=None,\n",
    "                                       type='depth',\n",
    "                                       site_name='farmers-creamers',\n",
    "                                       observers='chris larsen'\n",
    "                                      )\n",
    "print(flcf_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a sample plot of snow depth and the GPR track\n",
    "fig, ax = plt.subplots()\n",
    "show(flcf_sd, ax=ax, cmap='Blues', clim=(0,1.5))\n",
    "gpr_df.plot(ax=ax, color='orange', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above example using the SnowEx database isn't working, then the data is still accessible through `earthaccess`. Importing it with `earthaccess` might be more memory-intensive than most cloud computing hubs can handle, but it should work just fine on a local machine.\n",
    "\n",
    "`earthaccess` requires lat/lon coordinates if attempting to subset by location. So, we will convert the bounds to the proper format before querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Transformer\n",
    "\n",
    "#Input/output CRS information\n",
    "source_crs = \"EPSG:32606\"\n",
    "target_crs = \"EPSG:4326\"\n",
    "\n",
    "# Create CRS transformer\n",
    "transformer = Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "\n",
    "# Setup corners of polygon\n",
    "min_x, min_y, max_x, max_y = bounds\n",
    "corners = [\n",
    "    (min_x, min_y),\n",
    "    (max_x, min_y),\n",
    "    (max_x, max_y),\n",
    "    (min_x, max_y),\n",
    "    (min_x, min_y)\n",
    "]\n",
    "\n",
    "# Convert the polygon to lon/lat\n",
    "bounds_latlon = [transformer.transform(x, y) for x, y in corners]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "\n",
    "# Authenticate with Earthdata Login servers\n",
    "auth = earthaccess.login(strategy=\"interactive\")\n",
    "\n",
    "# Search for lidar data \n",
    "results = earthaccess.search_data(\n",
    "    doi = \"10.5067/BV4D8RRU1H7U\",\n",
    "    temporal = (\"2023-03-11\", \"2023-03-12\"),\n",
    "    polygon = bounds_latlon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "\n",
    "# Open the snow depth data\n",
    "f = earthaccess.open(results)\n",
    "flcf_sd = rxr.open_rasterio(f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lidar snow depths \n",
    "# If on CryoCloud, recommended to subset the data before running this cell.\n",
    "# Alternatively, coarsen the data just for this plotting example.\n",
    "\n",
    "flcf_sd.plot(vmin=0, vmax=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match GPR data with lidar data\n",
    "To match the GPR and lidar data, we can either rasterize the GPR data or vectorize the lidar data. For simplicity, we will vectorize the lidar data and perform a nearest neighbor search.\n",
    "\n",
    "The GPR data is ~0.1 m resolution, whereas the lidar data is ~0.5 m resolution. So, we can expect ~5 GPR data points per lidar pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find height and width of array\n",
    "height, width = flcf_sd.read(1).shape\n",
    "\n",
    "# Create arrays of the height, width of the lidar raster\n",
    "cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "# Get the easting/northing from the lidar raster\n",
    "x_lidar, y_lidar = rio.transform.xy(flcf_sd.transform, rows, cols)\n",
    "\n",
    "# Vectorize the raster data\n",
    "x_lidar_vec = np.array(x_lidar).flatten()\n",
    "y_lidar_vec = np.array(y_lidar).flatten()\n",
    "flcf_sd_vec = flcf_sd.read().flatten()\n",
    "\n",
    "# Pull vectors from GPR GeoDataFrame\n",
    "gpr_arr = np.stack([gpr_df.geometry.x, gpr_df.geometry.y, gpr_df['value']], axis=1)\n",
    "gpr_x = gpr_arr[:,0]\n",
    "gpr_y = gpr_arr[:,1]\n",
    "gpr_twt = gpr_arr[:,2].reshape(len(gpr_arr[:,2]), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the nearest-neighbor approach, we will be using a K-D tree to efficiently find adjacent points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create coordinate sets for nearest neighbor search\n",
    "set1 = np.column_stack((x_lidar_vec, y_lidar_vec))\n",
    "set2 = np.column_stack((gpr_x, gpr_y))\n",
    "\n",
    "# Build KDTree from GPR coordinates\n",
    "tree = cKDTree(set2)\n",
    "\n",
    "# Define the search radius (meters)\n",
    "radius = 0.25\n",
    "\n",
    "# Create function to find median travel times\n",
    "def median_travel_time(point, gpr_kdtree, gpr_coordinates, gpr_twt, radius):\n",
    "    indices = tree.query_ball_point(point, radius)\n",
    "    if indices:\n",
    "        # Retrieve travel times for the nearest neighbors\n",
    "        neighbor_twt = gpr_twt[indices]\n",
    "        median_twt = np.median(neighbor_twt)\n",
    "\n",
    "        return median_twt\n",
    "\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Find twt medians for each lidar point\n",
    "twt_median = np.array([median_travel_time(point,tree,set2,gpr_twt,radius) for point in set1])\n",
    "print(twt_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPR data is not as spatially continuous as the lidar data, so `twt_median` has a lot of NaN values. To reduce memory usage, let's remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask for GPR NaN values\n",
    "mask = np.isnan(twt_median)\n",
    "\n",
    "# Mask GPR/lidar vectors\n",
    "flcf_sd_vec_clean = flcf_sd_vec[~mask]\n",
    "set1_clean = set1[~mask]\n",
    "twt_median_clean = twt_median[~mask]\n",
    "\n",
    "print(twt_median_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is matched, and we have removed NaN values, we can now calculate relative permittivity and snow density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative permittivity\n",
    "c = 0.2998 # Speed of light in a vaccuum\n",
    "e_s = ((c*twt_median_clean) / (2*flcf_sd_vec_clean))**2\n",
    "\n",
    "# Calculate snow density\n",
    "rho_s = ((np.sqrt(e_s)-1) / 0.845)*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the derived densities\n",
    "Now that we've gone through all of this trouble, let's take a look at the derived snow densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot snow density\n",
    "plt.figure()\n",
    "plt.scatter(set1_clean[:,0], set1_clean[:,1], s=10,\n",
    "            c=rho_s, cmap='viridis', clim=(0,500),\n",
    "            edgecolor=None\n",
    "           )\n",
    "plt.colorbar()\n",
    "plt.title('Snow density [kg m-3]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the histogram distribution of snow density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bin edges\n",
    "bin_edges = np.arange(np.min(rho_s), np.max(rho_s), 25)\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure()\n",
    "plt.hist(rho_s, bins=bin_edges, edgecolor=None)\n",
    "plt.xlabel('Snow density [kg m-3]')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some pretty unrealistic values in the data, so let's zoom in to more realistic densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define bin edges\n",
    "bin_edges = np.arange(0, 500, 25)\n",
    "\n",
    "# Create refined histogram\n",
    "plt.figure()\n",
    "plt.hist(rho_s, bins=bin_edges, edgecolor='black')\n",
    "plt.xlabel('Snow density [kg m-3]')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even at this range, there is a fair amount of random error. This may be caused by the measurement accuracy of the lidar or the GPR, the depth of the snow, and/or geolocation uncertainty. \n",
    "\n",
    "A user could further improve these densities by upsampling the lidar to match the GPR's geolocation uncertainty (3 m in this case), remove erroneous values in the snow depth or permittivity data, or run a spatial averaging filter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
