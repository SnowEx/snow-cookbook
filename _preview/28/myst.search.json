{"version":"1","records":[{"hierarchy":{"lvl1":"Snow Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Snow Cookbook"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Snow Cookbook"},"type":"lvl1","url":"/#snow-cookbook","position":2},{"hierarchy":{"lvl1":"Snow Cookbook"},"content":"\n\n\n\n\n\n\n\nThis Project Pythia Cookbook is a compilation of tutorials and training\nmaterials in support of the NASA snow reserach community. Some tutorials\ncome from the 2020 to 2024 SnowEx Hackweek program hosted at the UW eScience\nInstitute. Other materials are drawn from the NASA Goddard “SnowPit” Science\nTask Group or STG. The purpose of the tutorials is to help people with data\naccess and to demonstrate a variety of disciplinary use cases.","type":"content","url":"/#snow-cookbook","position":3},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":4},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Motivation"},"content":"There are numerous data products and methods for accessing and analyzing\nsnow observations. These include field, airborne, and satellite missions.\nThe goal of these tutorials is to streamline data access, reduce duplication\nof effort and build an open science community around snow research\ndatasets, algorithms and software.","type":"content","url":"/#motivation","position":5},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":6},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Authors"},"content":"Zach Fair\n\n\nAnthony Arendt,\n\n\nMark Welden-Smith\n\nmore to be added","type":"content","url":"/#authors","position":7},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":8},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":9},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":10},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Structure"},"content":"This cookbook is broken up into three main sections: “Data Access”, “Observations”, and “Analysis and Machine Learning”. The current listing of subtopics is currently a work in progress.","type":"content","url":"/#structure","position":11},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Section 1: Data Access","lvl2":"Structure"},"type":"lvl3","url":"/#section-1-data-access","position":12},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Section 1: Data Access","lvl2":"Structure"},"content":"Field Campaigns Overview\n\nSnowExSQL Database","type":"content","url":"/#section-1-data-access","position":13},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Section 2: Observations","lvl2":"Structure"},"type":"lvl3","url":"/#section-2-observations","position":14},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Section 2: Observations","lvl2":"Structure"},"content":"GPR and Lidar\n\nTime-lapse Cameras\n\nUAVSAR\n\nMicrostructure\n\nAVIRIS-NG\n\nTerrestrial Laser Scanning","type":"content","url":"/#section-2-observations","position":15},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Section 3: Analysis and Machine Learning","lvl2":"Structure"},"type":"lvl3","url":"/#section-3-analysis-and-machine-learning","position":16},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Section 3: Analysis and Machine Learning","lvl2":"Structure"},"content":"Neural Networks with PyTorch\n\nSnow Modeling\n\nUCLA Reanalysis\n\nMERRA-2\n\nERA5","type":"content","url":"/#section-3-analysis-and-machine-learning","position":17},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":18},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using\n\n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":19},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":20},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of\nhow this works are not important for now. All you need to know is how to launch\na Pythia Cookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.\n\nNote, not all Cookbook chapters are executable. If you do not see\nthe rocket ship icon, such as on this page, you are not viewing an\nexecutable book chapter.","type":"content","url":"/#running-on-binder","position":21},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":22},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer,\nyou will need to follow this workflow:\n\nClone the https://github.com/ProjectPythia/snow-cookbook repository: git clone https://github.com/ProjectPythia/snow-cookbook.git\n\nMove into the snow-cookbook directorycd snow-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate snow-cookbook\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":23},{"hierarchy":{"lvl1":"GPR and Lidar"},"type":"lvl1","url":"/notebooks/gpr-lidar-hackweektutorial","position":0},{"hierarchy":{"lvl1":"GPR and Lidar"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial","position":1},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"Author: Randall Bonnell"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#author-randall-bonnell","position":2},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"Author: Randall Bonnell"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#author-randall-bonnell","position":3},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"Outline:"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#outline","position":4},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"Outline:"},"content":"GPR Methods for the Retrieval of Snow Depth and SWE\n\nLidar Methods for Snow Depth Retrieval and SWE Estimation\n\nLeveraging Coincident GPR and Lidar Data Sets to Derive Snow Density\n\nSnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska\n\nDiscussion: Improving Density Estimation\n\nGPR SnowEx Analysis-Ready Datasets\n\nReferences\n\n\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#outline","position":5},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-1-gpr-methods-for-the-retrieval-of-snow-depth-and-swe","position":6},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-1-gpr-methods-for-the-retrieval-of-snow-depth-and-swe","position":7},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Previous GPR tutorial developed by Tate Meehan (CRREL) that may be of interest: https://​snowex​-2021​.hackweek​.io​/tutorials​/gpr​/gpr​.html","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#previous-gpr-tutorial-developed-by-tate-meehan-crrel-that-may-be-of-interest-https-snowex-2021-hackweek-io-tutorials-gpr-gpr-html","position":8},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Previous GPR tutorial developed by Tate Meehan (CRREL) that may be of interest: https://​snowex​-2021​.hackweek​.io​/tutorials​/gpr​/gpr​.html","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#previous-gpr-tutorial-developed-by-tate-meehan-crrel-that-may-be-of-interest-https-snowex-2021-hackweek-io-tutorials-gpr-gpr-html","position":9},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"SnowEx Review","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#snowex-review","position":10},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"SnowEx Review","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"Ground-based, airborne, and satellite radars were operated as part of the NASA SnowEx campaigns.\n\nGround-based radars included ground-penetrating radar (GPR), frequency-modulated continuous-wave radar (FMCW), and tower mounted radars.\n\nWhat airborne and satellite radars were tasked?","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#snowex-review","position":11},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"A Brief Blurb on Radar Physics","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#a-brief-blurb-on-radar-physics","position":12},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"A Brief Blurb on Radar Physics","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"Radar is fully transmissible in dry snow, but there is frequency-dependent interaction between the radar signal and the snowpack.\n\nAt L-band frequencies (1–2 GHz, ~25 cm wavelength) there limited to no interaction with the snowpack.","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#a-brief-blurb-on-radar-physics","position":13},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"What is GPR?","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#what-is-gpr","position":14},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"What is GPR?","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"We use L-band GPR, which was operated during all SnowEx campaigns!\n\nGPR transmits a radar signal into the snowpack, which then reflects off objects/interfaces with contrasting dielectric permittivity. The GPR records the amplitude and two-way travel time (twt) of the reflections.\n\nDielectric permittivity refers to the dielectric properties of the snowpack that define how EM energy transmits through the medium.\n\nUsually, we are interested in the snow-ground interface and we measure the snowpack thickness in twt (nanoseconds).\n\nHowever, in complex vegetation, radargrams are difficult to interpret! Causes increased uncertainty.\n\nSee radargram examples below for the boreal forest GPR surveys (credit Kajsa Holland-Goon).\n\n\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#what-is-gpr","position":15},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Snow Depth, SWE, and Density Calculations","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#snow-depth-swe-and-density-calculations","position":16},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Snow Depth, SWE, and Density Calculations","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"To calculate snow depth (d_s) from twt, we need to estimate the relative permittivity (\\epsilon_s) and radar velocity (v_s) of the snowpack:\n\nv_s = \\frac{c}{\\sqrt{\\epsilon_s}}; --> Where c is the velocity of EM energy in a vacuum.\n\n\\epsilon_s = (1+\\frac{0.845\\rho_s}{1000})^2; --> Kovacs et al. (1995), but more than 19 equations exist for dry snow conditions.\n\nd_s = \\frac{twt}{2}*v_s;\n\nSWE = d_s\\rho_s;--> Where SWE is snow water equivalent.\n\nBut...If we know the snow depth, we can constrain the radar velocity and estimate relative permittivity and density!\n\n\\epsilon_s=(\\frac{c*twt}{2d_s})^2\n\n\\rho_s=(\\sqrt{\\epsilon_s}-1)\\frac{1000}{0.845}\n\nHow can we find the snow depth?","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#snow-depth-swe-and-density-calculations","position":17},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"A Shameless Plug...","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#a-shameless-plug","position":18},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"A Shameless Plug...","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"Most analysis-ready GPR products have twt, snow depth, and snow water equivalent. Some have been updated with derived snow densities. See 6. SnowEx GPR Analysis-Ready Datasets below.\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#a-shameless-plug","position":19},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-2-lidar-methods-for-snow-depth-retrieval-and-swe-estimation","position":20},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-2-lidar-methods-for-snow-depth-retrieval-and-swe-estimation","position":21},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Previous lidar tutorial developed by Naheem Adebisi (ESRI) that may be of interest: https://​snowex​-2022​.hackweek​.io​/tutorials​/lidar​/index​.html","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#previous-lidar-tutorial-developed-by-naheem-adebisi-esri-that-may-be-of-interest-https-snowex-2022-hackweek-io-tutorials-lidar-index-html","position":22},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Previous lidar tutorial developed by Naheem Adebisi (ESRI) that may be of interest: https://​snowex​-2022​.hackweek​.io​/tutorials​/lidar​/index​.html","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#previous-lidar-tutorial-developed-by-naheem-adebisi-esri-that-may-be-of-interest-https-snowex-2022-hackweek-io-tutorials-lidar-index-html","position":23},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"A (Very) General Review of Lidar","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#a-very-general-review-of-lidar","position":24},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"A (Very) General Review of Lidar","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"content":"Lidar emits photons and measures the twt of the returned photons\n\nThese twt are converted to elevation surfaces (e.g., DEM, DTM, DSM).\n\nLidar can be collected from a variety of platforms:\n\nTerrestrial\n\nUAV\n\nAirborne\n\nSatellite\n\nTwo acquisitions are required for snow, a snow-on acquisition and a snow-off acquisition. Snow depth can be calculated in two general ways:\n\nRaster-based approaches (see figure below, credit Airborne Snow Observatories Inc.)\n\nPoint cloud approaches","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#a-very-general-review-of-lidar","position":25},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"How is SWE calculated from lidar snow depths?","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#how-is-swe-calculated-from-lidar-snow-depths","position":26},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"How is SWE calculated from lidar snow depths?","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"content":"At larger scales, SWE is calculated via modeled densities (e.g., M3 Works and ASO).\n\nAt smaller field sites, it may be appropriate to use representative in situ measurements.\n\n\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#how-is-swe-calculated-from-lidar-snow-depths","position":27},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"3. Leveraging Coincident GPR and Lidar Data Sets to Derive Snow Density"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-3-leveraging-coincident-gpr-and-lidar-data-sets-to-derive-snow-density","position":28},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"3. Leveraging Coincident GPR and Lidar Data Sets to Derive Snow Density"},"content":"Density, liquid water content, and relative permittivity are understudied relative to snow depth and/or SWE.\n\nCombined coincident snow depths and twt can yield spatially distributed measurements of relative permittivity.\n\nIn wet snow, relative permittivity can be converted to liquid water content (e.g., Webb et al., 2018, 2020, 2022; Bonnell et al., 2021).\n\nIn dry snow, density can be estimated from the relative permittivity (Yildiz et al., 2021; McGrath et al., 2022; Bonnell et al., 2023; Meehan et al., 2024).\n\nThis technique has provided an unprecedented glimpse into the spatial properties of these parameters!\n\nCritically, studies have noted a large random error in derived products that should be considered (see figure below, credit: Meehan et al., 2024).\n\n\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-3-leveraging-coincident-gpr-and-lidar-data-sets-to-derive-snow-density","position":29},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-4-snowex23-gpr-lidar-derived-permittivities-densities-in-the-boreal-forest-alaska","position":30},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-4-snowex23-gpr-lidar-derived-permittivities-densities-in-the-boreal-forest-alaska","position":31},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Here, we will use this approach to derive densities at Farmer’s Loop Creamer’s Field during the SnowEx23 Alaska Campaign","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#here-we-will-use-this-approach-to-derive-densities-at-farmers-loop-creamers-field-during-the-snowex23-alaska-campaign","position":32},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Here, we will use this approach to derive densities at Farmer’s Loop Creamer’s Field during the SnowEx23 Alaska Campaign","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"Lidar data was collected on 11 March 2023\n\nGPR data was collected on 7, 11, 13, and 16 March 2023\n\n#1.1 Load relevant packages\nimport os\nimport numpy as np \nfrom datetime import date\nfrom scipy.spatial import cKDTree\n\n#packages for figures\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\n\n#geospatial packages\nimport geopandas as gpd #for vector data\nimport xarray as xr\nimport rioxarray #for raster data\nimport pandas as pd\nfrom shapely.geometry import box, Point\nimport rasterio as rio\n\n#Import SnowEx database\nfrom snowexsql.api import PointMeasurements, LayerMeasurements, RasterMeasurements\n\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#here-we-will-use-this-approach-to-derive-densities-at-farmers-loop-creamers-field-during-the-snowex23-alaska-campaign","position":33},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Part 1: Load the GPR data from the SnowEx data base","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#part-1-load-the-gpr-data-from-the-snowex-data-base","position":34},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Part 1: Load the GPR data from the SnowEx data base","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"-Huge thank you to Micah Johnson and Micah Sandusky for their support!\n\nNote that if we used the full GPR/Lidar dataset, we would need to allocate way more memory. This example focuses on a single date of collection in very dense forest.\n\nExamine the headers from the GPR csv --> what are the variables that we are interested in?\n\n# 1.2 Load GPR data\n\n#Note, memory space is fairly limited, will need to pull only one date\n\n#Set a number of dates to pull GPR for\n#dt1 = date(2023, 3, 7)\ndt2 = date(2023, 3, 11)\n#dt3 = date(2023, 3, 13)\n#dt4 = date(2023, 3, 16)\n\n#site1 = LayerMeasurements.from_filter(date=dt1, site_name='Fairbanks', site_id='FLCF', limit=1)\nsite2 = LayerMeasurements.from_filter(date=dt2, site_name='Fairbanks', site_id='FLCF', limit=1)\n#site3 = LayerMeasurements.from_filter(date=dt3, site_name='Fairbanks', site_id='FLCF', limit=1)\n#site4 = LayerMeasurements.from_filter(date=dt4, site_name='Fairbanks', site_id='FLCF', limit=1)\n\n#Use pandas ot read in csv data\n#gpr_df_dt1 = PointMeasurements.from_area(pt=site1.geometry[0], crs=26906, buffer=10000,\n#    type='two_way_travel',\n#    observers='Randall Bonnell',\n#    date=dt1, site_name='farmers-creamers',\n#    limit=29432)#The number of expected measurements\ngpr_df_dt2 = PointMeasurements.from_area(pt=site2.geometry[0], crs=26906, buffer=10000,\n    type='two_way_travel',\n    observers='Randall Bonnell',\n    date=dt2, site_name='farmers-creamers',\n    limit=20213)#The number of expected measurements\n#gpr_df_dt3 = PointMeasurements.from_area(pt=site3.geometry[0], crs=26906, buffer=10000,\n#    type='two_way_travel',\n#    observers='Randall Bonnell',\n#    date=dt3, site_name='farmers-creamers',\n#    limit=19024)\n#gpr_df_dt4 = PointMeasurements.from_area(pt=site4.geometry[0], crs=26906, buffer=10000,\n#    type='two_way_travel',\n#    observers='Randall Bonnell',\n#    date=dt4, site_name='farmers-creamers',\n#    limit=15785)\n\n\n#Compile into one dataframe\n#flcf_gpr_df = pd.concat([gpr_df_dt1,gpr_df_dt2,gpr_df_dt3,gpr_df_dt4],axis=0, join='outer', ignore_index=True, keys=None, levels=None,names=None,verify_integrity=False,sort=False,copy=None)\nflcf_gpr_df = gpr_df_dt2\n#Print out the csv headers and initial entries --> What's important here and what do we need?\nprint(flcf_gpr_df.head())\n\n# Let's look at the distribution of gpr two-way travel times and estimated snow depths\n#Estimate snow depths from twt by assuming a velocity of 0.25 m/ns --> Is this an appropriate velocity estimate?\nflcf_gpr_df['Depth_estimated'] = (flcf_gpr_df['value']/2)*0.25\n\nax1 = flcf_gpr_df.plot.hist(column=[\"value\"], edgecolor='black', title='two-way travel time (ns)')\nax2 = flcf_gpr_df.plot.hist(column=[\"Depth_estimated\"], edgecolor='black', title='Snow depth (m)')\n\n#Extract x/y limits from GPR data --> these will be used when loading the lidar snow depths\nbounds = flcf_gpr_df.total_bounds\n\n# Create a bounding box\ngpr_limits = box(*bounds)\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#part-1-load-the-gpr-data-from-the-snowex-data-base","position":35},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Let’s load in the lidar canopy heights and snow depths.","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#lets-load-in-the-lidar-canopy-heights-and-snow-depths","position":36},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Let’s load in the lidar canopy heights and snow depths.","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"We’ll look at the canopy heights to get an idea of what kind of forest the data were collected in.\n\nThen, we’ll look at the lidar snow depths to visualize the snow distribution.","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#lets-load-in-the-lidar-canopy-heights-and-snow-depths","position":37},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Discussion questions:","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#discussion-questions","position":38},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Discussion questions:","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"What type of survey design was implemented for the GPR?\n\nDo the lidar snow depth patterns seem to exhibit any kind of dependence upon the forest cover?\n\n# 1.3 Load Lidar vegetation/canopy heights --> This may take a few minutes\n#Read in the canopy heights raster from Farmer's Loop/Creamer's Field Alaska\nflcf_ch = RasterMeasurements.from_area(shp = gpr_limits, crs=26906,\n    buffer=None, type='canopy_height',\n    site_name='farmers-creamers',\n    observers='chris larsen')\nprint(flcf_ch)\n\n#Plot the datasets\nfig, ax = plt.subplots()\nshow(flcf_ch, ax=ax, cmap='Greens', clim=(0,5), title = 'Canopy Height (m)')\n#Plot the GPR points on top\nflcf_gpr_df.plot(ax=ax, color='blue', markersize = 10)\n\n# 1.4 Load Lidar Snow depths --> This will take a few minutes\n\n#Read in the canopy heights raster from Farmer's Loop/Creamer's Field Alaska\nflcf_ds = RasterMeasurements.from_area(shp = gpr_limits, crs=26906,\n    buffer=None, type='depth',\n    site_name='farmers-creamers',\n    observers='chris larsen')\n\n#Plot the datasets\nfig, ax = plt.subplots()\nshow(flcf_ds, ax=ax, cmap='Blues', clim=(0,1.5), title='Snow Depth (m)')\n#Plot the GPR points on top\nflcf_gpr_df.plot(ax=ax, color='red', markersize = 10)\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#discussion-questions","position":39},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Part 2: Match the GPR data to the lidar grid and derive relative permittivity and density","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#part-2-match-the-gpr-data-to-the-lidar-grid-and-derive-relative-permittivity-and-density","position":40},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Part 2: Match the GPR data to the lidar grid and derive relative permittivity and density","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"There are two conceptual paths forward:\n\nRasterize the GPR data or\n\nVectorize the lidar data\n\nFor simplicity, the following code:\n\nvectorizes the lidar data\n\nperforms a nearest neighbor search between the lidar and GPR coordinate vectors\n\nCalculates the median GPR twt from the nearest neighbors\n\nDerives relative permittivity and density from the lidar snow depths and median twt\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#part-2-match-the-gpr-data-to-the-lidar-grid-and-derive-relative-permittivity-and-density","position":41},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"We need to know the resolutions of the lidar and GPR datasets","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#we-need-to-know-the-resolutions-of-the-lidar-and-gpr-datasets","position":42},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"We need to know the resolutions of the lidar and GPR datasets","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"The GPR dataset consists of points that are spaced ~0.10 m apart.\n\nWhat about the lidar? Run the code block below to answer this question.\n\nHow many GPR points would you expect to have per lidar pixel? Assume linear transects through each pixel.\n\n#2.1 Let's learn a bit about the resolution of the lidar rasters\n\nheight, width = flcf_ds.read(1).shape #Find the height and width of the array\n\n#Use meshgrid to create two arrays matching the height/width of the input raster\n#The GPR dataset consists of vectors --> we will eventually need to vectorize these lidar arrays\ncols, rows = np.meshgrid(np.arange(width), np.arange(height)) \n\n\n#Extract the easting/northing from the raster \nx_lidar, y_lidar = rio.transform.xy(flcf_ds.transform, rows, cols) \n\n#What's the resolution of the lidar dataset?\nprint(\"The x resolution of the snow depth raster is:\",x_lidar[0][1]-x_lidar[0][0])\nprint(\"The y resolution of the snow depth raster is:\",y_lidar[0][0]-y_lidar[1][0])\n\n\n# 2.2 Matching GPR to the lidar grid\n\n#Two conceptual paths forward: rasterize the GPR data, or convert lidar data to points\n\n#Let's vectorize the raster data\nx_lidar_vec = np.array(x_lidar).flatten()\ny_lidar_vec = np.array(y_lidar).flatten()\nflcf_ds_vec = flcf_ds.read().flatten()\n\n#Pull vectors from geo dataframe\ngpr_arr = np.stack([flcf_gpr_df.geometry.x, flcf_gpr_df.geometry.y,flcf_gpr_df['value']], axis=1)\ngpr_x=gpr_arr[:,0]\ngpr_y=gpr_arr[:,1]\ngpr_twt=gpr_arr[:,2].reshape(len(gpr_arr[:,2]),1)\n\n\n#2.3 Create sets of coordinates for the nearest neighbors search\ncoordinates_set1 = np.column_stack((x_lidar_vec,y_lidar_vec))\ncoordinates_set2 = np.column_stack((gpr_x,gpr_y))\n\n# Build KDTree from the second set of coordinates\ntree = cKDTree(coordinates_set2)\n\n# Define the radius (in meters)\nradius = 0.25\n\n# Function to find the median of travel times within a radius --> Credit where credit is due, this function was generated in part by chatgpt\ndef find_median_travel_time_within_radius(point, tree, coordinates_set1, gpr_twt, radius):\n    indices = tree.query_ball_point(point, radius)\n    if indices:\n        # Retrieve travel times for the nearest neighbors\n        neighbor_twt = gpr_twt[indices]\n        median_twt = np.median(neighbor_twt)\n        return median_twt\n    else:\n        return np.nan  # Return NaN if no neighbors are within the radius\n# Find medians for each lidar point\nmedians = np.array([find_median_travel_time_within_radius(point, tree, coordinates_set2, gpr_twt, radius) for point in coordinates_set1])\n\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#we-need-to-know-the-resolutions-of-the-lidar-and-gpr-datasets","position":43},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"The GPR data is not as spatially continuous as the lidar data, so most of the median twt dataset consists of nan’s","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#the-gpr-data-is-not-as-spatially-continuous-as-the-lidar-data-so-most-of-the-median-twt-dataset-consists-of-nans","position":44},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"The GPR data is not as spatially continuous as the lidar data, so most of the median twt dataset consists of nan’s","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"Let’s remove the nan’s to free up memory and reduce processing time.\n\n#At this point, all lidar points should have an associated gpr twt --> most are likely nan's though. But let's check!\nprint(\"The gpr array has size:\",medians.shape)\nprint(\"The lidar array has size:\",flcf_ds_vec.shape)\n\n\n#2.4 Before we get to the math part, let's clear out the nan's from all important vectors:\n#Create mask for gpr medians that are nan's\nmask = np.isnan(medians)\n\n#Remove entries from the lidar snow depth, x, and y vectors that align with the nan twt values\nflcf_ds_vec_clean = flcf_ds_vec[~mask]\ncoordinates_set1_clean=coordinates_set1[~mask]\n\n#Lastly, remove entries from the twt medians\nmedians_clean = medians[~mask]\n\n#Let's check the new size of the twt array\nprint(medians_clean.shape)\n\n\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#the-gpr-data-is-not-as-spatially-continuous-as-the-lidar-data-so-most-of-the-median-twt-dataset-consists-of-nans","position":45},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Discussion questions:","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#discussion-questions-1","position":46},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Discussion questions:","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"Roughly, how many points were removed?\n\nWhen we are done, we will have derived 3788 snow density estimates. In the same area, about four snow pits were dug, resulting in four bulk density measurements. How useful do you think our data will be?\n\nIs more always better?","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#discussion-questions-1","position":47},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Let’s now transition to the relative permittivity and density calculations","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#lets-now-transition-to-the-relative-permittivity-and-density-calculations","position":48},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Let’s now transition to the relative permittivity and density calculations","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"\n\n#2.5 We finally get to the math part!!\n#Let's calculate relative permittivity first...\nc=0.2998#The speed of light in a vacuum\ne_s = ((c * medians_clean) / (2 * flcf_ds_vec_clean)) ** 2\n\n#And then calculate density\nrho_s = ((np.sqrt(e_s) - 1) / 0.845) * 1000\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#lets-now-transition-to-the-relative-permittivity-and-density-calculations","position":49},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Part 3: Examining the derived densities","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#part-3-examining-the-derived-densities","position":50},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Part 3: Examining the derived densities","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"\n\n# 3.1 Finally, let's take a peek at what the derived densities look like...\nplt.figure()\nplt.scatter(coordinates_set1_clean[:,0], coordinates_set1_clean[:,1], s=10, c=rho_s, cmap='viridis', clim=(0, 500), edgecolor=None)\n\n# Add colorbar to show the scale of color values\nplt.colorbar()\nplt.title('Snow Density (kg m-3)')\n\n# Show the plot\nplt.show()\n\n# 3.2 What does the histogram distribution look like??\n# Define bin edges\nbin_edges = np.arange(np.min(rho_s), np.max(rho_s), 25)  # Create bin edges from min(x) to max(x) with step size 25\n\n# Create the histogram\nplt.figure()  # Create a new figure\nplt.hist(rho_s, bins=bin_edges, edgecolor=None)  # Plot histogram with specified bin edges\n\nplt.title('Snow Density Histogram')\n\n# Show the plot\nplt.show()\n\n#Let's zoom in a little...\n# Define bin edges\nbin_edges = np.arange(0, 500, 25)  # Create bin edges from min(x) to max(x) with step size 25\n\n# Create the histogram\nplt.figure()  # Create a new figure\nplt.hist(rho_s, bins=bin_edges, edgecolor='black')  # Plot histogram with specified bin edges\n\nplt.title('Snow Density Histogram')\n\n# Show the plot\nplt.show()\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#part-3-examining-the-derived-densities","position":51},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"5. Discussion: Improving Density Estimation"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-5-discussion-improving-density-estimation","position":52},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"5. Discussion: Improving Density Estimation"},"content":"What do you think? Do the derived densities look usable at this stage?","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-5-discussion-improving-density-estimation","position":53},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"What contributes to the random error?","lvl2":"5. Discussion: Improving Density Estimation"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#what-contributes-to-the-random-error","position":54},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"What contributes to the random error?","lvl2":"5. Discussion: Improving Density Estimation"},"content":"There are three groups of factors that control the random error:\n\nMeasurement accuracy for lidar snow depths and GPR twt. Reduced accuracy for either or both of the techniques will lead to large errors. The boreal forest had a lot of complex vegetation that may have impeded the accuracy of these instruments.\n\nDepth of the snowpack. The accuracy of the lidar is not a function of snow depth. Thus, the random errors reduce as the snow depth increases. The boreal forest snow depths were shallow!\n\nGeolocation alignment. GPR coordinates were post-processed, but accuracy is still likely on the order of ±3 m.\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#what-contributes-to-the-random-error","position":55},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Improving the derived densities","lvl2":"5. Discussion: Improving Density Estimation"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#improving-the-derived-densities","position":56},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Improving the derived densities","lvl2":"5. Discussion: Improving Density Estimation"},"content":"Let’s say we want to learn something about snow density in the boreal forest. The derived densities offer a HUGE increase in the number of available density measurements compared to in situ. But, in situ are much more accurate. How can we improve this dataset?\n\nIncrease the footprint of the derived densities by upsampling the lidar (e.g., to 3 m).\n\nThis will reduce the impact of GPR geolocation accuracy and the lidar/GPR observation uncertainty.\n\nNeed to be careful! The GPR footprint is large, but it may not scale well past 3 m.\n\nRemove erroneous values.\n\nHow does the lidar survey time compare with the GPR survey time? Was the snow disturbed or did more snow accumulate between surveys?\n\nRelative permittivity of snow cannot be less than air (\\epsilon_a = 1.0) or greater than liquid water (\\epsilon_w = 88).\n\nFor dry snow, relative permittivity is usually between 1.0 and 2.0. The removal of values outside a certain number of standard deviations and/or the interquartile range may be warranted.\n\nRun a spatial averaging filter.\n\nOur surveys were primarily spirals --> should pair nicely with such a filter!\n\nExperiment with the window size of the filter. How would a 5 m x 5 m filter compare to a 25 m x 25 m filter?\n\nShould the data be parsed into different forest cover classes before such a filter is run?\n\nBe careful of linear transects! Large windows tend to remove any density variability along such transects.\n\nOnce you reach this point, it is likely that the densities will be analysis ready. You could run a predictive model to fill in the void spaces, use the densities to evaluate models, calculate experimental variograms, etc.\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#improving-the-derived-densities","position":57},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"6. SnowEx GPR Analysis-Ready Datasets"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-6-snowex-gpr-analysis-ready-datasets","position":58},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"6. SnowEx GPR Analysis-Ready Datasets"},"content":"Grand Mesa, Colorado (SnowEx 2017, 2020)\n\nWebb et al. (2019). \n\nWebb et al. (2019)\n\nBonnell et al. (2021). \n\nBonnell et al. (2021)\n\nMeehan (2021). \n\nMeehan (2021)\n\nWebb (2021). \n\nWebb (2021)\n\nMeehan & Hojatimalekshah (2024). \n\nMeehan & Hojatimalekshah (2024)\n\nCameron Pass, Colorado (SnowEx 2020, 2021)\n\nMcGrath et al. (2021). \n\nMcGrath et al. (2021)\n\nBonnell et al. (2022). \n\nBonnell et al. (2022)\n\nBonnell et al. (2024). \n\nBonnell et al. (2024)\n\nJemez Mountains, New Mexico (SnowEx 2020)\n\nWebb (2021). \n\nWebb (2021)\n\nArctic Coastal Plains, Alaska (SnowEx 2023)\n\nWebb (2024). \n\nWebb (2024)\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-6-snowex-gpr-analysis-ready-datasets","position":59},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"7. References"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-7-references","position":60},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"7. References"},"content":"Lidar Datasets\n\nLarsen (2024). \n\nLarsen (2024)\n\nRelevant GPR LWC Studies\n\nWebb et al. (2018). \n\nWebb et al. (2018)\n\nWebb et al. (2020). \n\nWebb et al. (2020)\n\nBonnell et al. (2021). \n\nBonnell et al. (2021)\n\nWebb et al. (2022). \n\nWebb et al. (2022)\n\nRelevant GPR Density Studies\n\nYildiz et al. (2021). \n\nYildiz et al. (2021)\n\nMcGrath et al. (2022). \n\nMcGrath et al. (2022)\n\nBonnell et al. (2023). \n\nBonnell et al. (2023)\n\nMeehan et al. (2024). \n\nMeehan et al. (2024)","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-7-references","position":61},{"hierarchy":{"lvl1":"Reanalysis Data Access"},"type":"lvl1","url":"/notebooks/ucla-data-access","position":0},{"hierarchy":{"lvl1":"Reanalysis Data Access"},"content":"This workbook is to access the Western United States snow reanalysis data set, as developed by UCLA. Normally, the data is provided as PNG or netCDF files, and it is not cloud-optimized. So, we will need to download the data into a tmp/ folder, as a direct access will be computationally inefficient.\n\nimport xarray as xr\nimport earthaccess\nimport boto3\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport matplotlib.pyplot as plt\nimport warnings\nfrom IPython.display import display, Markdown\n\nAs implied by the dataset’s name, the UCLA reanalysis is only for the Western United States. For this example, we will look over the Tuolumne River Basin in California.\n\nAs with the MERRA-2 example workbook, we will be using the dataset DOI to quickly access the data, and looking at the 2020-2021 water year.\n\n# Define bounding box over the Tuolumne\nbbox = (-119.85, 37.71, -119.1, 38.25)\n\n# Authenticate using Earthdata Login prerequisite files\nauth = earthaccess.login()\n\n# Search for the granule by DOI\nresults = earthaccess.search_data(\n    doi='10.5067/PP7T2GBI52I2',\n    temporal=(\"2020-10-01\", \"2021-09-30\"),\n    bounding_box = bbox\n)\n\n# Download the files to a tmp folder, and save paths as a list\nfiles = earthaccess.download(results, \"/home/jovyan/tmp/\")\nfiles\n\nThe four files we downloaded provide reanalysis data for snow water equivalent and snow cover (SWE_SCA_POST), as well as snow depth (SD_POST). We have two files for each to correspond to different latitudes (N37 and N38 in the file names).\n\nWe could load these files individually, but Xarray has functionality to load all of them at once with xarray.open_mfdataset()!\n\nds = xr.open_mfdataset(files)\n\nds\n\nOur DataArray has four dimensions: Day, Stats, Longitude, and Latitude. Days refers to the number of days after the start of the water year (October 1st), which isn’t very useful on its own. So, let’s change it into a datetime format.\n\nimport re\nimport pandas as pd\n\n# Find year in file name\nurl = files[0]\ndate_pattern = r'\\d{4}'\n\n# Convert year to start of water year (pd.datetime format)\nWY_start_date = pd.to_datetime(f'{re.search(date_pattern, url).group()}-10-01')\n\n# Define new coordinates that use dates rather than day numbers\nds.coords['time'] = (\"Day\", pd.date_range(WY_start_date, periods=ds.sizes['Day']))\nds = ds.swap_dims({'Day':'time'})\n\nThe Stats coordinate refers to the statistics that are available for each of the variables, but its inputs are numeric, rather than strings. The stats_dictionary below outlines the statistics associated with each number, with 25pct and 75pct referring to the 25th-percentile and the 75th-percentile, respectively.\n\n# Make dictionary of statistics\nstats_dictionary = {'mean':0, \n                    'std':1, \n                    'median':2, \n                    '25pct':3, \n                    '75pct':4}\n\n# Choose statistic of interest\nstat = stats_dictionary['mean']\n\nFor this example, we are grabbing the mean daily SWE (SWE_Post):\n\nmean_daily_swe = ds['SWE_Post'].sel(Stats=stat)\n\nmean_daily_swe\n\nLooking at the output, we can see that the data now has “Array” and “Chunk” information. This is because the data was lazy-loaded through the dask, given that there is a lot of data stored in each file.\n\nBefore we plot the data, we will need to reduce it to our time frequency of interest (monthly, in this case). We will then properly load the data into memory.\n\n# Resample the SWE data to a monthly mean\nmean_monthly_swe = mean_daily_swe.resample(time=\"1ME\").mean()\n\n# Load the monthly mean data into memory\nmean_monthly_swe = mean_monthly_swe.compute()\n\nCaution: The above plotting cell can be a bit time-consuming if you are working with a lot of files at once.\n\nFinally, we will make a figure showing the monthly SWE across an entire water year.\n\n# Define months as strings, for subplot titles\nmonths = ['October', 'November', 'December', 'January',\n          'February', 'March', 'April', 'May',\n          'June', 'July', 'August', 'September']\n\n# Plot the SWE data as monthly means\nfig = mean_monthly_swe.plot.imshow(\n    col='time',\n    col_wrap=4,\n    cmap=\"Blues\",\n    vmin=0,\n    vmax=1,\n)\n\n# Set titles to month\nfor ax, title in zip(fig.axs.flatten(), months):\n    ax.set_title(title, fontsize=12)\n\n# Change colorbar label and label sizes\nfig.cbar.ax.tick_params(labelsize=16)\nfig.cbar.set_label(label='SWE [m]', size=16, weight='bold')","type":"content","url":"/notebooks/ucla-data-access","position":1},{"hierarchy":{"lvl1":"AVIRIS-NG"},"type":"lvl1","url":"/notebooks/aviris-ng-data","position":0},{"hierarchy":{"lvl1":"AVIRIS-NG"},"content":"Lessons learned working with the NSIDC dataset.Dataset: SnowEx 2021; Senator Beck Basin and Grand MesaTutorial Author: \n\nBrent Wilder\n\nLearning Objectives\n\nUnderstand how this data is structured\n\nUnderstand where to find necessary terrain and illumination data\n\nLearn about the spectral python package and apply it to this dataset\n\n","type":"content","url":"/notebooks/aviris-ng-data","position":1},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"Computing environment"},"type":"lvl2","url":"/notebooks/aviris-ng-data#computing-environment","position":2},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"Computing environment"},"content":"We’ll be using the following open source Python libraries in this notebook:\n\nfrom spectral import *\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n","type":"content","url":"/notebooks/aviris-ng-data#computing-environment","position":3},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"SnowEx21 Spectral Reflectance Dataset"},"type":"lvl2","url":"/notebooks/aviris-ng-data#snowex21-spectral-reflectance-dataset","position":4},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"SnowEx21 Spectral Reflectance Dataset"},"content":"The data were collected using an airborne imaging spectrometer, AVIRIS-NG can be downloaded from here, \n\nhttps://​nsidc​.org​/data​/snex21​_ssr​/versions/1.\n\nReflectance is provided at 5 nm spectral resolution with a range of 380-2500 nm\n\nFor this dataset, the pixel resolution is 4 m\n\nData span from 19 March 2021 to 29 April 2021, and were collected in two snow-covered environments in Colorado: Senator Beck Basin and Grand Mesa\n\nEach file will have a “.img” and “.hdr”. You need to have both of these in the same directory to open data.\n\n\n\n","type":"content","url":"/notebooks/aviris-ng-data#snowex21-spectral-reflectance-dataset","position":5},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"Downloading necessary terrain and illumination data"},"type":"lvl2","url":"/notebooks/aviris-ng-data#downloading-necessary-terrain-and-illumination-data","position":6},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"Downloading necessary terrain and illumination data"},"content":"The NSIDC repository does not contain the terrain/illumination information.\n\nHowever, you can obtain it for the matching flightline (by its timestamp) at the following URL, \n\nhttps://​search​.earthdata​.nasa​.gov/ ,\n\nand searching for “AVIRIS-NG L1B Calibrated Radiance, Facility Instrument Collection, V1”\n\nYou only need to download the “obs_ort” files for the flight of interest. Please note these are different than “obs” files (ort means orthorectified).\n\nIn the Granule ID search, you can use wildcars “*” on either end of “obs_ort” to reduce your search.\n\nYou may also want to use this bounding box to reduce your search:\n\nSW: 37.55725,-108.58887\n\nNE: 39.78206,-106.16309\n\n","type":"content","url":"/notebooks/aviris-ng-data#downloading-necessary-terrain-and-illumination-data","position":7},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"Using python package, spectral, to open data"},"type":"lvl2","url":"/notebooks/aviris-ng-data#using-python-package-spectral-to-open-data","position":8},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"Using python package, spectral, to open data"},"content":"Important\n\nUpdate the paths below to your local environment\n\n# INSERT YOUR PATHS HERE\npath_to_aviris = '/data/Albedo/AVIRIS/ang20210429t191025_rfl_v2z1'\npath_to_aviris_hdr = '/data/Albedo/AVIRIS/ang20210429t191025_rfl_v2z1.hdr'\npath_to_terrain = '/data/Albedo/AVIRIS/ang20210429t191025_rfl_v2z1_obs_ort'\npath_to_terrain_hdr = '/data/Albedo/AVIRIS/ang20210429t191025_rfl_v2z1_obs_ort.hdr'\n\n# Open a test image\naviris = envi.open(path_to_aviris_hdr)\n\n# Save to an array in memory\nrfl_array = aviris.open_memmap(writeable=True)\n\n# print shape. You can see here we have 425 spectral bands for a grid of 1848x699 pixels\nrfl_array.shape\n\n\n# You can create an array of the bands centers like this\nbands = np.array(aviris.bands.centers)\nbands\n\n# A simple data visalization by selecting random indices\ni = 900\nj = 300\npixel = rfl_array[i,j,:]\n\nfig, ax = plt.subplots(1, 1, figsize=(10,5))\nplt.rcParams.update({'font.size': 18})\nax.scatter(bands, pixel, color='blue', s=20)\nax.set_xlabel('Wavelength [nm]')\nax.set_ylabel('Reflectance')\nplt.show()\n\n","type":"content","url":"/notebooks/aviris-ng-data#using-python-package-spectral-to-open-data","position":9},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"Lastly, a very important note!"},"type":"lvl2","url":"/notebooks/aviris-ng-data#lastly-a-very-important-note","position":10},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"Lastly, a very important note!"},"content":"Please notice that convention for aspect follows -\\pi to \\pi.\n\n# Terrain bands:\n# 0 - Path length (m)\n# 1 - To sensor azimuth\n# 2 - To sensor zenith\n# 3 - To sun azimuth\n# 4 - To sun zenith\n# 5 - Solar phase\n# 6 - Slope\n# 7 - Aspect\n# 8 - cosine(i) (local solar illumination angle)\n# 9 - UTC Time\n# 10 - Earth-sun distance (AU)\n\n# open envi object\nterrain = envi.open(path_to_terrain_hdr)\n\n# Save to an array in memory\nterrain_array = terrain.open_memmap(writeable=True)\n\n# Grab just aspect and flatten (remove nan)\naspects = terrain_array[:,:,7].flatten()\naspects = aspects[aspects>-9999]\n\n\n# Plot a histogram to show aspect range\nfig, ax = plt.subplots(1, 1, figsize=(10,5))\nplt.rcParams.update({'font.size': 18})\nax.hist(aspects, color='black', bins=50)\nax.set_xlabel('Aspect [degrees]')\nax.set_ylabel('Count')\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/aviris-ng-data#lastly-a-very-important-note","position":11},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"References"},"type":"lvl2","url":"/notebooks/aviris-ng-data#references","position":12},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"References"},"content":"To further explore these topics:\n\nhttps://​snowex​-2022​.hackweek​.io​/tutorials​/aviris​-ng​/AVIRIS​-NG​_Tutorial​.html\n\nhttps://​www​.spectralpython​.net​/​#documentation","type":"content","url":"/notebooks/aviris-ng-data#references","position":13},{"hierarchy":{"lvl1":"ERA5"},"type":"lvl1","url":"/notebooks/era5-data-access","position":0},{"hierarchy":{"lvl1":"ERA5"},"content":"This is a script designed to obtain snow data from the ERA5 reanalysis product. We will be using the Copernicus API to get global, daily snow cover and snow depth information.\n\nThis code is adapted from Tasha Snow’s ERA5 downloading script: \n\nERADownload.ipynb\n\nThe Copernicus Climate Data Store (CDS) API is not on CryoCloud by default, so the following cell needs to be run, followed by restarting the kernel.\n\nTo use the CDS API, the user needs credentials to the Copernicus Climate Data Store (CDS). Upon getting a user ID (uid) and an API key (api-key), they need to run the following cell (skip if you already have ./cdsapirc in the /home/jovyan/ directory).\n\n# !echo url: https://cds.climate.copernicus.eu/api/v2 >> /home/jovyan/.cdsapirc\n# !echo key: {uid}:{api-key} >> /home/jovyan/.cdsapirc\n\nfrom ecmwfapi import ECMWFDataServer # Need a ecmwf user name and password first\nimport cdsapi\n\nThe CDS API can be a bit picky with inputs from ERA5, so first-time users are encouraged to use the online request form (\n\nhttps://​cds​.climate​.copernicus​.eu​/datasets​/reanalysis​-era5​-single​-levels​?tab​=​download) to automatically generate a code for their API request, to ensure that the syntax is correct.\n\nThe below functions retrieve ERA5 snow depth and snow density and download them to a tmp/ folder. Additional parameters to consider:\n\nyearStart and yearEnd: Start and end year.\n\nmonthStart and monthEnd: Start and end month.\n\ndayStart and dayEnd: Start and end day.\n\nThe function currently grabs daily data from March 1, 2020 - April 30, 2020 at 12:00 UTC each day, and downloads as daily netCDF files. Because ERA5 is generated hourly, users can expand the time entry to include more hours per day.\n\nfrom pathlib import Path\n\n# Initialize the CDS API\nc = cdsapi.Client()\n\ndef retrieve_era5():\n    \"\"\"      \n       A function to demonstrate how to iterate efficiently over several years and months etc    \n       for a particular ERA5 request.\n    \"\"\"\n    yearStart = 2020\n    yearEnd = 2020\n    monthStart = 3\n    monthEnd = 3\n    dayStart = 1\n    dayEnd = 31\n    for year in list(range(yearStart, yearEnd + 1)):\n        for month in list(range(monthStart, monthEnd + 1)):\n            for day in list(range(dayStart, dayEnd + 1)):\n                startDy = '%02d' % (day)\n                startMo = '%02d' % (month)\n                startYr = '%04d' % (year)\n                tmp_dir = Path.cwd() / \"tmp\"\n                tmp_dir.mkdir(exist_ok=True)\n                target = f\"{tmp_dir}/era5_SWE_{startYr}{startMo}{startDy}.nc\"\n                era5_request(startYr, startMo, startDy, target)\n\ndef era5_request(startYr, startMo, startDy, target):\n    \"\"\"      \n        Helper function for era5_retrieve. An ERA-5 request for snow\n        depth and snow cover data for the given years/months/days.\n\n        Inputs\n        ------------\n        startYr: str\n            Starting year of data query, in YYYY format.\n        startMo: str\n            Starting month of data query, in MM format.\n        startDy: str\n            Starting day of data query, in DD format.\n        target: str\n            Path and name of netCDF file to be saved.\n    \"\"\"\n    c.retrieve(\n    'reanalysis-era5-land',\n    {\n        'product_type':['reanalysis'],\n        'data_format':'netcdf',\n        'variable':['snow_depth', 'snow_cover'],\n        'year':[startYr],\n        'month':[startMo],\n        'day':[startDy],\n        'time':['12:00']\n    },\n    target)\n        \nif __name__ == '__main__':\n    retrieve_era5()\n\nDepending on the number of files downloaded (31 in the case of the above example), it can take a while to download everything.\n\nWhen it finishes, there should now be daily ERA5 data in netCDF format! To efficiently load all of this data, we are going to use Xarray and its open_mfdataset() function.\n\nimport os\nimport re\nimport zipfile\nimport xarray as xr\n\nfrom os import listdir\nfrom os.path import join\n\ndef process_era5_data(tmp_path):\n    # Find ERA5 Zip files in downloaded directory\n    era5_files = [join(tmp_path,f) for f in listdir(tmp_path) if \"era5_\" in join(tmp_path, f)]\n    \n    # Iteratively unzip each file and collect into a list\n    tmp_files = era5_extract(era5_files, tmp_dir)\n\n    print('------------')\n    # Open all ERA5 files into single Xarray\n    ds = xr.open_mfdataset(tmp_files)\n    print(\"All data has been lazy-loaded into Xarray.\")\n\n    # Remove extracted files, for cleanliness\n    for file in tmp_files:\n        os.remove(file)\n    print(\"Extracted ERA-5 files deleted.\")\n\n    return ds\n\ndef era5_extract(era5_files, tmp_dir):\n    for file in era5_files:\n        with zipfile.ZipFile(file, 'r') as zfile:\n            print(f'Now extracting data for file: {file}')\n            # Extract all files from current Zip file\n            zfile.extractall(tmp_dir)\n\n            # Rename output file to prevent overwriting\n            outfile = join(tmp_dir, \"data_0.nc\")\n            date_pattern = re.search(r'\\d{8}', file).group(0)\n            newfile = join(tmp_dir, f'data_{date_pattern}.nc')\n            os.rename(outfile, newfile)\n            print(f'Data extracted and saved to file: data_{date_pattern}.nc')\n            print(' ')\n\n    # List of output files\n    tmp_files = [join(tmp_dir,f) for f in os.listdir(tmp_dir) if \"data_\" in join(tmp_dir, f)]\n\n    return tmp_files\n\ntmp_dir = Path.cwd() / \"tmp\"\nds = process_era5_data(tmp_dir)\n\nds\n\nThanks to the above function, loading all of that data is pretty easy! However, it is important to note that the data is currently “lazy-loaded” - we can easily subset and resample the data for our needs, but we will need to load it into memory if we wish to make figures.\n\nFully loading the data as is can be time-consuming, so let’s reduce the data first, starting with making monthly means of snow depth.\n\n# Calculate monthly mean snow depth and snow cover\nera5_monthly = ds.resample(valid_time='1ME').mean()\n\nResampling to monthly means reduces the data volume by quite a bit, so let’s now look at global snow depth from the month of March. We will go ahead and load the result into memory using the compute() function.\n\n# Load March snow depths into memory\nera5_sd_march = era5_monthly['snowc'].compute().squeeze()\n\nFinally, we can make a map figure showing global, monthly-averaged snow depth from ERA5.\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nera5_sd_march.plot.imshow(ax=ax, cmap='Blues')\nax.set_xlabel(\"Longitude\", fontsize=12)\nax.set_ylabel(\"Latitude\", fontsize=12)\nax.set_title(\"ERA5 Snow Cover, March 2020\", fontsize=12)\nfig.tight_layout()\n\nNow for a different example. Here, we will examine snow depths over Alaska only, and generate a state-wide time series for the month of March.\n\n# Making bounds for Alaska\nmask_lon = (ds.longitude >= -168.75+360) & (ds.longitude <= -136.01+360)\nmask_lat = (ds.latitude >= 52.64) & (ds.latitude <= 71.59)\n\n# Subset ERA5 data to Alaska lats/lons only\nera5_alaska = ds.where(mask_lon & mask_lat, drop=True)\n\nAs before, we need to load the Alaska data into memory. Because we are looking over a much smaller spatial domain, compute() will be much faster.\n\n# Load Alaska data into memory\nera5_alaska = era5_alaska.compute().squeeze()\n\nAgain, we can make a map figure showing snow depth over the state of Alaska, this time for March 1, 2020:\n\n# Map plot of Alaska snow depths\nera5_alaska['snowc'].isel(valid_time=0).plot.imshow(vmin=0, vmax=1, cmap=\"Blues\")\n\nWe can also create a spatially-averaged time series of snow depth over the state of Alaska for the entire time period March 1 - April 30:\n\n# Calculate spatial average of snow depths over Alaska\nera5_sd_alaska = era5_alaska['snowc'].mean(('longitude', 'latitude'))\n\n# Time series plot of Alaska snow depths\nfig, ax = plt.subplots()\nera5_sd_alaska.plot(ax=ax)\nax.set_xlabel(\"Day\", fontsize=12)\nax.set_ylabel(\"Snow depth [m]\", fontsize=12)\nax.set_title(\"March 1 - April 30, 2020\", fontsize=12)\nfig.tight_layout()","type":"content","url":"/notebooks/era5-data-access","position":1},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2"},"type":"lvl1","url":"/notebooks/is2-snow-depth-workflow","position":0},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2"},"content":"This notebook uses a combination of ICESat-2 and airborne lidar to derive snow depth. It uses data from the SnowEx 2023 campaign as an example, but can be applied to other locations if a shapefile or geoJSON is given.\n\nThis notebook is adapted from the 2023 ICESat-2 Hackweek, originally developed by Zachary Fair and Karina Zikan.\n\n","type":"content","url":"/notebooks/is2-snow-depth-workflow","position":1},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"User input"},"type":"lvl2","url":"/notebooks/is2-snow-depth-workflow#user-input","position":2},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"User input"},"content":"Acceptable field site IDs over Alaska are:\n\ncffl: Creamer’s Field/Farmer’s Loop\n\ncpcrw: Caribou/Poker Creek Experimental Watershed\n\nbcef: Bonanza Creek Experimental Forest\n\nacp: Arctic Coastal Plain\n\nutk: Toolik Research Station\n\nAcceptable IDs for Sliderule ATL08 class (use numeric ID):\n\nNo classification: -1\n\natl08_unclassified: 0\n\natl08_noise: 1\n\natl08_canopy: 2\n\natl08_top_of_canopy: 3\n\natl08_ground: 4\n\n# Field site ID\nfield_id = 'C:/Users/zfair/OneDrive - NASA/Documents/Python/Projects/SnowPit/Workflows/cffl_lidar_box.geojson'\n\n# Snow-on (True) or snow-off (False) analysis\nsnow_on = True\n\n# Use March UAF data ('mar') or October depths ('oct')\nuaf_depths = 'mar'\n\n# Base data path\npath = '/home/jovyan/icesat2-snowex'\n\n# Desired RGT and date range for data queries. Set rgt to \"all\" if\n# all ground tracks are desired\ndate_range = ['2023-03-01', '2023-04-01']\nrgt = '1356'\n\n# SlideRule parameters (optional)\ncnf_surface = 4\natl08_class = 4\nsegment_length = 40\nres = 20\n\nA breakdown of the SlideRule parameters above:\n\ncnf_surface: The confidence level of the ICESat-2 photons.\n\nHigh-confidence photons (recommended for snow): 4\n\nHigh-/medium-confidence photons: 3\n\nHigh-/medium-/low-confidence photons: 2\n\nSignal photons (high/medium/low) and noise: 1\n\nSignal photons, noise, and solar background (not recommended): 0\n\nsegment_length: The along-track length to sample and aggregate photons, in meters. Currently set at 40 m, the resolution of the ATL06 product.\n\nres: The along-track resolution of the returned data product. Currently set at 20 m to match ATL06.\n\n","type":"content","url":"/notebooks/is2-snow-depth-workflow#user-input","position":3},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"Read ICESat-2 data"},"type":"lvl2","url":"/notebooks/is2-snow-depth-workflow#read-icesat-2-data","position":4},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"Read ICESat-2 data"},"content":"To load the ICESat-2 data with minimal effort from the user, we will use SlideRule in the below function.\n\nfrom sliderule import sliderule, icesat2\n\ndef atl06srq(field_geojson, date_range, rgt, cnf_surface, atl08_class, \n             segment_length, res):\n    # Initiate SlideRule\n    icesat2.init('slideruleearth.io', verbose=False)\n\n    # Load geoJSON for site of interest\n    region = sliderule.toregion(field_geojson)['poly']\n\n    # Convert user-defined ATL08 class ID to string readable by SlideRule\n    atl08_ids = {-1: 'None',\n                 0: 'atl08_unclassified',\n                 1: 'atl08_noise',\n                 2: 'atl08_canopy',\n                 3: 'atl08_top_of_canopy',\n                 4: 'atl08_ground'}\n\n    # Construct dictionary of parameters\n    time_root = 'T00:00:00Z'\n    parms = {\n             \"poly\": region,\n             \"srt\": icesat2.SRT_LAND,\n             \"cnf\": cnf_surface,\n             \"len\": segment_length,\n             \"res\": res,\n             \"t0\": date_range[0]+time_root,\n             \"t1\": date_range[1]+time_root\n            }\n\n    # Check if all RGTs are considered, or only a subset\n    if rgt != \"all\":\n        parms[\"rgt\"] = rgt\n        print(f\"Subsetting to only include ICESat-2 RGT {rgt}.\")\n\n    # Check for ATL08 filter\n    if atl08_ids.get(atl08_class) != \"None\":\n        parms[\"atl08_class\"] = atl08_ids.get(atl08_class)\n        print(\"Subsetting by selected ATL08 filter...\")\n\n    # Query SlideRule\n    atl06sr = icesat2.atl06p(parms)\n\n    return atl06sr\n\n# Generate ICESat-2 data from SlideRule\natl06sr = atl06srq(field_id, date_range, rgt,\n                 cnf_surface=cnf_surface,\n                 atl08_class=atl08_class,\n                 segment_length=segment_length,\n                 res=res)\n\n# Convert ATL06SR to geodataframe in EPSG:32606\natl06sr['lon'], atl06sr['lat'] = atl06sr.geometry.x, atl06sr.geometry.y\natl06sr_gdf = atl06sr.to_crs('epsg:32606')\n\natl06sr\n\n","type":"content","url":"/notebooks/is2-snow-depth-workflow#read-icesat-2-data","position":5},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"Read Airborne Lidar Data"},"type":"lvl2","url":"/notebooks/is2-snow-depth-workflow#read-airborne-lidar-data","position":6},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"Read Airborne Lidar Data"},"content":"To derive snow depth with ICESat-2, we need a snow-off digital elevation model (DEM), which commonly originates from airborne lidar. This next step is designed to load and prepare some airborne lidar data from the University of Alaska, Fairbanks for this analysis.\n\nThe method currently presented uses earthaccess to stream the data without any download required. However, this process can be slow on a local machine, and may be memory-intensive on a cloud environment. A more streamlined workflow utilizing the SnowEx Database is in the works, though the given method is most effective for non-SnowEx DEM sources.\n\nimport earthaccess\nimport xarray as xr\nearthaccess.login(strategy='interactive', persist=True)\nauth = earthaccess.login()\n\nregion = sliderule.toregion(field_id)['poly']\ncoords = [(point[\"lon\"], point[\"lat\"]) for point in region]\n\n# Coordinates for SW/NE corners\nlon_min = min([coord[0] for coord in coords])\nlat_min = min([coord[1] for coord in coords])\nlon_max = max([coord[0] for coord in coords])\nlat_max = max([coord[1] for coord in coords])\n\n# Data query for lidar snow depth over Fairbanks, AK\nresults = earthaccess.search_data(\n    short_name='SNEX23_Lidar',\n    bounding_box = (lon_min, lat_min, lon_max, lat_max),\n    temporal = ('2023-03-10', '2023-03-15')\n)\n\nfiles = earthaccess.open(results)\nlidar_snow_on = xr.open_dataset(files[1], engine='rasterio')\n\nlidar_snow_on\n\n# Data query for lidar snow-off elevation over Fairbanks, AK\nresults = earthaccess.search_data(\n    short_name='SNEX23_Lidar',\n    bounding_box = (lon_min, lat_min, lon_max, lat_max),\n    temporal = ('2022-05-20', '2022-05-31')\n)\n\n# Open the resulting GeoTiff into Xarray\nfiles = earthaccess.open(results)\nlidar_snow_off = xr.open_dataset(files[1], engine='rasterio')\n\n","type":"content","url":"/notebooks/is2-snow-depth-workflow#read-airborne-lidar-data","position":7},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"Co-Locate ICESat-2 and UAF Lidar"},"type":"lvl2","url":"/notebooks/is2-snow-depth-workflow#co-locate-icesat-2-and-uaf-lidar","position":8},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"Co-Locate ICESat-2 and UAF Lidar"},"content":"For this step, we will co-locate ICESat-2 and UAF so that we can directly compare the two datasets. The co-location will use a statistical method called “spline interpolation”, and we will perform this co-location with both the snow-on and snow-off data.\n\nThe below function has the code needed to perform the co-location.\n\n# Packages needed for the below functions\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import RectBivariateSpline\n\ndef colocate_is2(lidar_snow_off, lidar_snow_on, is2_data):\n    # Ensure lidar/ICESat-2 projections match\n    if is2_data.crs!=32606:\n        is2_data.to_crs(\"EPSG:32606\", inplace=True)\n    \n    # Define x/y coordinates from snow-off data\n    x0, y0 = np.array(lidar_snow_off.x), np.array(lidar_snow_off.y)\n\n    # Do the same, but for the snow depth data\n    xs, ys = np.array(lidar_snow_on.x), np.array(lidar_snow_on.y)\n\n    # Remove filler values that would mess up the interpolator\n    dem_heights = np.array(lidar_snow_off['band_data'].sel(band=1))[::-1,:]\n    dem_heights[np.isnan(dem_heights)] = -9999\n    dem_depths = np.array(lidar_snow_on['band_data'].sel(band=1))[::-1,:]\n    dem_depths[np.isnan(dem_depths)] = -9999\n\n    # Generate interpolator schemes\n    interp_height = RectBivariateSpline(np.array(y0)[::-1], \n                                        np.array(x0),\n                                        dem_heights)\n    interp_depth = RectBivariateSpline(np.array(ys)[::-1],\n                                       np.array(xs),\n                                       dem_depths)\n\n    # Use the spline interpolator to align the lidar with ICESat-2\n    is2_lidar_df = pd.DataFrame()\n    for beam in np.unique(is2_data['gt']):\n        # Subset ICESat-2 data by current beam\n        is2_tmp = is2_data.loc[is2_data['gt']==beam]\n\n        # ICESat-2 x/y coordinates\n        xn, yn = is2_tmp.geometry.x, is2_tmp.geometry.y\n\n        # Define indices within x/y bounds of DEM\n        i1 = (xn>np.min(x0)) & (xn<np.max(x0))\n        i1 &= (yn>np.min(y0)) & (yn<np.max(y0))\n\n        # Estimate lidar elevation and snow depth along ICESat-2 track\n        lidar_height = interp_height(yn[i1], xn[i1], grid=False)\n        lidar_depth = interp_depth(yn[i1], xn[i1], grid=False)\n\n        # Construct dataframe of ICESat-2 and lidar data\n        tmp = pd.DataFrame(data={'lat': is2_tmp['lat'][i1],\n                                 'lon': is2_tmp['lon'][i1],\n                                 'x': xn[i1],\n                                 'y': yn[i1],\n                                 'rgt': is2_tmp['rgt'][i1],\n                                 'beam': is2_tmp['gt'][i1],\n                                 'is2_height': is2_tmp['h_mean'][i1],\n                                 'n_fit_photons': is2_tmp['n_fit_photons'][i1],\n                                 'h_sigma': is2_tmp['h_sigma'][i1],\n                                 'dh_fit_dx': is2_tmp['dh_fit_dx'][i1],\n                                 'lidar_height': lidar_height,\n                                 'lidar_snow_depth': lidar_depth\n                                    }\n                              )\n        # Concatenate the co-located data into  final DataFrame\n        is2_lidar_df = pd.concat([is2_lidar_df, tmp])\n\n    return is2_lidar_df\n\n# Use the above function to co-locate the airborne lidar and ICESat-2\natl06sr_uaf = colocate_is2(lidar_snow_off, lidar_snow_on, atl06sr)\n\n# Estimate the ICESat-2 snow depth\natl06sr_uaf['is2_snow_depth'] = atl06sr_uaf['is2_height'] - atl06sr_uaf['lidar_height']\n\n# Convert final DataFrame in GeoDataFrame\natl06sr_uaf_gdf = gpd.GeoDataFrame(atl06sr_uaf,\n                                   geometry=gpd.points_from_xy(atl06sr_uaf.lon, atl06sr_uaf.lat),\n                                   crs='EPSG:4326')\n\natl06sr_uaf_gdf\n\nAn outline of the variables in our current GeoDataFrame:\n\nlat and lon: The latitude and longitude along the ICESat-2 track.\n\nx and y: The easting and northing along the ICESat-2 track, in projection EPSG:32606.\n\nrgt: The reference ground track number of the ICESat-2 track of interest.\n\nbeam: The ICESat-2 beam designation (gt1l, gt2r, etc.)\n\nis2_height: ICESat-2 height estimate at the given location.\n\nn_fit_photons: Number of ICESat-2 photons used to derive is2_height.\n\nh_sigma: Approximate uncertainty of is2_height.\n\ndh_fit_dx: A rough measure of surface slope along the ICESat-2 track.\n\nlidar_height: Lidar height estimate at the given location.\n\nlidar_snow_depth: Lidar snow depth estimate at the given location.\n\nis2_snow_depth: ICESat-2 snow depth estimate at the given location.\n\nThe key variables are is2_snow_depth and lidar_snow_depth for our comparisons. Several of the other variables, such as n_fit_photons and h_sigma, can be used to filter or process the data further, if desired.\n\nLet’s look at a simple comparison between the two depth products.\n\n# Remove sub-zero values\natl06sr_uaf_gdf = atl06sr_uaf_gdf[atl06sr_uaf_gdf['is2_snow_depth']>=0]\natl06sr_uaf_gdf = atl06sr_uaf_gdf[atl06sr_uaf_gdf['lidar_snow_depth']>=0]\n\nimport matplotlib.pyplot as plt\n\nsingle_beam = atl06sr_uaf_gdf[atl06sr_uaf_gdf['beam']==60]\n\n# Line plot of along-track snow depths\nfig, ax = plt.subplots(figsize=(9,6))\nsingle_beam.plot(kind='line', ax=ax, x='lat', y='is2_snow_depth',\n                     linewidth=3, label='ICESat-2')\nsingle_beam.plot(kind='line', ax=ax, x='lat', y='lidar_snow_depth',\n                     linewidth=1.5, color='orange', label='UAF lidar')\nax.set_xlabel('Latitude', fontsize=14)\nax.set_ylabel('Snow depth [m]', fontsize=14)\nax.set_ylim([0, 2])\nax.legend()\n\nWe can also calculate the difference in snow depth between ICESat-2 and UAF, then make a spatial plot using geopandas.explore().\n\n# Calculate snow depth bias\natl06sr_uaf_gdf['snow_depth_residual'] = atl06sr_uaf_gdf['is2_snow_depth'] - atl06sr_uaf_gdf['lidar_snow_depth']\n\n# Create a spatial plot of the snow depth bias\natl06sr_uaf_gdf.explore(column='snow_depth_residual', \n                        tiles='Esri.WorldImagery',\n                        cmap='viridis',\n                        vmin=-1.5, vmax=1.5)\n\nIf the data looks good, then we can save the final GeoDataFrame as a geoJSON.\n\n# Save the GeoDataFrame\natl06sr_uaf_gdf.to_file(f'{path}/is2_uaf_snow-depths.geojson',\n                        driver='GeoJSON')","type":"content","url":"/notebooks/is2-snow-depth-workflow#co-locate-icesat-2-and-uaf-lidar","position":9},{"hierarchy":{"lvl1":"MERRA-2"},"type":"lvl1","url":"/notebooks/merra2-data-access","position":0},{"hierarchy":{"lvl1":"MERRA-2"},"content":"This code is designed to access reanalysis data from the Modern-Era Retrospective analysis for Research and Applications, Version 2 (MERRA-2). MERRA-2 is useful for its global data record of various land surface variables, including snow cover and snow depth.\n\nIn this example notebook, we are accessing the snow depth product, which is found in the “1-Hourly, Time-Averaged, Single-Level, Assimilation, Land Surface Diagnostics” product (M2T1NXLND), found here: \n\nhttps://​disc​.gsfc​.nasa​.gov​/datasets​/M2T1NXLND​_5​.12​.4​/summary.\n\nimport xarray as xr\nimport earthaccess\nimport boto3\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport matplotlib.pyplot as plt\nimport warnings\nfrom IPython.display import display, Markdown\n\nBecause MERRA-2 is a reanalysis product by NASA, we can get the data through earthaccess. In addition to using a short_name for the data product of interest, earthaccess also allows one to use the dataset’s DOI for queries. The below query uses the DOI (10.5067/RKPHT8KC1Y1T) for M2T1NXLND.\n\n# Authenticate using Earthdata Login prerequisite files\nauth = earthaccess.login()\n\n# Search for the granule by DOI\nresults = earthaccess.search_data(\n    doi='10.5067/RKPHT8KC1Y1T',\n    temporal=(\"2022-03-01\", \"2022-03-31\"),\n)\n\nlen(results)\n\nThe queried MERRA-2 data is organized such that 1 file = 1 day, so we should expect 31 files to be loaded below.\n\n# Access the MERRA-2 file(s) from the cloud\nfn = earthaccess.open(results)\n\n# Open MERRA-2 data in Xarray (may be time-/memory-intensive if multiple files are queried)\nds = xr.open_mfdataset(fn)\n\nWith the above lines of code, we now have global land surface diagnostics for 744 time steps, or hourly over the month of March 2022.\n\nSince we are interested in snow depth, we will focus on the SNODP variable, which provides snow depth in meters.\n\nLet’s look at global snow depths from March 1, 2022:\n\n# Sample global plot of snow depth at a single time\nds['SNODP'][0,:,:].plot(vmin=0, vmax=1)\n\nWe can also subset by latitude and longitude to look over a region of interest (Alaska, for this example).\n\n# Making bounds for Alaska\nmask_lon = (ds.lon >= -168.75) & (ds.lon <= -136.01)\nmask_lat = (ds.lat >= 52.64) & (ds.lat <= 71.59)\n\n# Subset MERRA-2 data to Alaska lats/lons only\nds_ak = ds.where(mask_lon & mask_lat, drop=True)\n\n# Sample Alaska plot of snow depth at a single time\nds_ak['SNODP'][0,:,:].plot(vmin=0, vmax=1.25)\n\nFinally, let’s generate a time series of snow depth for the month of March 2022 near Fairbanks, AK.\n\n# Resample snow depths to daily means\nak_daily_mean = ds_ak.resample(time='D').mean()\n\n# Plot daily mean snow depth over a month near Fairbanks, AK\nak_daily_mean['SNODP'][:,23,34].plot() ","type":"content","url":"/notebooks/merra2-data-access","position":1},{"hierarchy":{"lvl1":"Microstructure"},"type":"lvl1","url":"/notebooks/microstructure-tutorial","position":0},{"hierarchy":{"lvl1":"Microstructure"},"content":"by Mike Durand, School of Earth Sciences and Byrd Polar & Climate Research Center \n\ndurand.8@osu.edu","type":"content","url":"/notebooks/microstructure-tutorial","position":1},{"hierarchy":{"lvl1":"Microstructure","lvl2":"Learning Objectives"},"type":"lvl2","url":"/notebooks/microstructure-tutorial#learning-objectives","position":2},{"hierarchy":{"lvl1":"Microstructure","lvl2":"Learning Objectives"},"content":"At the end of this tutorial you should be able to...\n\nExplain why microstructure is important for remote sensing\n\nDefine measures of microstructure, especially specific surface area\n\nAccess and visualize tree different microstructure measurements from SnowEx Grand Mesa 2020","type":"content","url":"/notebooks/microstructure-tutorial#learning-objectives","position":3},{"hierarchy":{"lvl1":"Microstructure","lvl2":"Acknowledgments"},"type":"lvl2","url":"/notebooks/microstructure-tutorial#acknowledgments","position":4},{"hierarchy":{"lvl1":"Microstructure","lvl2":"Acknowledgments"},"content":"Contributions from: Micah Johnson, Mike Durand, HP Marshall, Tate Meehan, Megan Mason, Scott Henderson. This relies heavily on the snowexsqul database and example scripts created by Micah Johnson.","type":"content","url":"/notebooks/microstructure-tutorial#acknowledgments","position":5},{"hierarchy":{"lvl1":"Microstructure","lvl2":"Caveats"},"type":"lvl2","url":"/notebooks/microstructure-tutorial#caveats","position":6},{"hierarchy":{"lvl1":"Microstructure","lvl2":"Caveats"},"content":"The integrating sphere and the SMP data are published at NSIDC; you can read the pages there for documentation etc. However the microCT data are not yet published; please contact Lauren Farnsworth (\n\nlauren​.b​.farnsworth@usace​.army​.mil) for questions on the CT data.","type":"content","url":"/notebooks/microstructure-tutorial#caveats","position":7},{"hierarchy":{"lvl1":"Microstructure","lvl2":"Fun Facts About Snow Microstructure"},"type":"lvl2","url":"/notebooks/microstructure-tutorial#fun-facts-about-snow-microstructure","position":8},{"hierarchy":{"lvl1":"Microstructure","lvl2":"Fun Facts About Snow Microstructure"},"content":"Snow microstructure plays a super important role in snow physics and snow remote sensing, so a lot of effort went towards measuring it in SnowEx 2020!\n\nThere are several different quantities that are used to measure snow microstructure, including “grain size”. Grain size measurements are challenging to make in a repeatable way, and are also challenging to relate to the physical quantities that control remote sensing measurements. In the last ~15 years or so, a lot of effort has gone into more objective ways to measure microstructure.\n\nSnow microstructure  governs response of remote sensing to snow cover for visible, near-infrared and high-frequency microwave wavelengths. See Figure 1, below, and read \n\n, for more information.\n\nSnow microstructure governs visible and near-infrared reflectance. This is figure 2 from\n\n\n\nRadar measurements such as those made by the Ku-band SWEARR instrument are also very sensitive to snow microstructure.\n\nModeled response of radar backscatter to SWE and single-scatter albedo (which in turn is a function of snow microstructure), based on a simple model suggested by\n\n\n\nSnow microstructure is super important to efforts to launch a Ku-band SAR to measure global snow water equivalent (SWE). An important area of research right now is exploring how to use estimates of microstructure (e.g. from snowpack evolution models) to improve SWE retrievals.\n\nSnow microstructure evolves through the season, and varies a lot with depth. Snow microstructure evolution is controlled by other snow properties, such as snow temperature, snow height and snow liquid water content. A really great resource on snow microstructure is Kelly Elder’s recent talks:\n\nSnow Metamorphism\n\nSnow Grain Identification\n\n","type":"content","url":"/notebooks/microstructure-tutorial#fun-facts-about-snow-microstructure","position":9},{"hierarchy":{"lvl1":"Microstructure","lvl2":"SnowEx Microstructure Measurement Background"},"type":"lvl2","url":"/notebooks/microstructure-tutorial#snowex-microstructure-measurement-background","position":10},{"hierarchy":{"lvl1":"Microstructure","lvl2":"SnowEx Microstructure Measurement Background"},"content":"","type":"content","url":"/notebooks/microstructure-tutorial#snowex-microstructure-measurement-background","position":11},{"hierarchy":{"lvl1":"Microstructure","lvl3":"Basic Microstructure Definitions","lvl2":"SnowEx Microstructure Measurement Background"},"type":"lvl3","url":"/notebooks/microstructure-tutorial#basic-microstructure-definitions","position":12},{"hierarchy":{"lvl1":"Microstructure","lvl3":"Basic Microstructure Definitions","lvl2":"SnowEx Microstructure Measurement Background"},"content":"Microstructure definitions take a bit of getting used to. It’s very easy to get confused. Specific surface area (SSA) is one of the most important quantity used to measure snow microstructure, so that’s the focus here. Note that SSA is not the be-all and end-all, so there’s a short table describing how to relate SSA to other quantities just below.  A couple of good reads on all of this is \n\n and \n\n.\n\nCoarse and fine snow microstructure revealed by microCT. The microCT snow renderings on the left are Figure 2 from\n\n. The colorbars indicate that fine-grained snow (a) has high SSA and low D eq , whereas coarse-grained snow (b) has low SSA and high D eq .\n\nUse Figure 3 above to ground these definitions: SSA is the surface area of the ice-air interface, normalized in some way. Confusingly, SSA is defined in a couple of different ways in the literature: sometimes, surface area within a particular volume of interest (VOI) is normalized by the mass of the ice in the VOI. As defined in this way, SSA has units of length squared per mass, usually expressed as m2/kg. Instead of normalizing by mass, SSA is sometimes defined by normalizing by the volume of the VOI (this is SSAv in \n\n), and sometimes by normalizing by the volume of the ice in the VOI (this is SSAi in \n\n, and q in \n\n). Here let’s just go with the first definition I mentioned:SSA = \\frac{\\text{Surface area of ice-air interface}}{\\text{Mass of ice}} \\quad\n\nSSA tends to take values between 5 and 150 m2/kg: fresh, fine-grained snow has high SSA, and coarse snow has low SSA. Because it takes a little while for SSA values to become intuitive, a useful derived metric is the equivalent grain diameter (Deq; note that this is identical to Dq in \n\n), which by definition is the diameter that a sphere would have if it had a particular value of SSA. This is a one-to-one relationship, so there are no assumptions involved.D_{eq} = \\frac{6}{SSA \\rho_i} \\quad\n\nRelationships of specific surface area to other metrics are given in this list if you’re curious but otherwise just skip past this bit\n\nSometimes people refer to the “optical grain diameter”, which is the same as Deq. The “optical” refers to \n\n, who showed that any snow with a particular SSA had similar (not identical) radiative transfer properties regardless of particle shape in the visible and near-infrared parts of the spectrum. But note the same is not true in the microwave spectrum.\n\nAutocorrelation length is usually one of two metrics that summarize the two-point microstructure autocorelation function of the three-dimensional ice-air matrix. Think of the probability that you change media (from ice to air or vice versa) as you move a certain distance within the snow microstructure. The length that defines the likelihood that you’ll change media is (an approximation of the correlation length). SSA is by definition (with almost no assumptions) equal to the slope of the autocorrelation function at the origin. But microwave scattering is controlled by correlations at longer lags. For more check out \n\n. The lack of closing the loop between SSA and correlation length is a significant issue when we have measurements of SSA and microwaves as we do in SnowEx.\n\nGeometric grain size is what we usually measure when we measure with a hand lens. You can try to relate it to SSA or corelation length, but it is not always possible, and will change with different observers.\n\nTime to stop this list but there are many other metrics as well.\n\n","type":"content","url":"/notebooks/microstructure-tutorial#basic-microstructure-definitions","position":13},{"hierarchy":{"lvl1":"Microstructure","lvl3":"Microstructure Instruments","lvl2":"SnowEx Microstructure Measurement Background"},"type":"lvl3","url":"/notebooks/microstructure-tutorial#microstructure-instruments","position":14},{"hierarchy":{"lvl1":"Microstructure","lvl3":"Microstructure Instruments","lvl2":"SnowEx Microstructure Measurement Background"},"content":"Now that we know what we’re trying to measure (SSA, or correlation length) how do we actually measure? let’s talk just about three techniques used in SnowEx 2020 Grand Mesa.\n\nLeft: Lauren Farnsworth transports microCT samples from field sites back to Grand Mesa Lodge in a cold storage container. Right: the microCT machine in the lab at CRREL.\n\nMicro-computed tomography (microCT) is the only laboratory-based method used here, and it is the gold standard, although it does still come with caveats. The idea of microCT is to remove a sample of snow from a snow pit face, and either cast it with a compound such as diethyl pthalate that is still a liquid at 0° C, or preserve the same at a very cold temperature. Then the sample is sent back to the laboratory, and bombared with x-rays, similar to how you get x-rays to see if a bone is broken at the doctor. For much more on microCT, check out \n\n. microCT can be used to extract a ton of information about snow microstructure, including SSA, correlation length and many others.\n\nLeft: Kehan Yang operates an IceCube unit at the Grand Mesa Lodge intercomparison snowpit. Top right: schematic showing the integrating sphere measurement principle, from\n\n. Bottom right: snow in the IceCube sampling container, from\n\n.\n\nIntegrating spheres are field-based and you make the measurements on samples extracted from the snowpit face. The principle of the measurement is based on firing a laser at the snow sample, within a special reflective hollow sphere, where one side is filled by the snow sample, and measuring how much of the laser is reflected at a sensor at a known geometry. For more information, check out \n\n.  Most integrating sphere measurements are either made by a commercial firm (A2 Photonics) known as the \n\nIceCube or a version constructed at the University of Sherbrooke known as the IRIS \n\n. These approaches are set up to measure SSA only. There were three of these at Grand Mesa - one of the Sherbrooke IRIS units, and two IceCubes, one from Finnish Meteorological Institute, and one from Ohio State University.\n\nLeft: Megan Mason operates the SMP. Right: Closeup of the SMP sensor tip.\n\nSnow micropenetrometers are also a field-based approach, but they do not require a snowpit, enabling far more observations to be made. Instead, an automated motor pushes a probe vertically downwards into the snowpack. The probe measures the force required to break snow microstructure, yielding a wealth of information. Snow density, specific surface area and correlation length can be retrieved; for background see \n\n and \n\n. The micropen effort at Grand Mesa was led by Boise State University. A key thing to be aware of is that differences in the various SMP instruments mean that the empirical relationship of \n\n will give quite poor results for the particular instrument used in SnowEx, as fully explained in \n\n.\n\nThese methods are not the only ways to measure microstructure! There are several others not mentioned here, but not used at Grand Mesa 2020. Ask if interested.","type":"content","url":"/notebooks/microstructure-tutorial#microstructure-instruments","position":15},{"hierarchy":{"lvl1":"Microstructure","lvl2":"SnowEx Microstructure Measurement Data Overview"},"type":"lvl2","url":"/notebooks/microstructure-tutorial#snowex-microstructure-measurement-data-overview","position":16},{"hierarchy":{"lvl1":"Microstructure","lvl2":"SnowEx Microstructure Measurement Data Overview"},"content":"Of the three methods described above, microCT is by far the most expensive and most time consuming. Samples have to be transported back to the laboratory and the processing time requires a microCT machine. Thus the fewest CT sapmles are taken.\n\nThe integrating spheres require a snowpit to be dug, so we have an intermediate number of them: ~100.\n\nThe micropen measurements are by far the fastest to make, so a cross pattern of SMP measurements was made on orthogonal directions intersecting at the snowpit. There are thousands of SMP profiles from Grand Mesa 2020.\n\n","type":"content","url":"/notebooks/microstructure-tutorial#snowex-microstructure-measurement-data-overview","position":17},{"hierarchy":{"lvl1":"Microstructure","lvl2":"Working with the data"},"type":"lvl2","url":"/notebooks/microstructure-tutorial#working-with-the-data","position":18},{"hierarchy":{"lvl1":"Microstructure","lvl2":"Working with the data"},"content":"We’re going to do two things! First, we’ll intercompare the three different integrating sphere instruments at four different pits where we had multiple instruments operating. We’d expect these data to be fairly self-consistent. Second, we’ll compare all three methods (integrating sphere, SMP and microCT) at a single pit where we had all of these measurements present. Here especially with the SMP we would expect to need to intercalibrate the data to match local conditions; so far SSA has only been fit to SMP force measurements in one study, and we should assume we’ll need a local calibration to get a tight fit.\n\n","type":"content","url":"/notebooks/microstructure-tutorial#working-with-the-data","position":19},{"hierarchy":{"lvl1":"Microstructure","lvl3":"0. Load needed modules","lvl2":"Working with the data"},"type":"lvl3","url":"/notebooks/microstructure-tutorial#id-0-load-needed-modules","position":20},{"hierarchy":{"lvl1":"Microstructure","lvl3":"0. Load needed modules","lvl2":"Working with the data"},"content":"\n\n# Modules needed to access snowexsql: SnowEx field data database\nfrom snowexsql.db import get_db\nfrom snowexsql.data import LayerData, PointData\nfrom snowexsql.conversions import points_to_geopandas, query_to_geopandas\n\n# Modules needed to work with data\nimport geoalchemy2.functions as gfunc\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#note - this cell does not return any output\n\n","type":"content","url":"/notebooks/microstructure-tutorial#id-0-load-needed-modules","position":21},{"hierarchy":{"lvl1":"Microstructure","lvl3":"1. Intercompare Integrating Sphere Datasets","lvl2":"Working with the data"},"type":"lvl3","url":"/notebooks/microstructure-tutorial#id-1-intercompare-integrating-sphere-datasets","position":22},{"hierarchy":{"lvl1":"Microstructure","lvl3":"1. Intercompare Integrating Sphere Datasets","lvl2":"Working with the data"},"content":"\n\nThere were three integrating spheres. The IRIS unit from University of Sherbrooke was operated by Celine Vargel. The IceCube unit from the Finnish Meteorological Institute was operated by Juha Lemmetyinen. And the IceCube unit from Ohio State was operated by Kehan Yang and Kate Hale. Carefully read the \n\ndocumentation page at NSIDC if you are interested in the data. If you are using the data for a project, please contact the authors and mention what you’re doing - they’ll appreciate it! Contact for SSA is Mike Durand (\n\ndurand.8@osu.edu).\n\nSee Micah’s tutorial on datasets for more on this! Won’t explain too much here\n\ndb_name = 'snow:hackweek@db.snowexdata.org/snowex'\nengine, session = get_db(db_name)\n\n# Grab all the equivalent diameter profiles\nq = session.query(LayerData).filter(LayerData.type == 'specific_surface_area')\ndf = query_to_geopandas(q, engine)\n\n# End our database session to avoid hanging transactions\nsession.close()\n\ndf.head() #check out the results of the query\n\nSince we want to intercompare integrating spheres, we need to isolate only the sites that actually had multiple integrating spheres measuring the same snow.\n\n# Grab all the sites with equivalent diameter data (set reduces a list to only its unique entries)\nsites = df['site_id'].unique()\n\n# Store all site names that have multiple SSA instruments\nmulti_instr_sites = []\ninstruments = []\n\nfor site in sites:\n\n    # Grab all the layers associated to this site\n    site_data = df.loc[df['site_id'] == site]\n\n    # Do a set on all the instruments used here\n    instruments_used = site_data['instrument'].unique()\n\n    if len(instruments_used) > 1:\n        multi_instr_sites.append(site)\n\n# Get a unqique list of SSA instruments that were colocated\ninstruments = df['instrument'].unique()\n\ninstruments #check out the list of instruments. note that the IceCube values are displayed as serial numbers\n\nFinally, plot all Integrating Sphere SSA profiles at all Multi-Integrating Sphere Sites\n\n# Setup the subplot for each site for each instrument\nfig, axes = plt.subplots(1, len(multi_instr_sites), figsize=(4*len(multi_instr_sites), 8))\n\n# Establish plot colors unique to the instrument\nc = ['k', 'm', 'c']\ncolors = {inst:c[i] for i,inst in enumerate(instruments)}\n\n# Loop over all the multi-instrument sites \nfor i, site in enumerate(multi_instr_sites):\n    \n    # Grab the plot for this site\n    ax = axes[i]\n    \n    # Loop over all the instruments at this site\n    for instr in instruments:\n\n        # Grab our profile by site and instrument\n        ind = df['site_id'] == site \n        ind2 = df['instrument'] == instr\n        profile = df.loc[ind & ind2].copy()\n\n        # Don't plot it unless there is data\n        if len(profile.index) > 0:\n            \n            # Sort by depth so samples that are take out of order won't mess up the plot\n            profile = profile.sort_values(by='depth')\n            \n            # Layer profiles are always stored as strings. \n            profile['value'] = profile['value'].astype(float)\n            \n            # Plot our profile\n            ax.plot(profile['value'], profile['depth'], colors[instr], label=instr)\n   \n    # Labeling and plot style choices\n    ax.legend()\n    ax.set_xlabel('SSA [m^2/kg]')\n    ax.set_ylabel('Height above snow-soil interface [cm]')\n    ax.set_title('Site {}'.format(site.upper()))\n    \n    # Set the x limits to show more detail\n    ax.set_xlim((8, 75))\n    \nplt.tight_layout()\nplt.show()\n\n\n","type":"content","url":"/notebooks/microstructure-tutorial#id-1-intercompare-integrating-sphere-datasets","position":23},{"hierarchy":{"lvl1":"Microstructure","lvl3":"2. Pull the snowmicropenetrometer data and compute SSA","lvl2":"Working with the data"},"type":"lvl3","url":"/notebooks/microstructure-tutorial#id-2-pull-the-snowmicropenetrometer-data-and-compute-ssa","position":24},{"hierarchy":{"lvl1":"Microstructure","lvl3":"2. Pull the snowmicropenetrometer data and compute SSA","lvl2":"Working with the data"},"content":"The next step is to grab some SMP data to compare to. We’re going to get the SMP at site 2N13, where we have a copule of SSA profiles from integrating spheres (as well as microCT data, to be looked at in the next step!).\n\nThe SMP measurements for SnowEx 2020 GrandMesa were all made by Megan Mason. If you’re interested in working with the SMP data, please carefully read the NSIDC \n\ndocumentation page. If you’re planning to work with the data, please reach out to the author; the contact is (Megan Mason \n\nmeganmason491@u​.boisestate​.edu). If you use a profile, consider checking out the comments which are described in the \n\nExcel sheet linked from the Technical References part of the NSIDC documentation, where there are some really useful comments.\n\nThere are a few steps here, and one reason for that is just that the SMP data is quite large, and so the full-resolution SMP could not be included in Micah’s database. The full resolution profile from SMP is resolved ever 1.25 mm! Instead, the SMP data in Micah’s database is sampled to only every 100th datapoint, so it’s every 12.5 cm. But the database is still very useful! What we’ll do is use the database to find the right profile, then go and download that full resolution dataset from the NSIDC. Easy-peasey!\n\nAs mentioned above, \n\n tested applying the relationship of \n\n and got quite poor results, explained them by the difference in hardware between generations of SMP instruments. We were unaware of that when designing the tutorial, and so set up use of the so-called official SMP processing repository, linked below, which has not yet been updated with the latest relationship. This would make a great project, as mentioned later!\n\nFirst up, we’ll visualize the location of the SMP profiles, along with the snowpit location.\n\nsite = '2N13'\nengine_smp, session_smp = get_db(db_name)\nq_smp = session_smp.query(LayerData).filter(LayerData.type == 'force').filter(LayerData.site_id.contains(site) )\ndf_smp = query_to_geopandas(q_smp, engine_smp)\n\nq_pit=session_smp.query(LayerData).filter(LayerData.type == 'hand_hardness').filter(LayerData.site_id.contains(site) )\ndf_pit = query_to_geopandas(q_pit, engine_smp)\n\nsession_smp.close()\n\n# Plot SMP profile locations with colored by the time they were taken using upside down triangles\nax = df_smp.plot(column='time', cmap='jet', marker='v', label='SMP', figsize=(5,5), markersize=100, edgecolor='black')\n\nax.plot(df_pit.easting, df_pit.northing, color='black', marker='s', markersize=15, label='Pit ({})'.format(site))\n\n# Add important labels\nax.set_xlabel('Easting [m]')\nax.set_ylabel('Northing [m]')\nplt.suptitle('SMP Locations at Site {} Showing Acquisition Order'.format(site), fontsize=16)\n\n# Avoid using Scientific notation for coords.\nax.ticklabel_format(style='plain', useOffset=False)\nax.legend()\n# plt.tight_layout()\nplt.show()\n\nNext up, let’s find the closest SMP profile to the snowpit, and then find the profile ID of that profile, which is in the comments in the database.\n\n# find closest SMP profile to the pit\n\n# No profile is taken at the same time, so we grab all the unique times and sort them\ntimes = sorted(df_smp['time'].unique())\n\nnprofiles=len(times)\n\nids=np.empty(nprofiles)\n\np=0\nfor t in times:\n    ind = df_smp['time'] == t\n    data = df_smp.loc[ind].copy()\n    ids[p]=data.iloc[0].id\n    p+=1\n    \ni_dists=df_smp['id'].isin(ids)\n\ndf_smp_dists=df_smp.loc[i_dists]\ndf_smp_dists=df_smp_dists.assign(dists=-1)\ndf_smp_dists['dists']=np.sqrt((df_smp_dists['easting']-df_pit.iloc[0].easting)**2+(df_smp_dists['northing']-df_pit.iloc[0].northing)**2)\n\n    \ndf_smp_dists.sort_values(by='dists')[['comments','dists']].head() #check out the list of profiles sorted by distance to pit\n\nSo the id of the closest SMP profile is S19M1174.  I went to the \n\nSMP page on NSIDC, and went to “Download” and searched for this ID, downloaded the profile, and then re-uploaded to my home directory here in the Jupyter hub.\n\nOk next up, we have to compute SSA from the SMP data. For this, we’ll use the “snowmicropyn” modules created by the Swiss SLF. You can read more about them \n\nat this site. The software is a little out of date on Python versions; just ignore any warnings that pop up below! Also, the use of the  \n\n relationship is also out-of-date, as mentioned above. Getting it updated for use with this tutorial would make a perfect project!\n\nThe next cell pulls in the needed modules, and then plots the profile of force measurements needed to break through the snow microstructure.\n\nfrom snowmicropyn import Profile\nfrom snowmicropyn import proksch2015\n\n%%bash \n\n# Retrieve a copy of data files used in this tutorial from Zenodo.org:\n# Re-running this cell will not re-download things if they already exist\n\nmkdir -p /tmp/tutorial-data\ncd /tmp/tutorial-data\nwget -q -nc -O data.zip https://zenodo.org/record/5504396/files/microstructure.zip\nunzip -q -n data.zip\nrm data.zip\n\np = Profile.load('/tmp/tutorial-data/microstructure/SMP/SNEX20_SMP_S19M1174_2N13_20200206.PNT',)\nplt.plot(p.samples.distance, p.samples.force)\n# Prettify our plot a bit\nplt.title(p.name)\nplt.ylabel('Force [N]')\nplt.xlabel('Depth [mm]')\nplt.show()\n\nThe above shows the entire SMP profile; this includes the part of the force profile that is above the snow surface, and thus needs to be removed, in order to apply calculations only to the snow (not the air!).\n\n#extract the part of the profile that is in the snow (i.e. remove air)\ndepth_surf=p.detect_surface()\ndepth_ground=p.detect_ground()\nsamples_snow=p.samples_within_distance(begin=depth_surf, end=depth_ground, relativize=False)\nsamples_snow.distance-=depth_surf\nplt.plot(samples_snow.distance, samples_snow.force)\nplt.title(p.name)\nplt.ylabel('Force [N]')\nplt.xlabel('Depth [mm]')\nplt.show()\n\nThe above part of the profile is just the part that is in the snow. PLEASE NOTE that the automated functions are not infallible, and need to be used with care. For now, these need to be compared back to the notes and interpreted manually.\n\nThe next step is the actual calculation of SSA from the force data. It then displays the data and lets you see that there is now a column called SSA! Note that this function is “proksch2015”. You can read about how it works in Martin Proksch’s paper \n\n.\n\n# call using the snowmicropyn library proksch2015\np2015 = proksch2015.calc(p.samples) \np2015.head() #check out the first few values of SSA\n\n","type":"content","url":"/notebooks/microstructure-tutorial#id-2-pull-the-snowmicropenetrometer-data-and-compute-ssa","position":25},{"hierarchy":{"lvl1":"Microstructure","lvl3":"3. Read microCT data, and compare integrating sphere, SMP and CT data","lvl2":"Working with the data"},"type":"lvl3","url":"/notebooks/microstructure-tutorial#id-3-read-microct-data-and-compare-integrating-sphere-smp-and-ct-data","position":26},{"hierarchy":{"lvl1":"Microstructure","lvl3":"3. Read microCT data, and compare integrating sphere, SMP and CT data","lvl2":"Working with the data"},"content":"\n\nThe microCT samples were extracted in the field and processed at CRREL by Lauren Farnsworth, and is not yet published at NSIDC. Please contact her with questions (\n\nlauren​.b​.farnsworth@usace​.army​.mil)!\n\nThis module reads in microCT datafiles which are stored as text. Some additional data are available, showing the computer generated slices through the ice-air interface: contact Mike (\n\ndurand.8@osu.edu) if you want to look at a subset of these data that Lauren has shared.\n\nEquivalent grain size is a useful quantity to compare: because it’s proportional to 1/SSA, and because after a point as you increase SSA more and more, all fine-grained snow acts more-or-less the same (converging to e.g. the “fine-grained” curve in Figure 1, above), we’ll look at equivalent diameter instead of SSA in this comparison.\n\nfrom read_CT_txt_files import read_CT_txt_files\n\n# read micro CT for 2N13\ndata_dir='/tmp/tutorial-data/microstructure/microCT/txt/'\n[SSA_CT,height_min,height_max]=read_CT_txt_files(data_dir)\n\nSSA_CT #check out the SSA values read in from MicroCT\n\n# get data integrating sphere data for 2N13 and plot it \nsite='2N13'\nengine_is, session_is = get_db(db_name)\nq_is = session_is.query(LayerData).filter(LayerData.type == 'specific_surface_area').filter(LayerData.site_id.contains(site) )\ndf_is = query_to_geopandas(q_is, engine_is)\ninstruments_site = df_is['instrument'].unique()\n\n# Loop over all the integrating sphere instruments at this site. plot equivalent diameter\nfig,ax = plt.subplots()\nfor instr in instruments_site:\n\n    # Grab our profile by site and instrument\n    ind = df['site_id'] == site \n    ind2 = df['instrument'] == instr\n    profile = df.loc[ind & ind2].copy()\n\n    # Don't plot it unless there is data\n    if len(profile.index) > 0:\n\n        # Sort by depth so samples that are take out of order won't mess up the plot\n        profile = profile.sort_values(by='depth')\n\n        # Layer profiles are always stored as strings. \n        profile['value'] = 6/917/profile['value'].astype(float)*1000\n\n        # Plot our profile\n        ax.plot(profile['value'], profile['depth'], colors[instr], label=instr)\n        \n#All that's left to do is plot the CT and the SMP and label the plot!\nax.plot(6/917/SSA_CT*1000,height_min,label='microCT')        #CT data\n\nax.plot(6/917/p2015.P2015_ssa*1000,(max(p2015.distance)-p2015.distance)/10,label='SMP') #SMP data\n\n# Labeling and plot style choices\nax.legend()\nax.set_xlabel('Equivalent diameter, mm')\nax.set_ylabel('Height above snow-soil interface [cm]')\nax.set_title('Site {}'.format(site.upper()))\n    \nplt.tight_layout()\nplt.show()\n\nWow, so the datasets are so very different, with the SMP being by far the most different. Comparing with \n\n shows that the SMP is off in the same direction as diagnosed in that paper. Thus, the difference is most likely due to the difference in SMP instruments. Dr. Mel Sandells of Northumbria University has a github branch of the SMP software SnowMicropyn that has the newer fit relationship integrated in the software. It would be a nice project to loop in Mel’s branch with this notebook and see how well things compare to SnowEx data. I’d be happy to help anyone interested get rolling on that!\n\nThere’s also significant differences between the microCT and the two integrating spheres. This is science - sometimes when we start intercomparing these quantities, we do not get a perfect match. This would also be a fascinating thing to explore in a Hackweek project.\n\nSome of the ways that you could imagine connecting microstructure measurements to other quantities would be with the SWESARR radar data. Although the radar data does seem to have some orthorectification issues that haven’t been fully worked out, I can imagine these being worked around by careful choice of places you match up the microstructure to the radar. Note that places that are shallower tend to have larger Deq and vice versa, and the spatial variability in SSA was fairly low in general in Grand Mesa 2020, so looking at multiple SSA vs radar samples might not yield a great correlation. But you never know, could be fun to try! Generally speaking, we don’t expect a ton of impact of the microstructure on L-band (UAVSAR), but it would be interesting to explore that.\n\nOne thing that could be of great value is to calibrate the SMP estimates of SSA to the integrating spheres. If you’re interested in doing that, do reach out first. This could be a really interesting thing to explore!\n\nIt might also be interesting to compare the data to hand hardness measured in the snowpit, and to traditional hand lens measurements.","type":"content","url":"/notebooks/microstructure-tutorial#id-3-read-microct-data-and-compare-integrating-sphere-smp-and-ct-data","position":27},{"hierarchy":{"lvl1":"Neural Networks with PyTorch"},"type":"lvl1","url":"/notebooks/pytorch-tutorial","position":0},{"hierarchy":{"lvl1":"Neural Networks with PyTorch"},"content":"This is a notebook designed to introduce users to machine learning using PyTorch and station data. The metloom package developed by M3Works is needed to run this script.\n\nThis notebook is adapted from a SnowEx Hackweek tutorial developed by Ibrahim Alabi. The full tutorial may be found here: \n\nhttps://​snowex​-2024​.hackweek​.io​/tutorials​/NN​_with​_Pytorch​/intro​.html\n\n","type":"content","url":"/notebooks/pytorch-tutorial","position":1},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl2":"What is Machine Learning?"},"type":"lvl2","url":"/notebooks/pytorch-tutorial#what-is-machine-learning","position":2},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl2":"What is Machine Learning?"},"content":"Machine learning (ML) is a field of artificial intelligence (AI) that focuses on developing algorithms or computer models using data. The goal is to use these “trained” compuer models to make decisions. Unlike traditional programming, where we write explicit rules for every situation, ML models learn patterns from data to perform tasks.\n\nImportant\n\nMachine learning is useful when the function (f) cannot be explicitly programmed, or when the relationship between the feature(s) and outcome is unknown.\n\n","type":"content","url":"/notebooks/pytorch-tutorial#what-is-machine-learning","position":3},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl3":"Data Download and Cleaning","lvl2":"What is Machine Learning?"},"type":"lvl3","url":"/notebooks/pytorch-tutorial#data-download-and-cleaning","position":4},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl3":"Data Download and Cleaning","lvl2":"What is Machine Learning?"},"content":"\n\nTo begin with this tutorial, we will download SNOTEL data using the metloom package. Users that don’t have it installed can run the cell below.\n\n!pip install -q metloom torch torchvision torchaudio\n\nFor a more detailed explanation on metloom, check out the tutorial on SNOTEL data access.\n\nIn this notebook, we will grab the following variables: SWE (SWE), average temperature (TEMPAVG), snow depth (SNOWDEPTH), and precipitation (PRECIPITATION). These variables will be obtained from the SNOTEL station at Green Lake, WA.\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# Define variables of interest\nALLOWED_VARIABLES = [\n    SnotelPointData.ALLOWED_VARIABLES.SWE,\n    SnotelPointData.ALLOWED_VARIABLES.TEMPAVG,\n    SnotelPointData.ALLOWED_VARIABLES.SNOWDEPTH,\n    SnotelPointData.ALLOWED_VARIABLES.PRECIPITATION,\n]\n\n# Define SNOTEL station\nsnotel_point = SnotelPointData(station_id=\"502:WA:SNTL\", name=\"Green Lake\")\n\n# Get daily SNOTEL data from Green Lake, WA\ndata = snotel_point.get_daily_data(\n    start_date=datetime(*(2010, 1, 1)),\n    end_date=datetime(*(2023, 1, 1)),\n    variables=ALLOWED_VARIABLES\n)\n\ndata.head()\n\nLet’s take a look at the data that we just collected.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Reset the index of the DataFrame for plotting purposes\nfor_plotting=data.reset_index()\n\n# Define the units for each variable\nunits={\n    \"SWE\": \"in\",\n    \"SNOWDEPTH\": \"in\",\n    \"AVG AIR TEMP\": \"degF\",\n    \"PRECIPITATION\": \"in\"\n}\n\n# List the variables for plotting\nvariables_to_plot = [\n    \"SWE\", \"SNOWDEPTH\", \"AVG AIR TEMP\", \"PRECIPITATION\"\n]\n\n# Make the plot\nplt.figure(figsize=(12, 8))\n\nfor variable in variables_to_plot:\n    plt.subplot(2, 2, variables_to_plot.index(variable) + 1)\n    plt.plot(for_plotting[\"datetime\"], for_plotting[variable], label=variable)\n    plt.ylabel(f\"{variable} ({units[variable]})\", fontsize=14)\n    plt.xlabel(\"Date\", fontsize=14)\n\nplt.tight_layout()\nplt.show()\n\nWe will now process this data so it’s easier to work with. First, we convert from imperial to metric units for easier interpretation. We then set the dates as the index, so that we can derive weekly rolling averages of precipitation and tempoerature.\n\nThe dataframe is then cleaned so that only snow depth, SWE, weekly temperature, and weekly precipitation are left. The dataframe is then filtered  for any NaN values, and for any zero/unrealistic snow depth and SWE values. Finally, we derive snow density from the SWE and depth data.\n\nclean_df=(\n    for_plotting\n    .assign(\n        swe=lambda x: x.SWE.map(lambda y: y*2.54 if y is not None else None),\n        snowdepth=lambda x: x.SNOWDEPTH.map(lambda y: y*2.54 if y is not None else None),\n        precipitation=lambda x: x.PRECIPITATION.map(lambda y: y*2.54 if y is not None else None),\n        tempavg=lambda x: x['AVG AIR TEMP'].map(lambda y: (y-32)*5/9 if y is not None else None)\n    )\n    .set_index('datetime')\n    .assign(\n        precip_7_days_avg=lambda x: x.precipitation.shift().rolling(window=\"7D\", min_periods=7).mean(),\n        tempavg_7_days_avg=lambda x: x.tempavg.shift().rolling(window=\"7D\", min_periods=7).mean(),\n    )\n    .filter([\"datetime\", \"swe\", \"snowdepth\", \"tempavg_7_days_avg\", \"precip_7_days_avg\"])\n    .dropna()\n    .query(\n        \"snowdepth != 0 and swe != 0 and \"\n        \"snowdepth > 5 and swe > 3\"\n    )\n    .assign(snowdensity=lambda x: x.swe / x.snowdepth)\n)\n\nclean_df\n\n","type":"content","url":"/notebooks/pytorch-tutorial#data-download-and-cleaning","position":5},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl2":"Building and Training a Neural Network"},"type":"lvl2","url":"/notebooks/pytorch-tutorial#building-and-training-a-neural-network","position":6},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl2":"Building and Training a Neural Network"},"content":"\n\nNow that we have SNOTEL data configured in a desirable format, we can build a simple neural network using PyTorch.\n\n# Basic analysis libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# PyTorch libraries\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\n# Find the best available computing resource\navailable_device = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\n\nprint(f\"Available device: {available_device}\")\n\nThe above cell identifies the most appropriate computing resource, based on what is available. If NVIDIA GPU or Apple’s Metal Performance Shaders are available, then they are prioritized. Otherwise, the code defaults to CPU. The former options offer faster GPU support, though the CPU option is universally available.\n\n","type":"content","url":"/notebooks/pytorch-tutorial#building-and-training-a-neural-network","position":7},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl4":"Data Splitting","lvl2":"Building and Training a Neural Network"},"type":"lvl4","url":"/notebooks/pytorch-tutorial#data-splitting","position":8},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl4":"Data Splitting","lvl2":"Building and Training a Neural Network"},"content":"With our current SNOTEL data, we are going to split it into training, validation, and testing sets. A typical split is 70% training data, 15% validation data, and 15% testing data.\n\n# Non-snow density data\nfeatures = clean_df.drop('snowdensity', axis=1).values\n\n# Snow density data only\ntargets = clean_df['snowdensity'].values\n\n# Split the data into training and temporary sets (70% training, 30% temporary)\nfeatures_train,features_temp,targets_train,targets_temp = train_test_split(features, targets, \n                                                                           test_size=0.3, random_state=0)\n\n# Further split the temp set into validation and test sets (15% each)\nfeatures_val,features_test,targets_val,targets_test = train_test_split(features_temp, targets_temp, \n                                                                       test_size=0.5, random_state=0)\n\nHere is a breakdown of the above cell:\n\nWe identify our “features”, or datasets that will be used to predict snow density. These include SWE, snow depth, temperature, and precipitation.\n\nWe identify our “target”, which is snow density in this example.\n\nThe data is split into model training data (features_train, targets_train) and validation/testing data (features_temp, targets_temp).\n\nThe temporary data sets are further split in half to validation data (features_val, targets_val) and test data (features_test, targets_test).\n\nIn both splitting operations, we also set random_state=0. This means that the same training/validation/testing split occurs every time the code is run, to ensure reproducibility.\n\n","type":"content","url":"/notebooks/pytorch-tutorial#data-splitting","position":9},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl4":"Data Scaling","lvl2":"Building and Training a Neural Network"},"type":"lvl4","url":"/notebooks/pytorch-tutorial#data-scaling","position":10},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl4":"Data Scaling","lvl2":"Building and Training a Neural Network"},"content":"Now that we’ve split the data, we can apply scaling. The scaler should be fit on the training data and then used to transform the training, validation, and test sets.\n\n# Generate scaler\nscaler = StandardScaler()\n\n# Fit scaler to training data\nscaler.fit(features_train)\n\n# Transform the training, validation, and test sets\nfeatures_train = scaler.transform(features_train)\nfeatures_val = scaler.transform(features_val)\nfeatures_test = scaler.transform(features_test)\n\n","type":"content","url":"/notebooks/pytorch-tutorial#data-scaling","position":11},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl4":"Create Dataset Classes","lvl2":"Building and Training a Neural Network"},"type":"lvl4","url":"/notebooks/pytorch-tutorial#create-dataset-classes","position":12},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl4":"Create Dataset Classes","lvl2":"Building and Training a Neural Network"},"content":"Next, we define custom Dataset classes for each of the three sets: training, validation, and testing.\n\n# Create class for available data\nclass SNOTELDataset(Dataset):\n    def __init__(self, data, targets):\n        self.data = torch.tensor(data, dtype=torch.float32)\n        self.targets = torch.tensor(targets, dtype=torch.float32).view(-1, 1)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        target = self.targets[idx]\n        return sample, target\n\n# Create instances of the custom datasets for training, validation, and testing sets\ntrain_dataset = SNOTELDataset(data=features_train, targets=targets_train)\nval_dataset = SNOTELDataset(data=features_val, targets=targets_val)\ntest_dataset = SNOTELDataset(data=features_test, targets=targets_test)\n\nNow, we use DataLoader to manage our data in mini-batches during training, validation, and testing.\n\n# Create DataLoaders for training, validation, and testing sets\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n","type":"content","url":"/notebooks/pytorch-tutorial#create-dataset-classes","position":13},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl3":"Define the neural network","lvl2":"Building and Training a Neural Network"},"type":"lvl3","url":"/notebooks/pytorch-tutorial#define-the-neural-network","position":14},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl3":"Define the neural network","lvl2":"Building and Training a Neural Network"},"content":"To set up our model, we begin by generating a simple feedforward neural network using torch.nn.Module.\n\n# Define new class for neural network\nclass SNOTELNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SNOTELNN, self).__init__() # super class to inherit from nn.Module\n        # Define the layers\n        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer 1\n        self.relu = nn.ReLU()  # ReLU activation function\n        self.fc2 = nn.Linear(hidden_size, output_size)  # Fully connected layer 2\n    \n    def forward(self, x): # x is the batch of input\n        # Define the forward pass\n        out = self.fc1(x)  # Pass input through first layer\n        out = self.relu(out)  # Apply ReLU activation\n        out = self.fc2(out)  # Pass through second layer to get output\n        return out\n\n# Instantiate the model and move it to the device (GPU or CPU)\nmodel = SNOTELNN(input_size=features_train.shape[1], hidden_size=128, output_size=1).to(available_device)\n\nThe forward method defines how the input data flows through the network layers. It specifies the sequence of operations that the data undergoes as it moves from the input layer to the output layer. This method is automatically called when you pass data through the model (e.g., outputs = model(inputs)).\n\nFor any ML application, we need to define a loss function and an optimizer. Here, we’ll use Mean Squared Error Loss since we’re dealing with a regression problem. We’ll use the Adam optimizer, which is a good default choice due to its adaptive learning rates.\n\n# Loss function\ncriterion = nn.MSELoss()\n\n# Optimizer function\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\n","type":"content","url":"/notebooks/pytorch-tutorial#define-the-neural-network","position":15},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl3":"Training the Neural Network","lvl2":"Building and Training a Neural Network"},"type":"lvl3","url":"/notebooks/pytorch-tutorial#training-the-neural-network","position":16},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl3":"Training the Neural Network","lvl2":"Building and Training a Neural Network"},"content":"We now write the training loop, which includes zeroing the gradients, performing the forward pass, computing the loss, backpropagating, and updating the model parameters. We will also validate the model on the validation set after each epoch.\n\nNote\n\nAn Epoch refers to one complete pass through the entire training dataset. During each epoch, the model sees every example in the dataset once.\n\nnum_epochs = 5\n\n# Lists to store the training and validation losses\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    train_loss = 0.0  # Initialize cumulative training loss\n    \n    for inputs, labels in train_loader:\n        # Move data to the appropriate device\n        inputs, labels = inputs.to(available_device), labels.to(available_device)\n        \n        # Zero the gradients from the previous iteration\n        optimizer.zero_grad()\n        \n        # Perform forward pass\n        outputs = model(inputs)\n        \n        # Compute the loss\n        loss = criterion(outputs, labels)\n        \n        # Perform backward pass (compute gradients)\n        loss.backward()\n        \n        # Update the model parameters\n        optimizer.step()\n        \n        # Accumulate training loss\n        train_loss += loss.item()\n    \n    # Average training loss\n    train_loss /= len(train_loader)\n    train_losses.append(train_loss)  # Store the training loss for this epoch\n    \n    # Validation phase\n    model.eval()  # Set model to evaluation mode\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(available_device), labels.to(available_device)  # Move to device\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n    \n    # Average validation loss\n    val_loss /= len(val_loader)\n    val_losses.append(val_loss)  # Store the validation loss for this epoch\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n\nLet’s check out the loss from both the training data and the validation data.\n\n# Plotting the training and validation losses\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\nplt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Over Epochs')\nplt.legend()\nplt.show()\n\n","type":"content","url":"/notebooks/pytorch-tutorial#training-the-neural-network","position":17},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl3":"Testing the Model","lvl2":"Building and Training a Neural Network"},"type":"lvl3","url":"/notebooks/pytorch-tutorial#testing-the-model","position":18},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl3":"Testing the Model","lvl2":"Building and Training a Neural Network"},"content":"We’ve done a lot of work to prepare the machine learning model for our applications. Now, it’s finally time to test it against our observations.\n\n# Evaluate the model on the test set and collect predictions\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0  # Initialize cumulative test loss\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():  # Disable gradient computation for inference\n    for inputs, labels in test_loader:\n        # Move data to the appropriate device\n        inputs, labels = inputs.to(available_device), labels.to(available_device)\n        \n        # Perform forward pass\n        outputs = model(inputs)\n        \n        # Compute the loss\n        loss = criterion(outputs, labels)\n        \n        # Accumulate test loss\n        test_loss += loss.item()\n        \n        # Store the predictions and the corresponding labels\n        all_preds.extend(outputs.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Calculate the average test loss\ntest_loss /= len(test_loader)\nprint(f'Test Loss: {test_loss:.4f}')\n\n# Convert lists to numpy arrays for plotting\nall_preds = np.array(all_preds)\nall_labels = np.array(all_labels)\n\n# Plot observed vs predicted\nplt.figure(figsize=(8, 8))\nplt.scatter(all_labels, all_preds, alpha=0.7)\nplt.xlabel('Observed (Actual) Values')\nplt.ylabel('Predicted Values')\nplt.title('Observed vs. Predicted Values')\nplt.grid(True)\nplt.show()\n\n","type":"content","url":"/notebooks/pytorch-tutorial#testing-the-model","position":19},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl3":"Save the Model","lvl2":"Building and Training a Neural Network"},"type":"lvl3","url":"/notebooks/pytorch-tutorial#save-the-model","position":20},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl3":"Save the Model","lvl2":"Building and Training a Neural Network"},"content":"We now have some pretty good results for predicting snow density. Now it is essential to save our trained model, as doing so allows you to reuse the model for predictions, further training, or sharing with others without having to retrain it from scratch. In PyTorch, saving and loading models is straightforward and can be done using the torch.save and torch.load functions.\n\n# Save the model's state dictionary\ntorch.save(model.state_dict(), 'snotel_nn_model.pth')\n\n\n# Initialize the model architecture\nmodel = SNOTELNN(input_size=features_train.shape[1], hidden_size=128, output_size=1)\n\n# Load the model's state dictionary\nmodel.load_state_dict(torch.load('snotel_nn_model.pth', weights_only=True))\n\n# Set the model to evaluation mode before inference\nmodel.eval()\n\n","type":"content","url":"/notebooks/pytorch-tutorial#save-the-model","position":21},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl3":"Further Information","lvl2":"Building and Training a Neural Network"},"type":"lvl3","url":"/notebooks/pytorch-tutorial#further-information","position":22},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl3":"Further Information","lvl2":"Building and Training a Neural Network"},"content":"","type":"content","url":"/notebooks/pytorch-tutorial#further-information","position":23},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl4":"Hyperparameter Tuning","lvl3":"Further Information","lvl2":"Building and Training a Neural Network"},"type":"lvl4","url":"/notebooks/pytorch-tutorial#hyperparameter-tuning","position":24},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl4":"Hyperparameter Tuning","lvl3":"Further Information","lvl2":"Building and Training a Neural Network"},"content":"Hyperparameter tuning is a critical step in building machine learning models. Unlike model parameters (like weights and biases), which are learned from the data during training, hyperparameters are the settings you choose before the training process begins. These include:\n\nLearning Rate: Controls how much to adjust the model’s weights with respect to the loss gradient.\n\nBatch Size: Determines the number of training examples utilized in one iteration.\n\nNumber of Hidden Layers and Neurons: Specifies the architecture of the neural network.\n\nOptimizer: The algorithm used to update model weights based on the computed gradients (e.g., Adam, SGD).\n\nTuning these hyperparameters can significantly affect the performance of your model. However, finding the optimal set of hyperparameters can be a challenging and time-consuming process, often requiring experimentation.","type":"content","url":"/notebooks/pytorch-tutorial#hyperparameter-tuning","position":25},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl4":"Manual vs. Automated Tuning","lvl3":"Further Information","lvl2":"Building and Training a Neural Network"},"type":"lvl4","url":"/notebooks/pytorch-tutorial#manual-vs-automated-tuning","position":26},{"hierarchy":{"lvl1":"Neural Networks with PyTorch","lvl4":"Manual vs. Automated Tuning","lvl3":"Further Information","lvl2":"Building and Training a Neural Network"},"content":"Manual Tuning: Involves adjusting hyperparameters based on intuition, experience, or trial and error. While straightforward, this approach can be inefficient and might not always yield the best results.\n\nAutomated Tuning: Tools like Optuna can help automate the search for the best hyperparameters. These tools explore the hyperparameter space more systematically and can save a lot of time compared to manual tuning.","type":"content","url":"/notebooks/pytorch-tutorial#manual-vs-automated-tuning","position":27},{"hierarchy":{"lvl1":"SNOTEL Data Access"},"type":"lvl1","url":"/notebooks/snotel-data-access","position":0},{"hierarchy":{"lvl1":"SNOTEL Data Access"},"content":"This notebook allows for easy access to snow depths and SWE from the Snow Telemetry (SNOTEL) network. A simple example is used to show quick access to SNOTEL data over Creamer’s Field, AK using the metloom package.\n\nCredit: M3Works for the metloom package, which can be found here: \n\nhttps://​github​.com​/M3Works​/metloom​/tree​/main\n\n# Install metloom, if not added yet\n!pip install metloom\n\nfrom datetime import datetime\nimport pandas as pd\nimport geopandas as gpd\nfrom pathlib import Path\n\nMetloom allows for easy access to several weather station types, including SNOTEL, MesoWest, and NorwayMet. The primary query function for each is SnotelPointData (replace “Snotel” with station of choice), which also allows us to view the locations of weather stations. We’ll start this example doing just that.\n\n# Import the SNOTEL pointdata classes\nfrom metloom.pointdata import SnotelPointData\n\n# Import the SNOTEL variable classes\nfrom metloom.variables import SnotelVariables\n\nSnotelPointData.ALLOWED_VARIABLES.\n\nWe are going to look for active SNOTEL stations near Creamer’s Field in Fairbanks, AK. The below cells search for SNOTEL stations within 0.5 degrees latitude/longitude of the provided polygon.\n\nWe will also go ahead and define the variables we want from the station, using SnotelPointData.ALLOWED_VARIABLES. Some of the allowed variables include:\n\nSNOWDEPTH: Snow depth, typically in inches.\n\nSWE: Snow water equivalent, typically in inches.\n\nPRECIPITATION: Accumulated precipitation, in inches.\n\nTMP: Air temperature, in degrees Fahrenheit.\n\n# Load FLCF lidar box from SnowEx campaigns\nsf_path = Path(\"/home/jovyan/shared-public/SnowPit/cffl_lidar_box.geojson\").expanduser()\nsf = gpd.read_file(str(sf_path))\nsf[\"name\"] = [\"FLCF\"]\n\n# Load the desired variables for SNOTEL query\nvariables = [SnotelPointData.ALLOWED_VARIABLES.SNOWDEPTH]\n\n# Find SNOTEL stations within polygon with desired variables\npoints = SnotelPointData.points_from_geometry(sf, variables)\n\n# Print nearby SNOTEL stations within 0.5 degrees of polygon\nprint(SnotelPointData.points_from_geometry(sf, variables, buffer=0.5).points)\n\nLooks like we have a SNOTEL station here! Note the printed output: Metloom returns the station ID number (1302), the state it’s in (AK), and the type of weather station (SNTL).\n\nLet’s see where it’s located in Creamer’s Field, relative to the polyon we provided.\n\n# Plot lidar box over ESRI tiles\nm = sf.explore(\n    tooltip=False, color=\"grey\", highlight=False, tiles=\"Esri.WorldImagery\",\n    style_kwds={\"opacity\": 0.2}, popup=[\"name\"]\n)\n# Add plot showing location of SNOTEL station(s)\ndf = points.to_dataframe()\ndf.explore(m=m, tooltip=[\"name\", \"id\"], color=\"red\", marker_kwds={\"radius\":4})\n\nNow that we know which SNOTEL is in the area, we can query for the data.\n\n# Define SNOTEL station from FLCF\npt = SnotelPointData(\"1302:AK:SNTL\", \"Creamer's Field\")\n\nAs with other API requests, we can subset the data with a date range, given as datetime objects.\n\nNote here that we are requesting for snow depths on a daily basis. If desired, we could also obtain the hourly data instead, using pt.get_hourly_data().\n\n# Start and end date of SNOTEL query\nstart_date = datetime(2022, 3, 1)\nend_date = datetime(2023, 4, 1)\n\n# Query SNOTEL snow depths\ndf = pt.get_daily_data(start_date, end_date, variables)\n\ndf.head()\n\nEasy enough! We now have a data frame containing the basic information of the SNOTEL site, as well as the snow depth in inches.\n\nSince inches aren’t very useful in scientific analysis, and SNOWDEPTH can be a hassle to type out, let’s make a new column that shows the depth in meters.\n\n# Convert snow depth to meters\ndf['snow_depth_meters'] = df['SNOWDEPTH']*0.0254\n\nFrom there, it’s simple to plot the snow depth data as a time series.\n\nimport matplotlib.pyplot as plt\n\n# Plot time series of daily SNOTEL data\nfig, ax = plt.subplots()\ndf.reset_index().set_index(\"datetime\")[\"snow_depth_meters\"].plot(ax=ax)\nax.set_xlabel(\"Date\", fontsize=12)\nax.set_ylabel(\"Snow depth [m]\", fontsize=12)\nax.set_title(\"SNOTEL: Creamer's Field (1302)\", fontsize=12)\nfig.tight_layout()","type":"content","url":"/notebooks/snotel-data-access","position":1},{"hierarchy":{"lvl1":"Snow Pit Data Access and SWE Calculation"},"type":"lvl1","url":"/notebooks/snow-pit-data-access","position":0},{"hierarchy":{"lvl1":"Snow Pit Data Access and SWE Calculation"},"content":"This notebook is designed to access data from snow pits gathered during the SnowEx field campaigns. There are two embedded examples: a simple use case with earthaccess and snow depth data, and a more advanced example using the SnowEx database (snowexsql) to obtain snow depth and density.\n\nThe snowexsql example uses code from a SnowEx Database example found here: \n\nhttps://​snowexsql​.readthedocs​.io​/en​/latest​/gallery​/plot​_pit​_swe​_example​.html. Additional thanks goes to Anthony Arendt and Joachim Meyer for their insights on proper syntax with the code.\n\nThis notebook uses the contextily and cmcrameri packages for map plotting and colorblind-friendly coloring, respectively. Use the below cell to install both, and to ensure that snowexsql is up-to-date.\n\n!pip install contextily cmcrameri\n!pip install snowexsql -U\n\nimport earthaccess\nimport pandas as pd\nimport geopandas as gpd\nimport os\nimport tempfile\nfrom shapely.geometry import Point\nimport shutil\nimport cmcrameri.cm as cmc\nimport matplotlib.pyplot as plt\nimport contextily as ctx\n\n","type":"content","url":"/notebooks/snow-pit-data-access","position":1},{"hierarchy":{"lvl1":"Snow Pit Data Access and SWE Calculation","lvl2":"earthaccess example"},"type":"lvl2","url":"/notebooks/snow-pit-data-access#earthaccess-example","position":2},{"hierarchy":{"lvl1":"Snow Pit Data Access and SWE Calculation","lvl2":"earthaccess example"},"content":"For the earthaccess example, we are using the DOI of the “SnowEx20 Community Snow Depth Probe Measurements, Version 1” obtained at Grand Mesa, CO. These files are stored in CSV format, and contain snow depths using magnaprobes, Mesa2 tablets, and pit rulers.\n\nNote that the short_name of the dataset is provided in the cell. This may be used as an alternative to the DOI, if desired.\n\n# Authenticate with Earthdata Login servers\nauth = earthaccess.login(strategy=\"interactive\")\n\n# Search for granules\nresults = earthaccess.search_data(\n    #short_name=\"SNEX20_SD\",\n    doi = \"10.5067/9IA978JIACAR\",\n    temporal=('2020-01-01', '2020-03-01'),\n)\n\ndisplay(results[0])\n\nOur query returned a single CSV file over the time frame of interest. Note how we did not include a spatial bound for the data here - this is because the dataset of interest only gathered data over Grand Mesa, CO.\n\nTo obtain the data, we will create a temporary directory (tempfile.mkdtemp()), and load the data into a GeoDataFrame. The temporary directory (and data within) will be deleted after processing.\n\n# Create a temporary directory for downloads\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Using temporary directory: {temp_dir}\")\n\n# Download the data to the temp directory\ndownloaded_files = earthaccess.download(\n    results,\n    local_path=temp_dir,\n)\nprint(f\"Downloaded {len(downloaded_files)} files to {temp_dir}\")\n\n# Process CSV files and convert to GeoDataFrame\ngdf = gpd.GeoDataFrame()\ncsv_files = [file for file in downloaded_files if file.endswith('.csv')]\nif csv_files:\n    for i, csv_file in enumerate(csv_files):\n        print(f\"Processing: {os.path.basename(csv_file)}\")\n\n        # Read the csv file\n        tmp_df = pd.read_csv(csv_file)\n\n        # Convert to GeoDataFrame\n        geometry = [Point(xy) for xy in zip(tmp_df['Easting'], tmp_df['Northing'])]\n        tmp_gdf = gpd.GeoDataFrame(tmp_df, geometry=geometry, crs=\"EPSG:32612\")\n\n        # Add to final GeoDataFrame\n        gdf = pd.concat([gdf, tmp_gdf])\n\nprint(\"All files processed.\")\nprint(' ')\nprint(f\"Removing temporary directory: {temp_dir}\")\nshutil.rmtree(temp_dir)\n\nFast and easy! We can now check out the contents of the data.\n\ngdf.head()\n\nSome columns of interest include:\n\nMeasurement Tool [...]: The measurement tool used to measure snow depth: MP = magnaprobe, M2 = Mesa2 tablet, and PR = pit ruler.\n\nDate [...]: The date of the measurement, in yyyymmdd format.\n\nPitID: The designated pit ID associated with the measurement.\n\nDepth (cm): Snow depth, in centimeters.\n\nelevation (m): Surface elevation at the location of the measurement.\n\nThe key variables for this example - the measurement approach and the depth - could use renaming. Let’s do that now.\n\ngdf.rename(columns={\"Measurement Tool (MP = Magnaprobe; M2 = Mesa 2; PR = Pit Ruler)\": 'measurement_tool',\n                    \"Depth (cm)\": 'snow_depth'}, inplace=True\n          )\n\nNow, let’s make a map plot showing the locations of the measurements, colored by snow depth value.\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\n# Define min/max values for colormap\nvmin = gdf['snow_depth'].quantile(0.15)\nvmax = gdf['snow_depth'].quantile(0.85)\n\n# Convert to EPSG:3857 to match with the contextily basemap\nif gdf.crs != 'EPSG:3857':\n    gdf_web = gdf.to_crs(epsg=3857)\n    ax.set_xlim(gdf_web.total_bounds[[0, 2]])\n    ax.set_ylim(gdf_web.total_bounds[[1, 3]])\nelse:\n    ax.set_xlim(gdf.total_bounds[[0, 2]])\n    ax.set_ylim(gdf.total_bounds[[1, 3]])\n\n# Plot snow depths by location\ngdf_web.plot(\n    column='snow_depth',\n    ax=ax,\n    markersize=10,\n    cmap='cmc.navia',\n    legend=True,\n    legend_kwds={'shrink': 0.3, 'label': 'Snow depth [cm]'},\n    vmin=vmin,\n    vmax=vmax\n)\n\n# Add topographic map for spatial reference\nctx.add_basemap(\n    ax, \n    source=ctx.providers.OpenTopoMap,\n    zoom='auto'\n)\n\nax.set_xlabel(\"Easting [m]\", fontsize=14)\nax.set_ylabel(\"Northing [m]\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\nThe above map looks pretty cool, but we might be interested to see how the different measurement approaches differ in accuracy and uncertainty. Let’s now use seaborn to generate snow depth histograms for each instrument.\n\nimport seaborn as sns\n\n# Get the unique measurement values\nunique_measurements = gdf['measurement_tool'].unique()\n\n# Make 1x3 figure for each tool\nfig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n# Set consistent background\nsns.set_style(\"whitegrid\")\n\n# Loop through unique measurement tools to make a plot for each\nfor i, measurement in enumerate(unique_measurements):\n    subset = gdf[gdf['measurement_tool']==measurement]\n\n    # Make a KDE plot normalized by density, rather than raw counts\n    sns.histplot(subset['snow_depth'],\n                 ax=axs[i],\n                 kde=True,\n                 bins=30,\n                 edgecolor='black',\n                 linewidth=0.5,\n                 stat=\"density\",\n                 common_norm=False\n                )\n\n    # Draw a vertical line at the median snow depth\n    median_val = subset['snow_depth'].median()\n    axs[i].axvline(median_val, color='green', linestyle='--', linewidth=2,\n                   label=f'Median: {median_val} cm')\n\n    # Add text that notes the total number of measurements\n    axs[i].text(\n            0.05, 0.95,\n            f\"n = {len(subset)}\",\n            transform=axs[i].transAxes,\n            fontsize=12,\n            verticalalignment='top'\n    )\n\n    axs[i].set_title(f'{measurement}', fontsize=14)\n    axs[i].set_xlabel(\"Depth (cm)\", fontsize=14)\n\n    # Set y-label only for first figure\n    if i == 0:\n        axs[i].set_ylabel(\"Density\", fontsize=14)\n    else:\n        axs[i].set_ylabel(\" \")\n\n    axs[i].legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\nThanks to this plot, we can make a comparison between the different instruments. The magnaprobe depths are the lowest by a slight margin, and also appear to have the lowest spread in depths. The Mesa2 tablet depths are the highest by a few centimeters, and the pit rulers have the highest spread.\n\n","type":"content","url":"/notebooks/snow-pit-data-access#earthaccess-example","position":3},{"hierarchy":{"lvl1":"Snow Pit Data Access and SWE Calculation","lvl2":"SnowEx Database Example"},"type":"lvl2","url":"/notebooks/snow-pit-data-access#snowex-database-example","position":4},{"hierarchy":{"lvl1":"Snow Pit Data Access and SWE Calculation","lvl2":"SnowEx Database Example"},"content":"We will now attempt to achieve the same result, but with the SnowEx Database. Before we dive in, we will see what measurements are available through the snowexsql package.\n\nfrom snowexsql.api import PointMeasurements, LayerMeasurements\n\n# Instantiate the class to use the properties!\nmeasurements = PointMeasurements()\n\n# Get the unique data names/types in the table\nresults = measurements.all_types\nprint('Available types = {}'.format(', '.join([str(r) for r in results])))\n\n# Get the unique instrument in the table\nresults = measurements.all_instruments\nprint('\\nAvailable Instruments = {}'.format(', '.join([str(r) for r in results])))\n\n# Get the unique dates in the table\nresults = measurements.all_dates\nprint('\\nAvailable Dates = {}'.format(', '.join(sorted([str(r) for r in results]))))\n\n# Get the unique site names in the table\nresults = measurements.all_site_names\nprint('\\nAvailable sites = {}'.format(', '.join([str(r) for r in results])))\n\nSo, we have quite of data available! The date range spans from September 2019 to March 2023, and we have snow depth, SWE, and density measurements accessible for multiple sites and instrument types.\n\nSince this is a generally faster method to access data, we will take an extra step and calculate SWE from the available snow depth and density data.\n\nfrom datetime import datetime \n\n# Find some snow depth measurements at the Grand Mesa in early 2020.\nmagnaprobe = measurements.from_filter(\n    type=\"depth\",\n    site_name=\"Grand Mesa\",\n    instrument=\"magnaprobe\",\n    date_less_equal=datetime(2020, 3, 1),\n    date_greater_equal=datetime(2020, 1, 1),\n    limit=38000\n)\n\nmagnaprobe.head()\n\nA breakdown of the query:\n\ntype: The type of measurement we want (depth, swe, density, etc.)\n\ninstrument: The instrument used to obtain the measurement. Here, we obtained magnaprobe measurements only, but below we will also specify the mesa2 and pit ruler measurements.\n\ndate_less_equal: The latest date for any observations. Here, our end date is March 1, 2020.\n\ndate_greater_equal: The start date for our observations. Here, it is January 1, 2020.\n\nlimit: The maximum number of observations for our query. Larger numbers will result in longer loading times, though 38000 works fairly quickly.\n\nThe below plotting cell is identical to the one used in the earthaccess example. Let’s see if they agree!\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\n# Define min/max values for colormap\nvmin = magnaprobe['value'].quantile(0.15)\nvmax = magnaprobe['value'].quantile(0.85)\n\n# Convert to EPSG:3857 to match with the contextily basemap\nif magnaprobe.crs != 'EPSG:3857':\n    gdf_web = magnaprobe.to_crs(epsg=3857)\n    ax.set_xlim(gdf_web.total_bounds[[0, 2]])\n    ax.set_ylim(gdf_web.total_bounds[[1, 3]])\nelse:\n    gdf_web = magnaprobe\n    ax.set_xlim(gdf.total_bounds[[0, 2]])\n    ax.set_ylim(gdf.total_bounds[[1, 3]])\n\n# Plot snow depths by location\ngdf_web.plot(\n    column='value',\n    ax=ax,\n    markersize=10,\n    cmap='cmc.navia',\n    legend=True,\n    legend_kwds={'shrink': 0.5, 'label': 'Snow depth [cm]'},\n    vmin=vmin,\n    vmax=vmax\n)\n\n# Add topographic map for spatial reference\nctx.add_basemap(\n    ax, \n    source=ctx.providers.OpenTopoMap,\n    zoom='auto'\n)\n\nax.set_xlabel(\"Easting [m]\", fontsize=14)\nax.set_ylabel(\"Northing [m]\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\nIt’s quite close! We’re missing out on a few locations, but the number of missing points is negligible.\n\nNow, we are going to calculate SWE. This means we need to grab snow density over the same domain. Note that in the below cell that no instrument is defined. This is because a bulk density is provided over a handful of snow pits, with no specific instrument defined in the SnowEx Database.\n\ndensity = measurements.from_filter(\n    type=\"density\",\n    site_name=\"Grand Mesa\",\n    date_less_equal=datetime(2020, 3, 1),\n    date_greater_equal=datetime(2020, 1, 1),\n    limit=38000\n)\n\ndensity.head()\n\nWe have a much smaller number of snow density measurements than we do snow depths. To keep things simple, we are going to find the median density value across the domain, then use that value to calculate SWE for every snow depth measurement.\n\n# Calculate median density of available data\ndensity_median = density['value'][density['value']>=0].median()\n\n# Make a rough SWE calculation with median density and available snow depths\nmagnaprobe['swe'] = magnaprobe['value'] * (density_median/1000)\n\nWe are going to again repeat the process with the Mesa2 tablets and the pit rulers. As with earthaccess, we will use separate queries to grab these snow depths, then convert them to SWE with the median snow density.\n\n# Make a query for the Mesa2 tablets\nmesa2 = measurements.from_filter(\n    type=\"depth\",\n    site_name=\"Grand Mesa\",\n    instrument=\"mesa\",\n    date_less_equal=datetime(2020, 3, 1),\n    date_greater_equal=datetime(2020, 1, 1),\n    limit=38000\n)\n\nprint(f\"Found {len(mesa2)} Mesa2 tablet measurements.\")\n\n# Do the same for pit ruler measurements\npit_ruler = measurements.from_filter(\n    type=\"depth\",\n    site_name=\"Grand Mesa\",\n    instrument=\"pit ruler\",\n    date_less_equal=datetime(2020, 3, 1),\n    date_greater_equal=datetime(2020, 1, 1),\n    limit=38000\n)\n\nprint(f\"Found {len(pit_ruler)} pit ruler measurements.\")\n\n# Calculate SWE for Mesa2 and pit ruler measurements\nmesa2['swe'] = mesa2['value'] * (density_median*10e-6*1000)\npit_ruler['swe'] = pit_ruler['value'] * (density_median*10e-6*1000)\n\ngdf = pd.concat([magnaprobe, mesa2, pit_ruler])\n\nFinally, we will recreate the histogram figure from the earthaccess example, this time with SWE.\n\nimport seaborn as sns\n\n# Get the unique measurement values\nunique_measurements = gdf['instrument'].unique()\n\n# Make 1x3 figure for each tool\nfig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n# Set consistent background\nsns.set_style(\"whitegrid\")\n\n# Loop through unique measurement tools to make a plot for each\nfor i, measurement in enumerate(unique_measurements):\n    subset = gdf[gdf['instrument']==measurement]\n\n    # Make a KDE plot normalized by density, rather than raw counts\n    sns.histplot(subset['swe'],\n                 ax=axs[i],\n                 kde=True,\n                 bins=30,\n                 edgecolor='black',\n                 linewidth=0.5,\n                 stat=\"density\",\n                 common_norm=False\n                )\n\n    # Draw a vertical line at the median snow depth\n    median_val = subset['swe'].median()\n    axs[i].axvline(median_val, color='green', linestyle='--', linewidth=2,\n                   label=f'Median: {median_val:.3g} mm')\n\n    # Add text that notes the total number of measurements\n    axs[i].text(\n            0.05, 0.95,\n            f\"n = {len(subset)}\",\n            transform=axs[i].transAxes,\n            fontsize=12,\n            verticalalignment='top'\n    )\n\n    axs[i].set_title(f'{measurement}', fontsize=14)\n    axs[i].set_xlabel(\"SWE (mm)\", fontsize=14)\n\n    # Set y-label only for first figure\n    if i == 0:\n        axs[i].set_ylabel(\"Density\", fontsize=14)\n    else:\n        axs[i].set_ylabel(\" \")\n\n    axs[i].legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\nLooks great! We have a distribution of SWE values for all three instruments, and they have reasonable values distributed similarly to the earthaccess depths.","type":"content","url":"/notebooks/snow-pit-data-access#snowex-database-example","position":5},{"hierarchy":{"lvl1":"Field Campaigns Overview"},"type":"lvl1","url":"/notebooks/snowex-data-overview","position":0},{"hierarchy":{"lvl1":"Field Campaigns Overview"},"content":"\n\n","type":"content","url":"/notebooks/snowex-data-overview","position":1},{"hierarchy":{"lvl1":"Field Campaigns Overview"},"type":"lvl1","url":"/notebooks/snowex-data-overview#field-campaigns-overview","position":2},{"hierarchy":{"lvl1":"Field Campaigns Overview"},"content":"(5 minutes)\n\nBy: Megan Mason (NASA Goddard / SSAI) \n\nmegan​.a​.mason@nasa​.gov\n\nSupport by:  Carrie Vuyovich (NASA Goddard), Hans-Peter Marshall (Boise State), Svetlana Stuefer (University of Alaska Fairbanks)\n\nLearning Objectives\n\nVisual overview of the NASA SnowEx field campaigns\n\nGet a sense for the extent of data coverage\n\n\n\n","type":"content","url":"/notebooks/snowex-data-overview#field-campaigns-overview","position":3},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl2":"Data Coverage"},"type":"lvl2","url":"/notebooks/snowex-data-overview#data-coverage","position":4},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl2":"Data Coverage"},"content":"Each year we build upon our efforts to further investigate snow remote sensing science gaps identified in the NASA SnowEx Science Plan \n\n(Durand et al., 2016). The summary table lists the focus for each campaign by year and type. There are two different campaign types (IOP vs. TS); both result in the same types of measurements and data products. Depending on the  research application it may not matter at all which you choose to work with, or even combine! The important thing to grasp is the difference in spatial and temporal extent of the campaign periods. If the sampling protocols or data products change over time, it is for the sake of improvement. When possible, we aim to keep things consistent to continue to build a legacy data set.\n\nYear\n\nCampaign Type\n\nMeasurement Focus\n\n2017\n\nIOP\n\nColorado, focused on multiple instruments in a forest gradient.\n\n2020\n\nIOP, TS\n\nWestern U.S focused on Time Series of L-band InSAR, active/passive microwave for SWE and thermal IR for snow surface temp.\n\n2021\n\nTS\n\nWestern U.S, continued Time Series of L-band InSAR, also addressed prairie & snow albedo questions.\n\n2023\n\nIOP\n\nAlaska Tundra & Boreal forest, focused on addressing SWE/snow depth and albedo objectives.\n\n*IOP=Intense Observation Period (~2-3 week, daily observations) *; TS=Time Series (~3-5 month winter, weekly observations)\n\n","type":"content","url":"/notebooks/snowex-data-overview#data-coverage","position":5},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"Where has SnowEx Been?","lvl2":"Data Coverage"},"type":"lvl3","url":"/notebooks/snowex-data-overview#where-has-snowex-been","position":6},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"Where has SnowEx Been?","lvl2":"Data Coverage"},"content":"Campaign efforts are focused on various snow climates in the western United States. SnowEx partnerships and expertise are spread across the U.S and international.\n\n\nFigure 1. Map showing the locations of SnowEx field campaign areas (red dot). Base map shows snow classes defined in \n\nSturm and Liston, 2021. The snow pit images show a representative pit in each of the class types visited by SnowEx.\n\nTable 1. Number of manual depths and snow pits associated with NASA SnowEx measurement periods.\n\nSnowEx\n\nField Campaign Location\n\nTemporal Coverage\n\nManual Depths\n\nSnow Pits\n\nS17\n\nGrand Mesa & Senator Beck Basin, Colorado\n\nFebruary 6-25, 2017\n\n23,432\n\n265\n\nS20\n\nGrand Mesa, ColoradoWestern U.S Time Series (13 sites)\n\nNovember 4-7, 2019January 27-February 12, 2020October 24-May 20, 2020*\n\n16,21237,921TBD\n\n21154454\n\nS21\n\nWestern U.S Time Series (7 sites)\n\nNovember 16-May 27, 2021\n\n12,536\n\n247\n\nS23\n\nTundra & Boreal Forest, Alaska (pre-campaign site visit)Tundra & Boreal Forest, AlaskaTundra & Boreal Forest, AlaskaBoreal Forest, AlaskaTundra & Boreal Forest, Alaska\n\nMarch 7-17, 2022October 22-27, 2022March 7-16, 2023April 5-May 6, 2023October 17-28, 2023\n\n10,7289,04926,750TBD6,350\n\n1818617013127\n\n*The majority of sites in 2020 have a temporal coverage of January-March due to the Covid-19 pandemic.\n\n","type":"content","url":"/notebooks/snowex-data-overview#where-has-snowex-been","position":7},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"Snow Classification Coverage","lvl2":"Data Coverage"},"type":"lvl3","url":"/notebooks/snowex-data-overview#snow-classification-coverage","position":8},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"Snow Classification Coverage","lvl2":"Data Coverage"},"content":" Thanks to Sturm and Liston 2021 (and 1995), we have a global seasonal snow classification system. This is a vital mission planning tool for remote sensing snow studies. Revised from inception, the snow classification system offers improved utility of the climatological snow classes due to improved (much higher) resolution (300 m over North America). This data set can be found at NSIDC and downloaded at multiple resolutions.\n\n[NSIDC Global Seasonal-Snow Classification, Version 1](https://nsidc.org/data/NSIDC-0768/versions/1) \n\nCheck out [Sturm and Liston, 2021](https://journals.ametsoc.org/view/journals/hydr/22/11/JHM-D-21-0070.1.xml) to find out more \n    \n![](./content/01_snow-classes-sturm.png)\n**Figure 3.** Snow Classes across North America at 300 m (Sturm and Liston, 2021) \n\nAs part of the mission statement, SnowEx aims to quantify snow estimation uncertainty across a range of snow classes, terrain and vegetation types. This is important to determine what areas and time periods have high SWE uncertainty across the ensemble of instrument techniques.\n\n\nFigure 2. Map of the in situ field visits for the duration of SnowEx field campaigns (2017-2023). At this scale, points are overlapping, especially in the eastern Rocky Mountain region around Colorado. The total number of unique visits with recorded SWE are listed in the legend. Upper Right Bar chart of snow classes over the four SnowEx field campaign years. Counts represent in situ field visits. A range of sites in Boreal and Montane Forests occurred in open areas such as meadows and clearings. The snow classification colors match those used in \n\nSturm and Liston, 2021. ![](./content/01_snow-classes-barchart.png)\n**Figure 2.** Bar chart of snow classes over the four SnowEx field campagin years. Counts represent in situ field visits. A range of sites in Boreal and Montane Forests occured in open areas such as meadows and clearings. The snow classification colors match those used in [Sturm and Liston, 2021](https://journals.ametsoc.org/view/journals/hydr/22/11/JHM-D-21-0070.1.xml).   ![](./content/01_map-n-barchart.png)\n**Figure 2.** Bar chart of snow classes over the four SnowEx field campagin years. Counts represent in situ field visits. A range of sites in Boreal and Montane Forests occured in open areas such as meadows and clearings. The snow classification colors match those used in [Sturm and Liston, 2021](https://journals.ametsoc.org/view/journals/hydr/22/11/JHM-D-21-0070.1.xml). \n\n","type":"content","url":"/notebooks/snowex-data-overview#snow-classification-coverage","position":9},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"Recap","lvl2":"Data Coverage"},"type":"lvl3","url":"/notebooks/snowex-data-overview#recap","position":10},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"Recap","lvl2":"Data Coverage"},"content":"SnowEx campaigns are structured based on the objectives set out in the SnowEx Science Plan. Some of those objectives are meet by conducting an all hands-on, short and intense observation period (IOP), while others are addressed by studying the evolution of the snowpack over a much longer time series (TS) style campaign.\n\nThe coincident field and airborne campaigns are designed to directly respond to the current knowledge gaps in remote sensing of seasonal snow, thus the participant-driven SnowEx effort targets a range of snow classes, terrain and vegetation types.\n\n","type":"content","url":"/notebooks/snowex-data-overview#recap","position":11},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"References","lvl2":"Data Coverage"},"type":"lvl3","url":"/notebooks/snowex-data-overview#references","position":12},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"References","lvl2":"Data Coverage"},"content":"SnowEx Experimental Plans: 2017, \n\n2020, \n\n2021, \n\n2023\n\nSnowEx Science Plan\n\nSturm and Liston, 2021","type":"content","url":"/notebooks/snowex-data-overview#references","position":13},{"hierarchy":{"lvl1":"SnowExSQL Database"},"type":"lvl1","url":"/notebooks/snowex-database","position":0},{"hierarchy":{"lvl1":"SnowExSQL Database"},"content":"Tutorial Author Micah’: \n\nMicah Sandusky\n\nTutorial Author Micah_o: \n\nMicah Johnson\n\nSnowEx has introduced a unique opportunity to study SWE in a way that’s unprecedented, but with more data comes new challenges. \n<img src=\"https://snowexsql.readthedocs.io/en/latest/_images/gallery_overview_example_12_0.png\" alt=\"Grand Mesa Overview\" width=\"1000px\"> \n\nThe SnowEx database is a resource that shortcuts the time it takes to ask cross dataset questions\n\nStandardizing diverse data\n\nCross referencing data\n\nProvenance!\n\nAdded GIS functionality\n\nConnect w/ ArcGIS or QGIS!\n\nCITABLE\n\n2022- Estimating snow accumulation and ablation with L-band interferometric synthetic aperture radar (InSAR)\n\n2024 - Thermal infrared shadow-hiding in GOES-R ABI imagery: snow and forest temperature observations from the SnowEx 2020 Grand Mesa field campaign","type":"content","url":"/notebooks/snowex-database","position":1},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl2":"What’s in it?"},"type":"lvl2","url":"/notebooks/snowex-database#whats-in-it","position":2},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl2":"What’s in it?"},"content":"Snow pits - Density, hardness profiles, grain types + sizes\n\nManual snow depths - TONS of depths (Can you say spirals?)\n\nSnow Micropenetrometer (SMP) profiles - (Subsampled to every 100th)\n\nSnow depth + SWE rasters from ASO Inc.\n\nGPR\n\nPit site notes\n\nCamera Derived snow depths\n\nSnow off DEM from USGS 3DEP\n\nAnd almost all the associated metadata","type":"content","url":"/notebooks/snowex-database#whats-in-it","position":3},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl2":"Technically, what is it?"},"type":"lvl2","url":"/notebooks/snowex-database#technically-what-is-it","position":4},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl2":"Technically, what is it?"},"content":"PostgreSQL database\n\nPostGIS extension\n\nSupports vector and raster data\n\nAnd a host of GIS operations\n\nAND NOW WITH API!","type":"content","url":"/notebooks/snowex-database#technically-what-is-it","position":5},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl3":"So what’s the catch?","lvl2":"Technically, what is it?"},"type":"lvl3","url":"/notebooks/snowex-database#so-whats-the-catch","position":6},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl3":"So what’s the catch?","lvl2":"Technically, what is it?"},"content":"New tech can create barriers...\n\n","type":"content","url":"/notebooks/snowex-database#so-whats-the-catch","position":7},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl3":"TL;DR Do less wrangling, do more crunching.","lvl2":"Technically, what is it?"},"type":"lvl3","url":"/notebooks/snowex-database#tl-dr-do-less-wrangling-do-more-crunching","position":8},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl3":"TL;DR Do less wrangling, do more crunching.","lvl2":"Technically, what is it?"},"content":"\n\n","type":"content","url":"/notebooks/snowex-database#tl-dr-do-less-wrangling-do-more-crunching","position":9},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl2":"How do I get at this magical box of data ?"},"type":"lvl2","url":"/notebooks/snowex-database#how-do-i-get-at-this-magical-box-of-data","position":10},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl2":"How do I get at this magical box of data ?"},"content":"SQL\n\nsnowexsql \n\n← 😎","type":"content","url":"/notebooks/snowex-database#how-do-i-get-at-this-magical-box-of-data","position":11},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl3":"Welcome to API Land","lvl2":"How do I get at this magical box of data ?"},"type":"lvl3","url":"/notebooks/snowex-database#welcome-to-api-land","position":12},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl3":"Welcome to API Land","lvl2":"How do I get at this magical box of data ?"},"content":"\n\nfrom snowexsql.api import PointMeasurements\n\ndf = PointMeasurements.from_filter(type=\"depth\", instrument='pit ruler', limit=100)\ndf.plot(column='value', cmap='jet', vmin=10, vmax=150)\ndf\n\n","type":"content","url":"/notebooks/snowex-database#welcome-to-api-land","position":13},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl3":"Old Ways / Advanced Users","lvl2":"How do I get at this magical box of data ?"},"type":"lvl3","url":"/notebooks/snowex-database#old-ways-advanced-users","position":14},{"hierarchy":{"lvl1":"SnowExSQL Database","lvl3":"Old Ways / Advanced Users","lvl2":"How do I get at this magical box of data ?"},"content":"Advanced queries can be made using SQL or SQAlchemy under the hood.\n\nSee previous presentations\n\nEngine objects, session objects, and a crash course in ORM, oh my!\n\nHackweek 2021\n\nHackweek 2022","type":"content","url":"/notebooks/snowex-database#old-ways-advanced-users","position":15},{"hierarchy":{"lvl1":"Snow Modeling"},"type":"lvl1","url":"/notebooks/snowmodeling-tutorial-pt1","position":0},{"hierarchy":{"lvl1":"Snow Modeling"},"content":"Contributors: Melissa Wrzesien1,2, Brendan McAndrew1,3, Jian Li4,5, Caleb Spradlin4,6\n\n1 Hydrological Sciences Lab, NASA Goddard Space Flight Center, 2 University of Maryland, 3 Science Systems and Applications, Inc., 4 InuTeq, LLC, 5 Computational and Information Sciences and Technology Office (CISTO), NASA Goddard Space Flight Center, 6 High Performance Computing, NASA Goddard Space Flight Center\n\nLearning Objectives\n\nLearn about the role of modeling in a satellite mission\n\nOpen, explore, and visualize gridded model output\n\nCompare model output to raster- and point-based observation datasets\n\nThe goal of a future snow satellite is to provide improved understanding of global snow mass, as compared to current estimates. However, a single sensor likely won’t be able to accurately observe all types of snow in all conditions for the entire globe. Instead, we need to combine future snow observations with other tools - including models and observations from currently available satellites.\n\nModels can also help to extend snow observations to areas where observations are not available. Data may be missing due to orbit gaps or masked out in regions of higher uncertainty. Remote sensing observations and models can be combined through data assimilation, a process where observations are used to constrain model estimates.\n\nThe figure above shows a conceptual example of how satellite observations with orbit gaps could be combined with a model to produce results with no missing data. (Figure modified from diagram provided by Eunsang Cho)\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1","position":1},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"What is LIS?"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#what-is-lis","position":2},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"What is LIS?"},"content":"Since models will likely play a role in processing observations from a snow satellite mission, it is important to become comfortable working with gridded data products. Today we will use sample model output from NASA’s Land Information System (LIS).\n\nThe Land Information System is a software framework for land surface modeling and data assimilation developed with the goal of integrating satellite and ground-based observational data products with advanced modeling techniques to produce estimates of land surface states and fluxes.\n\nTL;DR LIS = land surface models + data assimilation\n\nOne key feature LIS provides is flexibility to meet user needs. LIS consists of a collection of plug-ins, or modules, that allow users to design simulations with their choice of land surface model, meteorological forcing, data assimilation, hydrological routing, land surface parameters, and more. The plug-in based design also provides extensibility, making it easier to add new functionality to the LIS framework.\n\nCurrent efforts to expand support for snow modeling include implementation of snow modules, such as \n\nSnowModel and \n\nCrocus, into the LIS framework. These models, when run at the scale of ~100 meters, will enable simulation of wind redistribution, snow grain size, and other important processes for more accurate snow modeling.\n\nDevelopment of LIS is led by the \n\nHydrological Sciences Laboratory at \n\nNASA’s Goddard Space Flight Center. (Figure above provided by the LIS team)\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#what-is-lis","position":3},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Working with Modeled Output"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#working-with-modeled-output","position":4},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Working with Modeled Output"},"content":"Here we will use sample model output from a LIS-SnowModel simulation over a western Colorado domain. Daily estimates of SWE, snow depth, and snow density are written to output. The SnowModel simulation has a spatial resolution of 100 m. We’ve provided four years of output, though here we’ll mostly use output from water year 2020.\n\nBelow, we’ll walk through how to open and interact with the LIS-SnowModel output. Our main objectives are to demonstrate how to do the following with a gridded dataset like LIS-SnowModel:\n\nUnderstand attributes of model data file and available variables\n\nCreate a spatial map of model output\n\nCreate time series at a point and averaged across domain\n\nCompare LIS-SnowModel to raster and point data\n\nCreate a visualization for quick evaluation of model estimates\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#working-with-modeled-output","position":5},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Import Libraries"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#import-libraries","position":6},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Import Libraries"},"content":"\n\n# interface to Amazon S3 filesystem\nimport s3fs\n\n# interact with n-d arrays\nimport numpy as np\nimport xarray as xr\n\n# interact with tabular data (incl. spatial)\nimport pandas as pd\nimport geopandas as gpd\n\n# interactive plots\nimport holoviews as hv\nimport geoviews as gv\nimport hvplot.pandas\nimport hvplot.xarray\n# set bokeh as the holoviews plotting backend\n#hv.extension('bokeh')\n# set holoviews backend to Bokeh\n#gv.extension('bokeh')\n\n# used to find nearest grid cell to a given location\nfrom scipy.spatial import distance\n\nimport fsspec\nfrom datetime import datetime as dt\n\nfrom geoviews import opts\nfrom geoviews import tile_sources as gvts\nfrom datashader.colors import viridis\nimport datashader\nfrom holoviews.operation.datashader import datashade, shade, dynspread, spread, rasterize\nfrom holoviews.streams import Selection1D, Params\nimport panel as pn\nimport param as pm\nimport warnings\nimport holoviews.operation.datashader as hd\nwarnings.filterwarnings(\"ignore\")\n\npn.extension()\npn.param.ParamMethod.loading_indicator =True\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#import-libraries","position":7},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Load the LIS-SnowModel Output"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#load-the-lis-snowmodel-output","position":8},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Load the LIS-SnowModel Output"},"content":"The xarray library makes working with labelled n-dimensional arrays easy and efficient. If you’re familiar with the pandas library it should feel pretty familiar.\n\nHere we load the LIS-SnowModel output into an xarray.Dataset object:\n\n# create S3 filesystem object\ns3 = s3fs.S3FileSystem(anon=False)\n\n# define the name of our S3 bucket\nbucket_name = 'eis-dh-hydro/SNOWEX-HACKWEEK'\n\n# define path to store on S3\nlis_output_s3_path_time_chunk = f's3://{bucket_name}/2022/ZARR/SURFACEMODEL/LIS_HIST_rechunkedV4.d01.zarr'\nlis_output_s3_path = f's3://{bucket_name}/2022/ZARR/SURFACEMODEL/LIS_HIST_default_chunks.d01.zarr/'\n\n# create key-value mapper for S3 object (required to read data stored on S3)\nlis_output_mapper = s3.get_mapper(lis_output_s3_path)\nlis_output_mapper_tc = s3.get_mapper(lis_output_s3_path_time_chunk)\n\n# open the dataset\nlis_output_ds = xr.open_zarr(lis_output_mapper, consolidated=True)\nlis_output_ds_tc = xr.open_zarr(lis_output_mapper_tc, consolidated=True)\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#load-the-lis-snowmodel-output","position":9},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Explore the Data"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#explore-the-data","position":10},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Explore the Data"},"content":"Display an interactive widget for inspecting the dataset by running a cell containing the variable name. Expand the dropdown menus and click on the document and database icons to inspect the variables and attributes.\n\nlis_output_ds\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#explore-the-data","position":11},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"Accessing Attributes","lvl3":"Explore the Data"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#accessing-attributes","position":12},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"Accessing Attributes","lvl3":"Explore the Data"},"content":"\n\nDataset attributes (metadata) are accessible via the attrs attribute:\n\nlis_output_ds.attrs\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#accessing-attributes","position":13},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"Accessing Variables","lvl3":"Explore the Data"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#accessing-variables","position":14},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"Accessing Variables","lvl3":"Explore the Data"},"content":"Variables can be accessed using either dot notation or square bracket notation. Here we will stick with square bracket notation:\n\n# square bracket notation\nlis_output_ds['SM_SnowDepth_inst']\n\nWatch out for large datasets!\n\nNote that the 4-ish years of model output (1694 daily time steps for a domain of size 6650 x 4800) has a size of over 200 gb! As we’ll see below, this is just for an area over western Colorado. If we’re interested in modeling the western United States or CONUS or even global at high resolution, we need to be prepared to work with some large datasets.\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#accessing-variables","position":15},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"Dimensions and Coordinate Variables","lvl3":"Explore the Data"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#dimensions-and-coordinate-variables","position":16},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"Dimensions and Coordinate Variables","lvl3":"Explore the Data"},"content":"The dimensions and coordinate variable fields put the “labelled” in “labelled n-dimensional arrays”:\n\nDimensions: labels for each dimension in the dataset (e.g., time)\n\nCoordinates: labels for indexing along dimensions (e.g., '2020-01-01')\n\nWe can use these labels to select, slice, and aggregate the dataset.\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#dimensions-and-coordinate-variables","position":17},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"Subsetting in Space or Time","lvl3":"Explore the Data"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#subsetting-in-space-or-time","position":18},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"Subsetting in Space or Time","lvl3":"Explore the Data"},"content":"xarray provides two methods for selecting or subsetting along coordinate variables:\n\nindex selection: ds.isel(time=0)\n\nvalue selection ds.sel(time='2020-01-01')\n\nFor example, we can use value selection to select based on the the coorindates of a given dimension:\n\nlis_output_ds.sel(time='2020-01-01')\n\nThe .sel() approach also allows the use of shortcuts in some cases. For example, here we select all timesteps in the month of January 2020. Note that length of the time dimension is now only 31.\n\nlis_output_ds.sel(time='2020-01')\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#subsetting-in-space-or-time","position":19},{"hierarchy":{"lvl1":"Snow Modeling","lvl5":"Latitude and Longitude","lvl4":"Subsetting in Space or Time","lvl3":"Explore the Data"},"type":"lvl5","url":"/notebooks/snowmodeling-tutorial-pt1#latitude-and-longitude","position":20},{"hierarchy":{"lvl1":"Snow Modeling","lvl5":"Latitude and Longitude","lvl4":"Subsetting in Space or Time","lvl3":"Explore the Data"},"content":"You may have noticed that latitude (lat) and longitude (lon) are not listed as dimensions. This dataset would be easier to work with if lat and lon were coordinate variables and dimensions. Here we define a helper function that reads the spatial information from the dataset attributes, generates arrays containing the lat and lon values, and appends them to the dataset:\n\ndef add_latlon_coords(dataset: xr.Dataset)->xr.Dataset:\n    \"\"\"Adds lat/lon as dimensions and coordinates to an xarray.Dataset object.\"\"\"\n    \n    # get attributes from dataset\n    attrs = dataset.attrs\n    \n    # get x, y resolutions\n    dx = .001 #round(float(attrs['DX']), 3)\n    dy = .001 #round(float(attrs['DY']), 3)\n       \n    # get grid cells in x, y dimensions\n    ew_len = len(dataset['east_west'])\n    ns_len = len(dataset['north_south'])\n    \n    # get lower-left lat and lon\n    ll_lat = round(float(attrs['SOUTH_WEST_CORNER_LAT']), 3)\n    ll_lon = round(float(attrs['SOUTH_WEST_CORNER_LON']), 3)\n    \n    # calculate upper-right lat and lon\n    ur_lat =  41.5122 #ll_lat + (dy * ns_len)\n    ur_lon = -103.9831 #ll_lon + (dx * ew_len)\n    \n    # define the new coordinates\n    coords = {\n        # create an arrays containing the lat/lon at each gridcell\n        'lat': np.linspace(ll_lat, ur_lat, ns_len, dtype=np.float32, endpoint=False).round(4),\n        'lon': np.linspace(ll_lon, ur_lon, ew_len, dtype=np.float32, endpoint=False).round(4)\n    }\n    \n    lon_attrs = dataset.lon.attrs\n    lat_attrs = dataset.lat.attrs\n    \n    # rename the original lat and lon variables\n    dataset = dataset.rename({'lon':'orig_lon', 'lat':'orig_lat'})\n    # rename the grid dimensions to lat and lon\n    dataset = dataset.rename({'north_south': 'lat', 'east_west': 'lon'})\n    # assign the coords above as coordinates\n    dataset = dataset.assign_coords(coords)\n    dataset.lon.attrs = lon_attrs\n    dataset.lat.attrs = lat_attrs\n    \n    \n    return dataset\n\nNow that the function is defined, let’s use it to append lat and lon coordinates to the LIS output:\n\nlis_output_ds = add_latlon_coords(lis_output_ds)\nlis_output_ds_tc = add_latlon_coords(lis_output_ds_tc)\n\nInspect the dataset:\n\nlis_output_ds\n\nNow lat and lon are listed as coordinate variables and have replaced the north_south and east_west dimensions. This will make it easier to spatially subset the dataset!\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#latitude-and-longitude","position":21},{"hierarchy":{"lvl1":"Snow Modeling","lvl5":"Basic Spatial Subsetting","lvl4":"Subsetting in Space or Time","lvl3":"Explore the Data"},"type":"lvl5","url":"/notebooks/snowmodeling-tutorial-pt1#basic-spatial-subsetting","position":22},{"hierarchy":{"lvl1":"Snow Modeling","lvl5":"Basic Spatial Subsetting","lvl4":"Subsetting in Space or Time","lvl3":"Explore the Data"},"content":"\n\nWe can use the slice() function on the lat and lon dimensions to select data between a range of latitudes and longitudes:\n\n# uncomment line below to work with the full domain -> this will be much slower!\n#lis_output_ds.sel(lat=slice(35.5, 41), lon=slice(-109, -104))\n\n# just Grand Mesa -> smaller domain for faster run times\n# note the smaller lat/lon extents in the dimensions\nlis_output_ds.sel(lat=slice(38.6, 39.4), lon=slice(-108.6, -107.1))\n\nNotice how the sizes of the lat and lon dimensions have decreased.\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#basic-spatial-subsetting","position":23},{"hierarchy":{"lvl1":"Snow Modeling","lvl5":"Subset Across Multiple Dimensions","lvl4":"Subsetting in Space or Time","lvl3":"Explore the Data"},"type":"lvl5","url":"/notebooks/snowmodeling-tutorial-pt1#subset-across-multiple-dimensions","position":24},{"hierarchy":{"lvl1":"Snow Modeling","lvl5":"Subset Across Multiple Dimensions","lvl4":"Subsetting in Space or Time","lvl3":"Explore the Data"},"content":"\n\nNow we will combine the above examples for subsetting both spatially and temporally:\n\n# define a range of dates to select\nwy_2020_slice = slice('2019-10-01', '2020-09-30')\n\n# define lat/lon for Grand Mesa area\nlat_slice = slice(38.6, 39.4)\nlon_slice = slice(-108.6, -107.1)\n\n# select the snow depth and subset to wy_2020_slice\nsnd_GM_wy2020_ds = lis_output_ds['SM_SnowDepth_inst'].sel(time=wy_2020_slice, lat=lat_slice, lon=lon_slice)\nsnd_GM_wy2020_ds_tc = lis_output_ds_tc['SM_SnowDepth_inst'].sel(time=wy_2020_slice, lat=lat_slice, lon=lon_slice)\n\n# inspect resulting dataset\nsnd_GM_wy2020_ds\n\nSubset for more manageable sizes!\n\nWe’ve now subsetted both spatially (down to just Grand Mesa domain) and temporally (water year 2020). Note the smaller size of the data array -> a decrease from over 200 gb to 1.7 gb! This smaller dataset will be much easier to work with, but feel free to try some of the commands with the full dataset, just give it a few minutes to process!\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#subset-across-multiple-dimensions","position":25},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"Plotting","lvl3":"Explore the Data"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#plotting","position":26},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"Plotting","lvl3":"Explore the Data"},"content":"We’ve imported two plotting libraries:\n\nmatplotlib: static plots\n\nhvplot: interactive plots\n\nWe can make a quick matplotlib-based plot for the subsetted data using the .plot() function supplied by xarray.Dataset objects. For this example, we’ll select one day and plot it:\n\n# simple matplotlilb plot\nsnd_GM_wy2020_ds.sel(time='2020-01-01').plot();\n\nSimilarly we can make an interactive plot using the hvplot accessor and specifying a quadmesh plot type:\n\n# hvplot based map\nsnd_GM_20200101_plot = snd_GM_wy2020_ds.sel(time='2020-01-01').hvplot.quadmesh(geo=True, rasterize=True, project=True,\n                                                                               xlabel='lon', ylabel='lat', cmap='viridis',\n                                                                               tiles='EsriImagery')\n\nsnd_GM_20200101_plot\n\nPan, zoom, and scroll around the map. Hover over the LIS-SnowModel data to see the data values.\n\nIf we try to plot more than one time-step hvplot will also provide a time-slider we can use to scrub back and forth in time:\n\nNote: the time-slider below will only work running in Jupyter Lab (not on the snowex website)\n\nsnd_GM_wy2020_ds.sel(time='2020-01').hvplot.quadmesh(geo=True, rasterize=True, project=True,\n                             xlabel='lon', ylabel='lat', cmap='viridis',\n                             tiles='EsriImagery')\n\nFrom here on out we will stick with hvplot for plotting.\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#plotting","position":27},{"hierarchy":{"lvl1":"Snow Modeling","lvl5":"Timeseries Plots","lvl4":"Plotting","lvl3":"Explore the Data"},"type":"lvl5","url":"/notebooks/snowmodeling-tutorial-pt1#timeseries-plots","position":28},{"hierarchy":{"lvl1":"Snow Modeling","lvl5":"Timeseries Plots","lvl4":"Plotting","lvl3":"Explore the Data"},"content":"We can generate a timeseries for a given grid cell by selecting and calling the plot function:\n\n# define point to take timeseries (note: must be present in coordinates of dataset)\nts_lon, ts_lat = (-107.8702, 39.0504)\n\n# plot timeseries\nsnd_GM_wy2020_ds_tc.sel(lat=ts_lat, lon=ts_lon).hvplot.line(title=f'Snow Depth Timeseries @ Lon: {ts_lon}, Lat: {ts_lat}',\n                                                   xlabel='Date', ylabel='Snow Depth (m)') + \\\n    snd_GM_20200101_plot * gv.Points([(ts_lon, ts_lat)]).opts(size=10, color='red')\n    \n\nIn the next section we’ll learn how to create a timeseries over a broader area.\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#timeseries-plots","position":29},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Aggregation"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#aggregation","position":30},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Aggregation"},"content":"We can perform aggregation operations on the dataset such as min(), max(), mean(), and sum() by specifying the dimensions along which to perform the calculation.\n\nFor example we can calculate the daily mean snow depth for the region around Grand Mesa for water year 2020:\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#aggregation","position":31},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"Area Average","lvl3":"Aggregation"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#area-average","position":32},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"Area Average","lvl3":"Aggregation"},"content":"\n\n# take area-averaged mean at each timestep\nmean_snd_GM_wy2020_ds = snd_GM_wy2020_ds_tc.mean(['lat', 'lon'])\n\n# inspect the dataset\nmean_snd_GM_wy2020_ds\n\n# plot timeseries (hvplot knows how to plot based on dataset's dimensionality!)\nmean_snd_GM_wy2020_ds.hvplot(title='Mean LIS-SnowModel Snow Depth for Grand Mesa area', xlabel='Date', ylabel='Snow Depth (m)')\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#area-average","position":33},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Comparing Model Output"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#comparing-model-output","position":34},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Comparing Model Output"},"content":"Now that we’re familiar with the model output, let’s compare it to two other datasets: SNODAS (raster) and SNOTEL (point).\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#comparing-model-output","position":35},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"LIS-SnowModel (raster) vs. SNODAS (raster)","lvl3":"Comparing Model Output"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#lis-snowmodel-raster-vs-snodas-raster","position":36},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"LIS-SnowModel (raster) vs. SNODAS (raster)","lvl3":"Comparing Model Output"},"content":"\n\nFirst, we’ll load the SNODAS dataset which we also have hosted on S3 as a Zarr store. This command will take a minute or two to run.\n\n# load SNODAS dataset\n\n#snodas depth\nkey = \"SNODAS/snodas_snowdepth_20161001_20200930.zarr\"    \nsnodas_depth_ds = xr.open_zarr(s3.get_mapper(f\"{bucket_name}/{key}\"), consolidated=True)\n\n# apply scale factor to convert to meters (0.001 per SNODAS user guide)\nsnodas_depth_ds = snodas_depth_ds * 0.001\n\nNext we define a helper function to extract the (lon, lat) of the nearest grid cell to a given point:\n\ndef nearest_grid(ds, pt):\n    \n    \"\"\"\n    Returns the nearest lon and lat to pt in a given Dataset (ds).\n    \n    pt : input point, tuple (longitude, latitude)\n    output:\n        lon, lat\n    \"\"\"\n    \n    if all(coord in list(ds.coords) for coord in ['lat', 'lon']):\n        df_loc = ds[['lon', 'lat']].to_dataframe().reset_index()\n    else:\n        df_loc = ds[['orig_lon', 'orig_lat']].isel(time=0).to_dataframe().reset_index()\n    \n    loc_valid = df_loc.dropna()\n    pts = loc_valid[['lon', 'lat']].to_numpy()\n    idx = distance.cdist([pt], pts).argmin()\n    \n    return loc_valid['lon'].iloc[idx], loc_valid['lat'].iloc[idx]\n\nThe next cell will look pretty similar to what we did earlier to plot a timeseries of a single point in the LIS-SnowModel data. The general steps are:\n\nExtract the coordinates of the SNODAS grid cell nearest to our LIS-SnowModel grid cell (ts_lon and ts_lat from earlier)\n\nSubset the SNODAS and LIS-SnowModel data to the grid cells and date ranges of interest\n\nCreate the plots!\n\n# drop off irrelevant variables\ndrop_vars = ['orig_lat', 'orig_lon']\nlis_output_ds = lis_output_ds.drop(drop_vars)\n\n# get lon, lat of snodas grid cell nearest to the LIS coordinates we used earlier\nsnodas_ts_lon, snodas_ts_lat = nearest_grid(snodas_depth_ds, (ts_lon, ts_lat))\n\n# define a date range to plot (shorter = quicker for demo)\nstart_date, end_date = ('2020-01-01', '2020-06-01')\nplot_daterange = slice(start_date, end_date)\n\n# select SNODAS grid cell and subset to plot_daterange\nsnodas_snd_subset_ds = snodas_depth_ds.sel(lon=snodas_ts_lon,\n                                             lat=snodas_ts_lat,\n                                             time=plot_daterange)\n\n# select LIS grid cell and subset to plot_daterange\nlis_snd_subset_ds = lis_output_ds['SM_SnowDepth_inst'].sel(lat=ts_lat,\n                                                        lon=ts_lon,\n                                                        time=plot_daterange)\n\n# create SNODAS snow depth plot\nsnodas_snd_plot = snodas_snd_subset_ds.hvplot(label='SNODAS')\n\n# create LIS snow depth plot\nlis_snd_plot = lis_snd_subset_ds.hvplot(label='LIS-SnowModel')\n\n# create SNODAS vs LIS snow depth plot\nlis_vs_snodas_snd_plot = (lis_snd_plot * snodas_snd_plot)\n\n# display the plot\nlis_vs_snodas_snd_plot.opts(title=f'Snow Depth @ Lon: {ts_lon}, Lat: {ts_lat}',\n                            legend_position='right',\n                            xlabel='Date',\n                            ylabel='Snow Depth (m)')\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#lis-snowmodel-raster-vs-snodas-raster","position":37},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"LIS-SnowModel (raster) vs. SNODAS (raster) vs. SNOTEL (point)","lvl3":"Comparing Model Output"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#lis-snowmodel-raster-vs-snodas-raster-vs-snotel-point","position":38},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"LIS-SnowModel (raster) vs. SNODAS (raster) vs. SNOTEL (point)","lvl3":"Comparing Model Output"},"content":"Now let’s add SNOTEL point data to our plot.\n\nFirst, we’re going to define some helper functions to load the SNOTEL data:\n\n# load csv containing metadata for SNOTEL sites in a given state (e.g,. 'colorado')\ndef load_site(state):\n    \n    # define the path to the file\n    key = f\"SNOTEL/snotel_{state}.csv\"\n    \n    # load the csv into a pandas DataFrame\n    df = pd.read_csv(s3.open(f's3://{bucket_name}/{key}', mode='r'))\n    \n    return df\n\n# load SNOTEL data for a specific site\ndef load_snotel_txt(state, var):\n    \n    # define the path to the file\n    key = f\"SNOTEL/snotel_{state}{var}_20162020.txt\"\n    \n    # determine how many lines to skip in the file (they start with #)\n    fh = s3.open(f\"{bucket_name}/{key}\")\n    lines = fh.readlines()\n    skips = sum(1 for ln in lines if ln.decode('ascii').startswith('#'))\n    \n    # load the data into a pandas DataFrame\n    df = pd.read_csv(s3.open(f\"s3://{bucket_name}/{key}\"), skiprows=skips)\n    \n    # convert the Date column from strings to datetime objects\n    df['Date'] = pd.to_datetime(df['Date'])\n    return df\n\nFor the purposes of this tutorial let’s load the SNOTEL data for sites in Colorado. We’ll pick one site to plot in a few cells.\n\n# load SNOTEL snow depth for Colorado into a dictionary\nsnotel_depth = {'CO': load_snotel_txt('CO', 'depth')}\n\nWe’ll need another helper function to load the depth data:\n\n# get snotel depth\ndef get_depth(state, site, start_date, end_date):\n    \n    # grab the depth for the given state (e.g., CO)\n    df = snotel_depth[state]\n    \n    # define a date range mask\n    mask = (df['Date'] >= start_date) & (df['Date'] <= end_date)\n    \n    # use mask to subset between time range\n    df = df.loc[mask]\n    \n    # extract timeseries for the given site\n    return pd.concat([df.Date, df.filter(like=site)], axis=1).set_index('Date')\n\nLoad the site metadata for Colorado:\n\nco_sites = load_site('colorado')\n\n# peek at the first 5 rows\nco_sites.head()\n\nThe point we’ve been using so far in the tutorial actually corresponds to the coordinates for the Park Reservoir SNOTEL on Grand Mesa! Let’s extract the site data for that point:\n\n# get the depth data by passing the site name to the get_depth() function\npark_res_snd_df = get_depth('CO', 'Park Reservoir (682)', start_date, end_date)\n\n# convert from cm to m\npark_res_snd_df = park_res_snd_df / 100\n\nNow we’re ready to plot:\n\n# create SNOTEL plot\npark_res_plot = park_res_snd_df.hvplot(label='SNOTEL')\n\n# combine the SNOTEl plot with the LIS vs SNODAS plot\n(lis_vs_snodas_snd_plot * park_res_plot).opts(title=f'Snow Depth @ Lon: {ts_lon}, Lat: {ts_lat}', legend_position='right')\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#lis-snowmodel-raster-vs-snodas-raster-vs-snotel-point","position":39},{"hierarchy":{"lvl1":"Snow Modeling","lvl2":"Interactive Data Exploration"},"type":"lvl2","url":"/notebooks/snowmodeling-tutorial-pt1#interactive-data-exploration","position":40},{"hierarchy":{"lvl1":"Snow Modeling","lvl2":"Interactive Data Exploration"},"content":"Note:You must run the following code in JupyterLab for interactivity\n\nWe’ve now built up the separate pieces for visualizing LIS-SnowModel output spatially and temporally, and we’ve combined our time series with those from SNOTEL stations and SNODAS. Now we can bring it all together in an interactive tool.\n\nThe code in the cells below will generate an interactive panel for comparing of LIS-SnowModel output, SNODAS, and SNOTEL snow depth and snow water equivalent at SNOTEL site locations.\n\nNote: some cells below take several minutes to run and some aspects of the interactive widgets may not work in the rendered version on the Hackweek site.\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#interactive-data-exploration","position":41},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Load Data","lvl2":"Interactive Data Exploration"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#load-data","position":42},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Load Data","lvl2":"Interactive Data Exploration"},"content":"\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#load-data","position":43},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"SNOTEL Sites info","lvl3":"Load Data","lvl2":"Interactive Data Exploration"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#snotel-sites-info","position":44},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"SNOTEL Sites info","lvl3":"Load Data","lvl2":"Interactive Data Exploration"},"content":"\n\n# create dictionary linking state names and abbreviations\nsnotel = {\"CO\" : \"colorado\",\n          \"ID\" : \"idaho\",\n          \"NM\" : \"newmexico\",\n          \"UT\" : \"utah\",\n          \"WY\" : \"wyoming\"}\n\n# load SNOTEL site metadata for sites in the given state\ndef load_site(state):\n    \n    # define path to file\n    key = f\"SNOTEL/snotel_{state}.csv\"\n    \n    # load csv into pandas DataFrame\n    df = pd.read_csv(s3.open(f'{bucket_name}/{key}', mode='r'))\n    \n    return df \n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#snotel-sites-info","position":45},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"SNOTEL Depth & SWE","lvl3":"Load Data","lvl2":"Interactive Data Exploration"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#snotel-depth-swe","position":46},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"SNOTEL Depth & SWE","lvl3":"Load Data","lvl2":"Interactive Data Exploration"},"content":"\n\ndef load_snotel_txt(state, var):\n    \n    # define path to file\n    key = f\"SNOTEL/snotel_{state}{var}_20162020.txt\"\n\n    # open text file\n    fh = s3.open(f\"{bucket_name}/{key}\")\n    \n    # read each line and note those that begin with '#'\n    lines = fh.readlines()\n    skips = sum(1 for ln in lines if ln.decode('ascii').startswith('#'))\n    \n    # load txt file into pandas DataFrame (skipping lines beginning with '#')\n    df = pd.read_csv(s3.open(f\"{bucket_name}/{key}\"), skiprows=skips)\n    \n    # convert Date column from str to pandas datetime objects\n    df['Date'] = pd.to_datetime(df['Date'])\n    return df\n\n# load SNOTEL depth & swe into dictionaries\n\n# define empty dicts\nsnotel_depth = {}\nsnotel_swe = {}\n\n# loop over states and load SNOTEL data\nfor state in snotel.keys():\n    print(f\"Loading state {state}\")\n    snotel_depth[state] = load_snotel_txt(state, 'depth')\n    snotel_swe[state] = load_snotel_txt(state, 'swe')\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#snotel-depth-swe","position":47},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"SNODAS SWE","lvl3":"Load Data","lvl2":"Interactive Data Exploration"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#snodas-swe","position":48},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"SNODAS SWE","lvl3":"Load Data","lvl2":"Interactive Data Exploration"},"content":"We’ve already loaded in SNODAS snow depths, but here we’ll load in SWE. As above, this cell might take a few minutes to run since it’s a large dataset to read.\n\n# load snodas swe data\nkey = \"SNODAS/snodas_swe_20161001_20200930.zarr\"\nsnodas_swe_ds = xr.open_zarr(s3.get_mapper(f\"{bucket_name}/{key}\"), consolidated=True)\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#snodas-swe","position":49},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"LIS-SnowModel Outputs","lvl3":"Load Data","lvl2":"Interactive Data Exploration"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#lis-snowmodel-outputs","position":50},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"LIS-SnowModel Outputs","lvl3":"Load Data","lvl2":"Interactive Data Exploration"},"content":"\n\nWe’ve already read in the LIS-SnowModel data above. Here we subset by time for October 2019 - June 2020.\n\n# subset LIS data for winter 2020 \ntime_range = slice('2019-10-01', '2020-06-30')\nlis_sf = lis_output_ds_tc.sel(time=time_range)\nlis_sf = lis_sf.drop(drop_vars)\n\nIn the next cell, we extract the data variable names and timesteps from the LIS outputs. These will be used to define the widget options.\n\n# gather metadata from LIS\n# get variable names:string\nvnames = list(lis_sf.data_vars)\nprint(vnames)\n\n# get time-stamps:string\ntstamps = list(np.datetime_as_string(lis_sf.time.values, 'D'))\nprint(len(tstamps), tstamps[0], tstamps[-1])\n\nBy default, the holoviews plotting library automatically adjusts the range of plot colorbars based on the range of values in the data being plotted. This may not be ideal when comparing data on different timesteps. In the next cell we assign the upper and lower bounds for each data variable which we’ll later use to set a static colorbar range.\n\ncmap_lims = {'SM_SWE_inst': (0.0, 3.0),\n 'SM_SnowDensity_inst': (100, 550.0),\n 'SM_SnowDepth_inst': (0.0, 6.5)}\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#lis-snowmodel-outputs","position":51},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Interactive Widget","lvl2":"Interactive Data Exploration"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#interactive-widget","position":52},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Interactive Widget","lvl2":"Interactive Data Exploration"},"content":"","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#interactive-widget","position":53},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"SNOTEL Site Map and Timeseries","lvl3":"Interactive Widget","lvl2":"Interactive Data Exploration"},"type":"lvl4","url":"/notebooks/snowmodeling-tutorial-pt1#snotel-site-map-and-timeseries","position":54},{"hierarchy":{"lvl1":"Snow Modeling","lvl4":"SNOTEL Site Map and Timeseries","lvl3":"Interactive Widget","lvl2":"Interactive Data Exploration"},"content":"The two cells that follow will create an interactive panel for comparing LIS-SnowModel, SNODAS, and SNOTEL snow depth and snow water equivalent. The SNOTEL site locations are plotted as points on an interactive map for each state. Hover over the sites to view metadata and click on a site to generate a timeseries!\n\nNote: it will take some time for the timeseries to display.\n\n# get snotel depth\ndef get_depth(state, site, ts, te):\n    df = snotel_depth[state]\n    \n    # subset between time range\n    mask = (df['Date'] >= ts) & (df['Date'] <= te)\n    df = df.loc[mask]\n    \n    # extract timeseries for the site\n    return pd.concat([df.Date, df.filter(like=site)], axis=1).set_index('Date')\n\n# get snotel swe\ndef get_swe(state, site, ts, te):\n    df = snotel_swe[state]\n    \n    # subset between time range\n    mask = (df['Date'] >= ts) & (df['Date'] <= te)\n    df = df.loc[mask]\n    \n    # extract timeseries for the site\n    return pd.concat([df.Date, df.filter(like=site)], axis=1).set_index('Date')\n\n# co-locate site & LIS-SnowModel model cell\ndef nearest_grid(pt):\n    # pt : input point, tuple (longtitude, latitude)\n    # output:\n    #        x_idx, y_idx \n    loc_valid = df_loc.dropna()\n    pts = loc_valid[['lon', 'lat']].to_numpy()\n    idx = distance.cdist([pt], pts).argmin()\n\n    return loc_valid['east_west'].iloc[idx], loc_valid['north_south'].iloc[idx]\n\n# get LIS-SnowModel variable \ndef var_subset(dset, v, lon, lat, ts, te):\n    return dset[v].sel(lon=lon, lat=lat, method=\"nearest\").sel(time=slice(ts, te)).load()\n\n# line plots\ndef line_callback(index, state, vname, ts_tag, te_tag):\n    sites = load_site(snotel[state])\n    row = sites.iloc[1]\n    \n    tmp = var_subset(lis_sf, vname, row.lon, row.lat, ts_tag, te_tag) \n    xr_sf = xr.zeros_like(tmp)\n    \n    xr_snodas = xr_sf\n    \n    ck = get_depth(state, row.site_name, ts_tag, te_tag).to_xarray().rename({'Date': 'time'})\n    xr_snotel = xr.zeros_like(ck)\n    \n    if not index:\n        title='Var: -- Lon: -- Lat: --'\n        return (xr_sf.hvplot(title=title, color='blue', label='LIS-SnowModel') \\\n                * xr_snotel.hvplot(color='red', label='SNOTEL') \\\n                * xr_snodas.hvplot(color='green', label='SNODAS')).opts(legend_position='right')\n        \n\n    else:\n        sites = load_site(snotel[state])\n        first_index = index[0]\n        row = sites.iloc[first_index]\n    \n        \n        xr_sf = var_subset(lis_sf, vname, row.lon, row.lat, ts_tag, te_tag)\n    \n        vs = vname.split('_')[1]\n        title=f'Var: {vs} Lon: {row.lon} Lat: {row.lat}'\n\n        \n        # update snotel data \n        if 'depth' in vname.lower():\n            xr_snotel =  get_depth(state, row.site_name, ts_tag, te_tag).to_xarray().rename({'Date': 'time'})*0.01\n            xr_snodas =  var_subset(snodas_depth_ds, 'SNOWDEPTH', row.lon, row.lat, ts_tag, te_tag)\n        \n        if 'swe' in vname.lower():\n            xr_snotel =  get_swe(state, row.site_name, ts_tag, te_tag).to_xarray().rename({'Date': 'time'})/1000\n            xr_snodas =  var_subset(snodas_swe_ds, 'SWE', row.lon, row.lat, ts_tag, te_tag)/1000\n\n    \n        return xr_sf.hvplot(title=title, color='blue', label='LIS-SnowModel') \\\n               * xr_snotel.hvplot(color='red', label='SNOTEL') \\\n               * xr_snodas.hvplot(color='green', label='SNODAS')\n\n\n\n# satic snowdepth as background\ndds = lis_sf['SM_SnowDepth_inst'].sel(time='2020-02-01').load()\n\n# prepare the panel that will display a static plot of snow depth. Snotel sites will be plotted on top\nimg = dds.hvplot.quadmesh(geo=True, rasterize=True, project=True,\n                             xlabel='lon', ylabel='lat', cmap='viridis',\n                             clim=(0,1))\nsnow_depth_bg=hd.regrid(img)\n\n# sites on map\ndef plot_points(state):  \n    # dataframe to hvplot obj Points\n    sites=load_site(snotel[state])\n    pts_opts=dict(size=10, nonselection_alpha=0.5,tools=['tap', 'hover'])\n    #site_points=sites.hvplot.points(x='lon', y='lat', c='elev', cmap='fire', geo=True, hover_cols=['site_name', 'ntwk', 'state', 'lon', 'lat']).opts(**pts_opts)  \n    site_points=sites.hvplot.points(x='lon', y='lat', color='black',geo=True, hover_cols=['site_name', 'ntwk', 'state', 'lon', 'lat']).opts(**pts_opts)  \n    return site_points\n\n# base map\ntiles = gvts.OSM()\n\n# state widget\nstate_select = pn.widgets.Select(options=list(snotel.keys()), name=\"State\")\nstate_stream = Params(state_select, ['value'], rename={'value':'state'})\n\n# variable widget\nvar_select = pn.widgets.Select(options=['SM_SnowDepth_inst', 'SM_SWE_inst'], name=\"LIS Variable List\")\nvar_stream = Params(var_select, ['value'], rename={'value':'vname'})\n\n# date range widget\ndate_fmt = '%Y-%m-%d'\nsdate_input = pn.widgets.DatetimeInput(name='Start date', value=dt(2019,10,1),start=dt.strptime(tstamps[0], date_fmt), end=dt.strptime(tstamps[-1], date_fmt), format=date_fmt)\nsdate_stream = Params(sdate_input, ['value'], rename={'value':'ts_tag'})\nedate_input = pn.widgets.DatetimeInput(name='End date', value=dt(2020,6,30),start=dt.strptime(tstamps[0], date_fmt), end=dt.strptime(tstamps[-1], date_fmt),format=date_fmt)\nedate_stream = Params(edate_input, ['value'], rename={'value':'te_tag'})\n\n# generate site points as dynamic map\n# plots points and calls plot_points() when user selects a site\nsite_dmap = hv.DynamicMap(plot_points, streams=[state_stream]).opts(height=400, width=600)\n# pick site\nselect_stream = Selection1D(source=site_dmap)\n\n# link widgets to callback function\nline = hv.DynamicMap(line_callback, streams=[select_stream, state_stream, var_stream, sdate_stream, edate_stream])\n\n# create panel layout\npn.Row(snow_depth_bg*site_dmap*tiles, pn.Column(state_select, var_select, pn.Row(sdate_input, edate_input), line))\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#snotel-site-map-and-timeseries","position":55},{"hierarchy":{"lvl1":"Snow Modeling","lvl2":"Exercises"},"type":"lvl2","url":"/notebooks/snowmodeling-tutorial-pt1#exercises","position":56},{"hierarchy":{"lvl1":"Snow Modeling","lvl2":"Exercises"},"content":"You now know how to see what variables are available in a typical model output file and you’ve learned how to inspect gridded model estimates both spatially and temporally. Below are a few exercises for both practicing the above skills and for becoming more familiar with the sample model output.","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#exercises","position":57},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Exercise 1","lvl2":"Exercises"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#exercise-1","position":58},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Exercise 1","lvl2":"Exercises"},"content":"So far we’ve mostly worked with the Grand Mesa region. Can you spatially subset the data to inspect the LIS-SnowModel estimates for the Front Range? What about for Senator Beck basin? Try to use hvplot to plot a map of SWE values for February 2020.","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#exercise-1","position":59},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Exercise 2","lvl2":"Exercises"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#exercise-2","position":60},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Exercise 2","lvl2":"Exercises"},"content":"Here we focused on water year 2020. Can you select a point (maybe the same Park Reservoir SNOTEL point we worked with here) and make a multi-year time series?","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#exercise-2","position":61},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Exercise 3 (for an extra challenge!)","lvl2":"Exercises"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#exercise-3-for-an-extra-challenge","position":62},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Exercise 3 (for an extra challenge!)","lvl2":"Exercises"},"content":"We learned about the SnowEx database in a tutorial earlier this week. Can you create a new notebook for combining the model output with your choice of field observation available in the database? Does the model over or underestimate the SnowEx observation?\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#exercise-3-for-an-extra-challenge","position":63},{"hierarchy":{"lvl1":"Snow Modeling","lvl2":"Conclusion"},"type":"lvl2","url":"/notebooks/snowmodeling-tutorial-pt1#conclusion","position":64},{"hierarchy":{"lvl1":"Snow Modeling","lvl2":"Conclusion"},"content":"We’re now more familiar with model output and how to interact with it in Python. The code in this notebook is a great jumping off point for developing more advanced comparisons and interactive widgets.\n\nThe Python code can be adapted to other LIS simulations and to other model output as well, with minor modifications. Anyone interested in testing your new skills can combine what you learned here with the other SnowEx Hackweek tutorials - try comparing the LIS-SnowModel output with other snow observations collected during the 2017 field campaign!\n\nFor more information on \n\nNASA’s Land Information System please see the links below\n\n","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#conclusion","position":65},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Resources","lvl2":"Conclusion"},"type":"lvl3","url":"/notebooks/snowmodeling-tutorial-pt1#resources","position":66},{"hierarchy":{"lvl1":"Snow Modeling","lvl3":"Resources","lvl2":"Conclusion"},"content":"Links\n\nLIS Website\n\nLIS on GitHub\n\nLIS Documentation\n\nLIS Test Case Walkthrough\n\nReferences\n\nKumar, S.V., C.D. Peters-Lidard, Y. Tian, P.R. Houser, J. Geiger, S. Olden, L. Lighty, J.L. Eastman, B. Doty, P. Dirmeyer, J. Adams, K. Mitchell, E. F. Wood, and J. Sheffield, 2006: Land Information System - An interoperable framework for high resolution land surface modeling. Environ. Modeling & Software, 21, 1402-1415, \n\nKUMAR et al. (2006)\n\nPeters-Lidard, C.D., P.R. Houser, Y. Tian, S.V. Kumar, J. Geiger, S. Olden, L. Lighty, B. Doty, P. Dirmeyer, J. Adams, K. Mitchell, E.F. Wood, and J. Sheffield, 2007: High-performance Earth system modeling with NASA/GSFC’s Land Information System. Innovations in Systems and Software Engineering, 3(3), 157-165, \n\nPeters-Lidard et al. (2007)","type":"content","url":"/notebooks/snowmodeling-tutorial-pt1#resources","position":67},{"hierarchy":{"lvl1":"Time-lapse Cameras"},"type":"lvl1","url":"/notebooks/timelapse-camera-tutorial","position":0},{"hierarchy":{"lvl1":"Time-lapse Cameras"},"content":"Learning Objectives\n\nAt the conclusion of this tutorial, you will...:\n\nKnow about all the time-lapse images available from the SnowEx 2017 and 2020 field campaigns\n\nView example time-lapse images from SnowEx 2020 and visualize their locations\n\nAccess snow depth measurements extracted from the SnowEx 2020 time-lapse images\n\nCompare snow depths from different SnowEx 2020 time-lapse cameras\n\n","type":"content","url":"/notebooks/timelapse-camera-tutorial","position":1},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"type":"lvl2","url":"/notebooks/timelapse-camera-tutorial#time-lapse-cameras-on-grand-mesa-during-snowex-field-campaigns","position":2},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"content":"Time-lapse cameras were installed in both the SnowEx 2017 and 2020 field campaigns on Grand Mesa in similar locations.\n\nSnowEx 2017 Time-lapse Cameras\n\n28 Total Time-lapse Cameras\n\nCapturing the entire winter season (September 2016-June 2017)\n\nTaking 4 photos/day at 8AM, 10AM, 12PM, 2PM, 4PM\n\nAn orange pole was installed in front of 15 cameras for snow depth measurements\n\nTime-lapse images have been submitted to the NSIDC by Mark Raleigh with all the required metadata (e.g., locations, naming convention, etc.) for use.\n\nSnowEx 2020 Time-lapse Cameras\n\n29 Total Time-lapse Cameras\n\nCapturing the entire winter season (September 2019-June 2020)\n\nTaking 3 photos/day at 11AM, 12PM, 1PM or 2 photos/day at 11AM and 12PM\n\nA red pole was installed in front of each camera for snow depth measurements.\n\nCameras were installed on the east and west side of the Grand Mesa, across a vegetation scale of 1-9, using the convention XMR:\n\nX = East (E) or West (W) areas of the Mesa\n\nM = number 1-9, representing 1 (least vegetation) to 9 (most vegetation). Within each vegetation class, there were three sub-classes of snow depths derived from 2017 SnowEx lidar measurements.\n\nR = Replicate of vegetation assignment, either A, B, C, D, or E.\n\nThe complete set of time-lapse images from 2020 are in progress for submission to the NSIDC. A subset of them are available here for you to use during hackweek.\n\n","type":"content","url":"/notebooks/timelapse-camera-tutorial#time-lapse-cameras-on-grand-mesa-during-snowex-field-campaigns","position":3},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl3":"An automated way of viewing and mapping time-lapse photos","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"type":"lvl3","url":"/notebooks/timelapse-camera-tutorial#an-automated-way-of-viewing-and-mapping-time-lapse-photos","position":4},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl3":"An automated way of viewing and mapping time-lapse photos","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"content":"\n\nFirst, import all the packages we’ll need for this tutorial\n\n# Packages to connect and download from the SnowEx database \nimport snowexsql.db # import SnowEx SQL library\nfrom snowexsql.db import get_db # connection function from the snowexsql library\nfrom snowexsql.data import PointData, SiteData # point and table classes \nfrom snowexsql.conversions import query_to_geopandas # function to convert incoming data from database to geopandas df\n\n# Packages for data analysis \nimport geopandas as gpd # geopandas library for data analysis and visualization\nimport pandas as pd # pandas as to read csv data and visualize tabular data\nimport numpy as np # numpy for data analysis \n\n# Packages for data visualization\nimport matplotlib.pyplot as plt # matplotlib.pyplot for plotting images and graphs\nfrom IPython.display import Image # used to display images inside the notebook\nimport seaborn as sns # a module that adds some plotting capabilities \nimport matplotlib as mpl # import all of matplotlib for plotting \n\n# The following settings just set some defaults for the plots in this notebook\nsns.set() # activates some of the default settings from seaborn\nsns.set_style(\"dark\", {\"xtick.bottom\": True, 'ytick.left': True}) # improves x and y ticks \n\nplt.rcParams['figure.figsize']  = (10, 4) # figure size\nplt.rcParams['axes.titlesize']  = 14 # title size \nplt.rcParams['axes.labelsize']  = 12 # axes label size \nplt.rcParams['xtick.labelsize'] = 11 # x tick label size \nplt.rcParams['ytick.labelsize'] = 11 # y tick label size \nplt.rcParams['legend.fontsize'] = 11 # legend size \nmpl.rcParams['figure.dpi'] = 100\n\nWe will map 2020 time-lapse camera locations on the Grand Mesa with the 2020 snow pits for reference.\n\ncode credit: Micah Johnson\n\n# Connect to the database\ndb_name = 'snow:hackweek@db.snowexdata.org/snowex'\nengine, session = get_db(db_name)\n\n# Grab all the point data that was that was measured with a time-lapse camera \nqry = session.query(PointData)\nqry = qry.filter(PointData.instrument == 'camera')\n\n# Convert it to a geopandas df and visualize the dataframe\ncamera_depths = query_to_geopandas(qry, engine)\ncamera_depths.head()\n\nGrab all the unique geometry objects (i.e., locations)\n\nqry = session.query(SiteData.geom).distinct()\npits = query_to_geopandas(qry, engine)\npits.head()\n\nAnd, print out how many of each we found\n\nprint(f'Found {len(camera_depths[\"geom\"].unique())} time-lapse camera locations')\nprint(f'Found {len(pits.index)} pit site locations')\n\n# End our database session to avoid hanging transactions\nsession.close()\n\n","type":"content","url":"/notebooks/timelapse-camera-tutorial#an-automated-way-of-viewing-and-mapping-time-lapse-photos","position":5},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl4":"Plot the camera locations, using snow pit locations for reference.","lvl3":"An automated way of viewing and mapping time-lapse photos","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"type":"lvl4","url":"/notebooks/timelapse-camera-tutorial#plot-the-camera-locations-using-snow-pit-locations-for-reference","position":6},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl4":"Plot the camera locations, using snow pit locations for reference.","lvl3":"An automated way of viewing and mapping time-lapse photos","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"content":"\n\ncamera_depths.explore(tooltip=['equipment','date','latitude','longitude','value','type','units'])\n\nWhat do you notice? Is there overlap between the snow pit and camera trap locations?\n\n","type":"content","url":"/notebooks/timelapse-camera-tutorial#plot-the-camera-locations-using-snow-pit-locations-for-reference","position":7},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl3":"Viewing the time-lapse photos","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"type":"lvl3","url":"/notebooks/timelapse-camera-tutorial#viewing-the-time-lapse-photos","position":8},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl3":"Viewing the time-lapse photos","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"content":"Download the sample datasets for this tutorial For ease of access during the hackweek, sample files are available for download for running the command in the cell below.\n\nimport earthaccess\n\n# Authenticate with Earthdata Login servers\nauth = earthaccess.login(strategy=\"interactive\")\n\n# Search for snow-on granules\nresults = earthaccess.search_data(\n    doi = \"10.5067/WYRNU50R9L5R\"\n)\n\nfiles = earthaccess.open(results)\n\ndownloaded_files = earthaccess.download(\n    results[0],\n    local_path = \"C:/Users/zfair/OneDrive - NASA/Documents/Python/Projects/SnowPit/Workflows/tmp\", # Change this string to download to a different path\n)\n\n## Improved way to work with camera files directly - WORK IN PROGRESS\nimport requests\nfrom io import BytesIO\n\n# Get the URL of the zip file from the collection metadata\nzip_url = 'https://n5eil01u.ecs.nsidc.org/DP6/SNOWEX/SNEX_Met_Raw.001/2016.10.09/SNEX_Met_Raw.zip' \n\n# Download the zip file\nresponse = requests.get(zip_url)\nresponse.raise_for_status()\n\n# Open the zip file in memory\nwith zipfile.ZipFile(BytesIO(response.content)) as z:\n    # List all files in the zip archive\n    file_list = z.namelist()\n    print(\"Files in the archive:\", file_list)\n    \n    # Extract CSV files\n    for file_name in file_list:\n        if file_name.endswith('.csv'):\n            with z.open(file_name) as csv_file:\n                # Read the CSV content\n                csv_data = csv_file.read().decode('utf-8')\n                print(f\"Contents of {file_name}:\\n\", csv_data)\n\nimport tarfile\n\npath = \"/tmp\"\n\nwith tarfile.open(downloaded_files[0], \"r:gz\") as tar:\n    tar.extractall(path=path)\n\nNow display an example time-lapse image inside the notebook\n\nWe will now pull time-lapse imagery for one camera, camera E9B, from the SnowEx 2020 field campaign. E9B is from the E ast side of the Mesa, in a high vegetation area (9), and it is the second replicate of this combination (B). We will pull images and display a sample from various times of the winter season to provide a sense of what image and the snow poles look like.\n\nimport os\nimport datetime\nfrom IPython.display import Image, display\n \nfiles = ['C:/Users/zfair/OneDrive - NASA/Documents/Python/Projects/SnowPit/Workflows/tmp/TLS-L2E/WSCT1463.JPG',\n         'C:/Users/zfair/OneDrive - NASA/Documents/Python/Projects/SnowPit/Workflows/tmp/TLS-L2E/WSCT1644.JPG',\n         'C:/Users/zfair/OneDrive - NASA/Documents/Python/Projects/SnowPit/Workflows/tmp/TLS-L2E/WSCT1844.JPG']\nfor file in files:\n    creationTime = os.path.getmtime(file)\n    dt_c = datetime.datetime.fromtimestamp(creationTime)\n    formatted_datetime = dt_c.strftime(\"%m/%d/%Y %H:%M\")\n\n    print(f'Site L2E: {formatted_datetime}')\n    display(Image(file, width=300))\n    print(' ')\n\n","type":"content","url":"/notebooks/timelapse-camera-tutorial#viewing-the-time-lapse-photos","position":9},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl2":"Time-lapse Camera Applications"},"type":"lvl2","url":"/notebooks/timelapse-camera-tutorial#time-lapse-camera-applications","position":10},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl2":"Time-lapse Camera Applications"},"content":"Installing snow poles in front of time-lapse camera provides low-cost, long-term snow depth timeseries. Snow depths from the 2020 SnowEx time-lapse imagery have been manually processed with estimation of submission to the NSIDC database in summer 2021.\n\nThe snow depth is the difference between the number of pixels in a snow-free image and an image with snow, with a conversion from pixels to centimeters (Figure 1).\n\n\n\nFigure 1: Equation to extract snow depth from camera images. For each image, take the difference in pixels between the length of a snow-free stake and the length of the stake and multiply by length(cm)/pixel. The ratio can be found by dividing the full length of the stake (304.8 cm) by the length of a snow-free stake in pixels.\n\nSnow depth can be obtained in this manner manually, but it is now easier to determine the pixel size of the stakes through machine learning. For the sake of completeness, we will provide a brief example using the camera imagery above. Otherwise, users interested in using the camera imagery with machine learning are encouraged to check out the following resources by Katherine Breen and others:\n\nPublication on methodBreen C. M., W. R. Currier, C. Vuyovich, et al. 2024. “Snow Depth Extraction From Time‐Lapse Imagery Using a Keypoint Deep Learning Model.” Water Resources Research 60 (7): [10.1029/2023wr036682]\n\nGithub page for algorithm\n\nhttps://​github​.com​/catherine​-m​-breen​/snowpoles\n\nIn the example images above, we use the red pole in the fully snow-off and snow-on images for estimation.\n\nFor the snow-off image, the length of the red pole is 136 pixels. If we assume that the pole is 304.8 cm in length, then each pixel is approximately 2.24 cm in length.\n\nFor the snow-on image, the length of the red pole is 72 pixels, much shorter than the snow-off length. So, there is a ~64 pixel difference between the snow-on and snow-off lengths. Using the equation in Figure 1, we can calculate snow depth:\n\nDepth = 2.24 * (136-72) = 143.36 cm\n\nAcknowledgements: Anthony Arendt, Scott Henderson, Micah Johnson, Carrie Vuyovich, Ryan Currier, Megan Mason, Mark Raleigh\n\nAdditional ReferencesDickerson-Lange et al., 2017. Snow disappearance timing is dominated by forest effects on snow accumulation in warm winter climates of the Pacific Northwest, United States. Hydrological Processes. Vol 31, Issue 10. 13 February 2017. \n\nDickerson‐Lange et al. (2017)\n\nRaleigh et al., 2013. Approximating snow surface temperature from standard temperature and humidity data: New possibilities for snow model and remote sensing evaluation. Water Resources Research. Vol 49, Issue 12. 07 November 2013.  \n\nRaleigh et al. (2013)","type":"content","url":"/notebooks/timelapse-camera-tutorial#time-lapse-camera-applications","position":11},{"hierarchy":{"lvl1":"Terrestrial Laser Scanning"},"type":"lvl1","url":"/notebooks/tls-data-access","position":0},{"hierarchy":{"lvl1":"Terrestrial Laser Scanning"},"content":"This notebook is designed to take terrestrial laser scanner (TLS) data from the SnowEx Alaska Campaigns and derive snow depth. The TLS data is provided in both a raw point cloud format and a processed DEM format. For this example, we will be focusing on the TLS DEMs.\n\nThe TLS data is available through the cloud on NSIDC, so we will be using the earthaccess package. The first example will involve a single TLS image for simplicity, then we will have a second example that examines multiple TLS scans from the campaigns.\n\n!pip install --upgrade earthaccess\n\nimport earthaccess\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport rioxarray as rxr\nimport shutil\nimport tempfile\nimport xarray as xr\n\nThe TLS data was gathered in Bonanza Creek near Fairbanks, AK in two months: October 2022 and March 2023. These months correspond to the snow-off and snow-on seasons, respectively. We will start by getting some sample snow-on TLS data from a single day.\n\n# Authenticate with Earthdata Login servers\nauth = earthaccess.login(strategy=\"interactive\")\n\n# Search for snow-on granules\nresults = earthaccess.search_data(\n    #short_name=\"SNEX23_BCEF_TLS\",\n    doi = \"10.5067/R466GRXNA61S\",\n    temporal=('2023-03-15', '2023-03-15'),\n)\n\nBecause the TLS data is available on-demand through the cloud, we do not need to download it. Instead, we can stream it directly with rioxarray!\n\n# Load a single TLS scan\nfiles = earthaccess.open(results)\nsnow_on = rxr.open_rasterio(files[1])\n\nsnow_on.rio.width\n\n# Visualize the snow-on data\nfig, ax = plt.subplots()\nsnow_on.plot(ax=ax, vmin=123, vmax=126,\n             cbar_kwargs={'label': \"Elevation [m]\"})\nax.set_xlabel(\"Easting [m]\")\nax.set_ylabel(\"Northing [m]\")\nax.set_title(\" \")\n\nTwo things are noticeable from this TLS data:\n\nIt has a very high resolution (0.15 m).\n\nThe signal attenutates after ~60 m, so we have a small field of view.\n\nThis suggests that we will be able to obtain very fine-scale measurements of snow depth, but we will need scans from multiple locations to better characterize snow in Bonanza Creek.\n\nIn any case, let’s grab the snow-off data from the same location, and try to derive snow depth.\n\n# Now search for snow-off granules\nresults = earthaccess.search_data(\n    #short_name=\"SNEX23_BCEF_TLS\",\n    doi = \"10.5067/R466GRXNA61S\",\n    temporal=('2022-10-25', '2022-10-25'),\n)\n\ndisplay(results)\n\n# Again, load a single snow-off TLS scan\nfiles = earthaccess.open(results)\nsnow_off = rxr.open_rasterio(files[1])\n\nfig, ax = plt.subplots()\nsnow_off.plot(vmin=123, vmax=126,\n              cbar_kwargs={'label': \"Elevation [m]\"})\nax.set_xlabel(\"Easting [m]\")\nax.set_ylabel(\"Northing [m]\")\nax.set_title(\" \")\n\nAlthough the snow-on/-off data look similar to each other, there are slight differences, meaning that we cannot perform a difference right away. We must first interpolate the data, ensuring that fill values are accounted for, then perform the difference.\n\n# Interpolate snow-on data onto the x/y grid of snow-off data\nsnow_on_interp = snow_on.interp(\n    x=snow_off.x,\n    y=snow_off.y,\n    kwargs={\"fill_value\": snow_on.attrs.get('_FillValue', np.nan)}\n)\n\n# Calculate the difference (snow depth)\ndifference = snow_on_interp - snow_off\n\n# Define fill values in data\nfill = snow_off.attrs.get('_FillValue', -9999.0)\n\n# Include only data that is not equal to the fill value\ndifference = difference.where((snow_off != fill) & (snow_on_interp != fill))\n\n# Plot snow depth over the TLS scene\nfig, ax = plt.subplots()\ndifference.plot(vmin=0, vmax=1.5,\n                cbar_kwargs={'label': \"Snow depth [m]\"})\nax.set_xlabel(\"Easting [m]\")\nax.set_ylabel(\"Northing [m]\")\nax.set_title(\" \")\n\nAlthough not perfect, this provides a very reasonable snow depth DEM for the TLS data gathered in this location. If we want, we can perform basic statistics on the derived snow depths.\n\n# Calculate median snow depth over the scene\nmedian_depth = difference.where(difference>=0).median()\n\n# Make histogram plot of snow depth\nfig, ax = plt.subplots()\ndifference.where(difference>=0).plot.hist(ax=ax, bins=50)\nax.axvline(x=median_depth, color='black', linewidth=2, linestyle='--') # Median depth line\nax.set_xlim([0, 2.5])\nax.set_ylabel(\"Counts\")\nax.set_xlabel(\"Snow depth [m]\")\nax.set_title(' ')\nax.text(1, 8000, f'Median depth = {median_depth:.2f} m', fontsize=12)\n\n","type":"content","url":"/notebooks/tls-data-access","position":1},{"hierarchy":{"lvl1":"Terrestrial Laser Scanning","lvl2":"Multiple Scans Example"},"type":"lvl2","url":"/notebooks/tls-data-access#multiple-scans-example","position":2},{"hierarchy":{"lvl1":"Terrestrial Laser Scanning","lvl2":"Multiple Scans Example"},"content":"Because we can stream the TLS data through the cloud, this example is very similar to the above code. The main exception is that we will generate a list of DataArrays, from which we derive snow depth for three TLS scanning locations.\n\n# Search for snow-on granules\nsnow_on_results = earthaccess.search_data(\n    #short_name=\"SNEX23_BCEF_TLS\",\n    doi = \"10.5067/R466GRXNA61S\",\n    temporal=('2023-03-01', '2023-03-31'),\n)\n\nsnow_off_results = earthaccess.search_data(\n    #short_name=\"SNEX23_BCEF_TLS\",\n    doi = \"10.5067/R466GRXNA61S\",\n    temporal=('2022-10-01', '2022-10-31'),\n)\n\n# Create list of snow-on DataArrays\nsnow_on_files = earthaccess.open(snow_on_results)\nsnow_on_rasters = [rxr.open_rasterio(f) for f in snow_on_files]\n\n# Create list of snow-off DataArrays\nsnow_off_files = earthaccess.open(snow_off_results)\nsnow_off_rasters = [rxr.open_rasterio(f) for f in snow_off_files]\n\nTo make the final plot of this example cleaner, we will assign each TLS scan a label based on the site ID at Bonanza Creek.\n\nsnon_site_ids = []\nsnoff_site_ids = []\n# Get site IDs for each snow-on DataArray\nfor f in snow_on_files:\n    # Get path from file name\n    path = f.path\n    # Use regex to extract the site ID from file path, given pattern _SW_YYYYMMDD_SITEID_V\n    m = re.search(r'_(SW|N)_\\d{8}_(.*?)_V', path)\n    if m:\n        snon_site_ids.append(m.group(2))\n    else:\n        snon_site_ids.append(\"unknown\")\n\n# Get site IDs for each snow-off DataArray\nfor f in snow_off_files:\n    # Step 1: Extract path\n    path = f.path\n    # Step 2: Use regex to extract the site ID\n    # Pattern: _SW_YYYYMMDD_SITEID_V\n    m = re.search(r'_(SW|N|NE)_\\d{8}_(.*?)_V', path)\n    if m:\n        snoff_site_ids.append(m.group(2))\n    else:\n        snoff_site_ids.append(\"unknown\")\n\nprint(snon_site_ids)\nprint(snoff_site_ids)\n\n# Add site ID to attributes of DataArrays\nfor r, site in zip(snow_on_rasters, snon_site_ids):\n    r.attrs['site_id'] = site\n\nfor r, site in zip(snow_off_rasters, snoff_site_ids):\n    r.attrs['site_id'] = site\n\n# Create dictionaries linking each DataArray to a site ID\nsnow_on_dict = {r.attrs['site_id']: r for r in snow_on_rasters}\nsnow_off_dict = {r.attrs['site_id']: r for r in snow_off_rasters}\n\nNow each TLS scan is linked to a site ID. However, we can see that the snow-on data has many more scans than the snow-off data. Because snow depth data is our priority, we will only consider snow-on scans that share a site ID with the snow-off data.\n\n# Determine site IDs with recorded data for both snow-off and snow-on season\ncommon_site_ids = sorted(set(snow_on_dict).intersection(snow_off_dict))\nprint(\"Common site IDs:\", common_site_ids)\n\n# Create lists of DataArrays for the common sites only\nsnow_on_paired = [snow_on_dict[sid] for sid in common_site_ids]\nsnow_off_paired = [snow_off_dict[sid] for sid in common_site_ids]\n\nNow that the site IDs are matched, deriving snow depth is the same as the first example, only with looping to make the calculation (and plotting) easier.\n\nsnow_depths = []\n# Interpolate DataArrays and derive snow depth, as before\nfor so, soff, site in zip(snow_on_paired, snow_off_paired, common_site_ids):\n    # Interpolate snow-on data onto the x/y grid of snow-off data\n    tmp_interp = so.interp(\n        x=soff.x,\n        y=soff.y,\n    )\n\n    tmp_diff = tmp_interp - soff\n    tmp_diff.attrs['site_id'] = site\n\n    tmp_diff = tmp_diff.where((tmp_diff[0]>0)&(tmp_diff[0]<=2))\n    snow_depths.append(tmp_diff)\n\n# Plot the derived snow depths in a 3x3 figure\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx, data_array in enumerate(snow_depths):\n    data_array.plot(ax=axes[idx], vmin=0, vmax=2)\n    axes[idx].set_title(f\"{snow_depths[idx].attrs['site_id']}\")\n\nplt.tight_layout()\nplt.show()\n\nThat’s all there is to it! Some of the coverage is a bit sparse, and the depths over site DEC look rather high, but we otherwise have reasonable snow depths over 9 sites in Bonanza Creek. These could then be compared to other ground based efforts or airborne data to cross-calibrate observation methods.","type":"content","url":"/notebooks/tls-data-access#multiple-scans-example","position":3},{"hierarchy":{"lvl1":"UAVSAR"},"type":"lvl1","url":"/notebooks/uavsar-accessing-imagery-pt1","position":0},{"hierarchy":{"lvl1":"UAVSAR"},"content":"\n\nDevelopers: Jack Tarricone, University of Nevada, Reno Zach Keskinen, Boise State University\n\nOther contributors: Ross Palomaki, Montana State UniversityNaheem Adebisi, Boise State University\n\n","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1","position":1},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"What is UAVSAR?"},"type":"lvl2","url":"/notebooks/uavsar-accessing-imagery-pt1#what-is-uavsar","position":2},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"What is UAVSAR?"},"content":"UAVSAR is a low frequency plane-based synthetic aperture radar. UAVSAR stands for “Uninhabited Aerial Vehicle Synthetic Aperture Radar”. It captures imagery using a L-band radar. This low frequency means it can penetrate into and through clouds, vegetation, and snow.\n\nfrequency (cm)\n\nresolution (rng x azi m)\n\nSwath Width (km)\n\nPolarizations\n\nLaunch date\n\nL-band 23\n\n1.8 x 5.5\n\n16\n\nVV, VH, HV, HH\n\n2007","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#what-is-uavsar","position":3},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"NASA SnowEx 2020 and 2021 UAVSAR Campaigns","lvl2":"What is UAVSAR?"},"type":"lvl3","url":"/notebooks/uavsar-accessing-imagery-pt1#nasa-snowex-2020-and-2021-uavsar-campaigns","position":4},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"NASA SnowEx 2020 and 2021 UAVSAR Campaigns","lvl2":"What is UAVSAR?"},"content":"During the winter of 2020 and 2021, NASA conducted an L-band InSAR timeseries across the Western US with the goal of tracking changes in SWE. Field teams in 13 different locations in 2020, and in 6 locations in 2021, deployed on the date of the flight to perform calibration and validation observations.\n\nThe site locations from the above map along with the \n\nUAVSAR defined campaign name and currently processed pairs of InSAR images for each site. Note that the image pair count may contain multiple versions of the same image and may increase as more pairs of images are processed by JPL. Also note that the Lowman campaign name is the wrong state when searching.\n\nSite Location\n\nCampaign Name\n\nImage Pairs\n\nGrand Mesa\n\nGrand Mesa, CO\n\n13\n\nBoise River Basin\n\nLowman, CO\n\n17\n\nFrazier Experimental Forest\n\nFraser, CO\n\n16\n\nSenator Beck Basin\n\nIronton, CO\n\n9\n\nEast River\n\nPeeler Peak, CO\n\n4\n\nCameron Pass\n\nRocky Mountains NP, CO\n\n15\n\nReynold Creek\n\nSilver City, ID\n\n1\n\nCentral Agricultral Research Center\n\nUtica, MT\n\n2\n\nLittle Cottonwoody Canyon\n\nSalt Lake City, UT\n\n21\n\nJemez River\n\nLos Alamos, NM\n\n3\n\nAmerican River Basin\n\nEldorado National Forest, CA\n\n4\n\nSagehen Creek\n\nDonner Memorial State Park, CA\n\n4\n\nLakes Basin\n\nSierra National Forest, CA\n\n3\n\n","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#nasa-snowex-2020-and-2021-uavsar-campaigns","position":5},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"Why would I use UAVSAR?"},"type":"lvl2","url":"/notebooks/uavsar-accessing-imagery-pt1#why-would-i-use-uavsar","position":6},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"Why would I use UAVSAR?"},"content":"UAVSAR works with low frequency radar waves. These low frequencies (< 3 GHz) can penetrate clouds and maintain coherence (a measure of radar image quality) over long periods. For these reasons, time series was captured over 13 sites as part of the winter of 2019-2020 and 2020-2021 for snow applications. Additionally the UAVSAR is awesome!\n\n","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#why-would-i-use-uavsar","position":7},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"Accessing UAVSAR Images"},"type":"lvl2","url":"/notebooks/uavsar-accessing-imagery-pt1#accessing-uavsar-images","position":8},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"Accessing UAVSAR Images"},"content":"UAVSAR imagery can be downloaded from both the \n\nJPL and \n\nAlaska Satellite Facility. However both provide the imagery in a binary format that is not readily usable or readable by GIS software or python libraries.","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#accessing-uavsar-images","position":9},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"Data Download and Conversion with uavsar_pytools"},"type":"lvl2","url":"/notebooks/uavsar-accessing-imagery-pt1#data-download-and-conversion-with-uavsar-pytools","position":10},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"Data Download and Conversion with uavsar_pytools"},"content":"uavsar_pytools (\n\nGithub) is a Python package developed out of work started at SnowEx Hackweek 2021. It nativiely downloads, formats, and converts this data in analysis ready rasters projected in WSG-84 Lat/Lon (\n\nEPSG:4326. The data traditionally comes in a binary format, which is not injestible by traditional geospatial analysis software (Python, R, QGIS, ArcGIS). It can download and convert either individual images - UavsarScene or entire collections of images - UavsarCollection.","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#data-download-and-conversion-with-uavsar-pytools","position":11},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"Netrc Authorization","lvl2":"Data Download and Conversion with uavsar_pytools"},"type":"lvl3","url":"/notebooks/uavsar-accessing-imagery-pt1#netrc-authorization","position":12},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"Netrc Authorization","lvl2":"Data Download and Conversion with uavsar_pytools"},"content":"In order to download uavsar images you will need a \n\nnetrc file that contains your earthdata username and password. If you need to register for a NASA earthdata account use this \n\nlink. A netrc file is a hidden file, it won’t appear in the your file explorer, that is in your home directory and that programs can access to get the appropriate usernames and passwords. While you’ll have already done this for the Hackweek virtual machines, uavsar_pytools has a tool to create this netrc file on a local computer. You only need to create this file once and then it should be permanently stored on your computer.\n\n# ## Creating .netrc file with Earthdata login information\n# from uavsar_pytools.uavsar_tools import create_netrc\n\n# # This will prompt you for your username and password and save this\n# # information into a .netrc file in your home directory. You only need to run\n# # this command once per computer. Then it will be saved.\n# create_netrc()\n\n","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#netrc-authorization","position":13},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"Downloading and converting a single UAVSAR interferogram scene","lvl2":"Data Download and Conversion with uavsar_pytools"},"type":"lvl3","url":"/notebooks/uavsar-accessing-imagery-pt1#downloading-and-converting-a-single-uavsar-interferogram-scene","position":14},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"Downloading and converting a single UAVSAR interferogram scene","lvl2":"Data Download and Conversion with uavsar_pytools"},"content":"You can find urls for UAVSAR images at the \n\nASF vertex website. Make sure to change the platform to UAVSAR and you may also want to filter to ground projected interferograms.\n\ntry:\n    from uavsar_pytools import UavsarScene\nexcept ModuleNotFoundError:\n    print('Install uavsar_pytools with `pip install uavsar_pytools`')\n\n## This is the directory you want to download and convert the images in.\nwork_dir = '/tmp/uavsar_data'\n\n## This is a url you want to download. Can be obtained from vertex\nurl = 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/\\\nlowman_23205_21009-004_21012-000_0007d_s01_L090_01_int_grd.zip'\n\n## clean = True will delete the binary and zip files leaving only the tiffs\nscene = UavsarScene(url = url, work_dir=work_dir, clean= True)\n\n## After running url_to_tiffs() you will download the zip file, unzip the binary \n## files, and convert them to geotiffs in the directory with the scene name in\n## the work directory. It also generate a .csv pandas dictionary of metadata.\n# scene.url_to_tiffs()\n\n","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#downloading-and-converting-a-single-uavsar-interferogram-scene","position":15},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"Downloading and converting a full UAVSAR collection","lvl2":"Data Download and Conversion with uavsar_pytools"},"type":"lvl3","url":"/notebooks/uavsar-accessing-imagery-pt1#downloading-and-converting-a-full-uavsar-collection","position":16},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"Downloading and converting a full UAVSAR collection","lvl2":"Data Download and Conversion with uavsar_pytools"},"content":"If you want to download and convert an entire Uavsar collection for a larger analysis you can use UavsarCollection. The collection names for the SnowEx campaign are listed in the table in the introduction. The UavsarCollection can download either InSAR pairs and PolSAR images.\n\nfrom uavsar_pytools import UavsarCollection\n## Collection name, the SnowEx Collection names are listed above. These are case \n## and space sensitive.\ncollection_name = 'Grand Mesa, CO'\n\n## Directory to save collection into. This will be filled with directory with \n## scene names and tiffs inside of them.\nout_dir = '/tmp/collection_ex/'\n\n## This is optional, but you will generally want to at least limit the date\n## range between 2019 and today.\ndate_range = ('2019-11-01', 'today')\n\n# Keywords: to download incidence angles with each image use `inc = True`\n# For only certain pols use `pols = ['VV','HV']`\n\ncollection = UavsarCollection(collection = collection_name, work_dir = out_dir, dates = date_range)\n\n## You can use this to check how many image pairs have at least one image in\n## the date range.\n\n#collection.find_urls()\n\n## When you are ready to download all the images run:\n\n# collection.collection_to_tiffs()\n\n## This will take a long time and a lot of space, ~1-5 gB and 10 minutes per \n## image pair depending on which scene, so run it if you have the space and time.","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#downloading-and-converting-a-full-uavsar-collection","position":17}]}