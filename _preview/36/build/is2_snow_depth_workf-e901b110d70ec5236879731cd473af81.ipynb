{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bacc892-0d6d-49d8-bda6-ba5b68261c1a",
   "metadata": {},
   "source": [
    "# Snow Depth Estimates with ICESat-2\n",
    "\n",
    "This notebook uses a combination of ICESat-2 and airborne lidar to derive snow depth. It uses data from the SnowEx 2023 campaign as an example, but can be applied to other locations if a shapefile or geoJSON is given.\n",
    "\n",
    "This notebook is adapted from the 2023 ICESat-2 Hackweek, originally developed by Zachary Fair and Karina Zikan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a1b15-8a4f-449f-af53-72dfc5caf8c3",
   "metadata": {},
   "source": [
    "## User input\n",
    "\n",
    "Acceptable field site IDs over Alaska are:\n",
    "* `cffl`: Creamer's Field/Farmer's Loop\n",
    "* `cpcrw`: Caribou/Poker Creek Experimental Watershed\n",
    "* `bcef`: Bonanza Creek Experimental Forest\n",
    "* `acp`: Arctic Coastal Plain\n",
    "* `utk`: Toolik Research Station\n",
    "\n",
    "Acceptable IDs for Sliderule ATL08 class (use numeric ID):\n",
    "* `No classification:` -1\n",
    "* `atl08_unclassified`: 0\n",
    "* `atl08_noise`: 1\n",
    "* `atl08_canopy`: 2\n",
    "* `atl08_top_of_canopy`: 3\n",
    "* `atl08_ground`: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9de8092c-392e-4e2a-9cee-7d2a877a02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field site ID\n",
    "field_id = 'data/toolik_lidar_boxes.geojson'\n",
    "\n",
    "# Snow-on (True) or snow-off (False) analysis\n",
    "snow_on = True\n",
    "\n",
    "# Use March UAF data ('mar') or October depths ('oct')\n",
    "uaf_depths = 'mar'\n",
    "\n",
    "# Base data path\n",
    "path = '/home/jovyan/icesat2-snowex'\n",
    "\n",
    "# Desired RGT and date range for data queries. Set rgt to \"all\" if\n",
    "# all ground tracks are desired\n",
    "date_range = ['2023-03-01', '2023-04-01']\n",
    "rgt = '1356'\n",
    "\n",
    "# SlideRule parameters (optional)\n",
    "cnf_surface = 4\n",
    "atl08_class = 4\n",
    "segment_length = 40\n",
    "res = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0050f-4764-4424-b1ca-7251e8548a53",
   "metadata": {},
   "source": [
    "A breakdown of the SlideRule parameters above:\n",
    "\n",
    "`cnf_surface`: The confidence level of the ICESat-2 photons.\n",
    "* High-confidence photons (recommended for snow): 4\n",
    "* High-/medium-confidence photons: 3\n",
    "* High-/medium-/low-confidence photons: 2\n",
    "* Signal photons (high/medium/low) and noise: 1\n",
    "* Signal photons, noise, and solar background (not recommended): 0\n",
    "\n",
    "`segment_length`: The along-track length to sample and aggregate photons, in meters. Currently set at 40 m, the resolution of the ATL06 product.\n",
    "\n",
    "`res`: The along-track resolution of the returned data product. Currently set at 20 m to match ATL06."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4cf0a5-f283-4655-b480-81ad16bf259b",
   "metadata": {},
   "source": [
    "## Read ICESat-2 data\n",
    "To load the ICESat-2 data with minimal effort from the user, we will use SlideRule in the below function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641af757-cb54-470d-bb82-c8b0834ff6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from sliderule import sliderule, icesat2\n",
    "\n",
    "def atl06srq(field_geojson, date_range, rgt, cnf_surface, atl08_class, \n",
    "             segment_length, res):\n",
    "    # Initiate SlideRule\n",
    "    icesat2.init('slideruleearth.io', verbose=False)\n",
    "\n",
    "    # Load geoJSON for site of interest\n",
    "    region = sliderule.toregion(field_geojson)['poly']\n",
    "\n",
    "    # Convert user-defined ATL08 class ID to string readable by SlideRule\n",
    "    atl08_ids = {-1: 'None',\n",
    "                 0: 'atl08_unclassified',\n",
    "                 1: 'atl08_noise',\n",
    "                 2: 'atl08_canopy',\n",
    "                 3: 'atl08_top_of_canopy',\n",
    "                 4: 'atl08_ground'}\n",
    "\n",
    "    # Construct dictionary of parameters\n",
    "    time_root = 'T00:00:00Z'\n",
    "    parms = {\n",
    "             \"poly\": region,\n",
    "             \"srt\": icesat2.SRT_LAND,\n",
    "             \"cnf\": cnf_surface,\n",
    "             \"len\": segment_length,\n",
    "             \"res\": res,\n",
    "             \"t0\": date_range[0]+time_root,\n",
    "             \"t1\": date_range[1]+time_root\n",
    "            }\n",
    "\n",
    "    # Check if all RGTs are considered, or only a subset\n",
    "    if rgt != \"all\":\n",
    "        parms[\"rgt\"] = rgt\n",
    "        print(f\"Subsetting to only include ICESat-2 RGT {rgt}.\")\n",
    "\n",
    "    # Check for ATL08 filter\n",
    "    if atl08_ids.get(atl08_class) != \"None\":\n",
    "        parms[\"atl08_class\"] = atl08_ids.get(atl08_class)\n",
    "        print(\"Subsetting by selected ATL08 filter...\")\n",
    "\n",
    "    # Query SlideRule\n",
    "    atl06sr = icesat2.atl06p(parms)\n",
    "\n",
    "    return atl06sr \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933cd46b-512b-4c60-9fab-529764c27cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning, this environment is using an outdated client (v4.19.1). The code will run but some functionality supported by the server (v4.20.0) may not be available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting to only include ICESat-2 RGT 1356.\n",
      "Subsetting by selected ATL08 filter...\n"
     ]
    }
   ],
   "source": [
    "\"\"\" # Generate ICESat-2 data from SlideRule\n",
    "atl06sr = atl06srq(field_id, date_range, rgt,\n",
    "                 cnf_surface=cnf_surface,\n",
    "                 atl08_class=atl08_class,\n",
    "                 segment_length=segment_length,\n",
    "                 res=res) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d8006-1871-4604-b2cf-6ba466cf1e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Convert ATL06SR to geodataframe in EPSG:32606\n",
    "atl06sr['lon'], atl06sr['lat'] = atl06sr.geometry.x, atl06sr.geometry.y\n",
    "atl06sr_gdf = atl06sr.to_crs('epsg:32606') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba514bdc-5a7d-4660-8b51-97dfdb78f21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty GeoDataFrame\n",
       "Columns: [geometry, lon, lat]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# atl06sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c7d2b8-606f-43ec-8b62-d472f7b8d870",
   "metadata": {},
   "source": [
    "## Read Airborne Lidar Data\n",
    "\n",
    "To derive snow depth with ICESat-2, we need a snow-off digital elevation model (DEM), which commonly originates from airborne lidar. This next step is designed to load and prepare some airborne lidar data from the University of Alaska, Fairbanks for this analysis.\n",
    "\n",
    "The method currently presented uses `earthaccess` to stream the data without any download required. However, this process can be slow on a local machine, and may be memory-intensive on a cloud environment. A more streamlined workflow utilizing the SnowEx Database is in the works, though the given method is most effective for non-SnowEx DEM sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd0059-afd6-4c6f-916a-b98f0c69f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import earthaccess\n",
    "import xarray as xr\n",
    "earthaccess.login(strategy='interactive', persist=True)\n",
    "auth = earthaccess.login() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726c65c-51e9-41f2-a6d9-62f76dbd8416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" region = sliderule.toregion(field_id)['poly']\n",
    "coords = [(point[\"lon\"], point[\"lat\"]) for point in region] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4cd651-9faf-453a-acc4-021348de26af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Coordinates for SW/NE corners\\nlon_min = min([coord[0] for coord in coords])\\nlat_min = min([coord[1] for coord in coords])\\nlon_max = max([coord[0] for coord in coords])\\nlat_max = max([coord[1] for coord in coords])\\n\\n# Data query for lidar snow depth over Fairbanks, AK\\nresults = earthaccess.search_data(\\n    short_name='SNEX23_Lidar',\\n    bounding_box = (lon_min, lat_min, lon_max, lat_max),\\n    temporal = ('2023-03-10', '2023-03-15')\\n)\\n\\nfiles = earthaccess.open(results)\\nlidar_snow_on = xr.open_dataset(files[1], engine='rasterio') \""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Coordinates for SW/NE corners\n",
    "lon_min = min([coord[0] for coord in coords])\n",
    "lat_min = min([coord[1] for coord in coords])\n",
    "lon_max = max([coord[0] for coord in coords])\n",
    "lat_max = max([coord[1] for coord in coords])\n",
    "\n",
    "# Data query for lidar snow depth over Fairbanks, AK\n",
    "results = earthaccess.search_data(\n",
    "    short_name='SNEX23_Lidar',\n",
    "    bounding_box = (lon_min, lat_min, lon_max, lat_max),\n",
    "    temporal = ('2023-03-10', '2023-03-15')\n",
    ")\n",
    "\n",
    "files = earthaccess.open(results)\n",
    "lidar_snow_on = xr.open_dataset(files[1], engine='rasterio') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607450f1-4f31-4e7d-962c-ae518d989923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lidar_snow_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c515c49-fdbf-457d-93c4-71c9869dcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Data query for lidar snow-off elevation over Fairbanks, AK\n",
    "results = earthaccess.search_data(\n",
    "    short_name='SNEX23_Lidar',\n",
    "    bounding_box = (lon_min, lat_min, lon_max, lat_max),\n",
    "    temporal = ('2022-05-20', '2022-05-31')\n",
    ")\n",
    "\n",
    "# Open the resulting GeoTiff into Xarray\n",
    "files = earthaccess.open(results)\n",
    "lidar_snow_off = xr.open_dataset(files[1], engine='rasterio') \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed880e3-a179-478c-b92e-556fc65a8577",
   "metadata": {},
   "source": [
    "## Co-Locate ICESat-2 and UAF Lidar\n",
    "\n",
    "For this step, we will co-locate ICESat-2 and UAF so that we can directly compare the two datasets. The co-location will use a statistical method called \"spline interpolation\", and we will perform this co-location with both the snow-on and snow-off data.\n",
    "\n",
    "The below function has the code needed to perform the co-location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d2a56-e91d-496c-880a-ca67f0f94464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Packages needed for the below functions\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "\n",
    "def colocate_is2(lidar_snow_off, lidar_snow_on, is2_data):\n",
    "    # Ensure lidar/ICESat-2 projections match\n",
    "    if is2_data.crs!=32606:\n",
    "        is2_data.to_crs(\"EPSG:32606\", inplace=True)\n",
    "    \n",
    "    # Define x/y coordinates from snow-off data\n",
    "    x0, y0 = np.array(lidar_snow_off.x), np.array(lidar_snow_off.y)\n",
    "\n",
    "    # Do the same, but for the snow depth data\n",
    "    xs, ys = np.array(lidar_snow_on.x), np.array(lidar_snow_on.y)\n",
    "\n",
    "    # Remove filler values that would mess up the interpolator\n",
    "    dem_heights = np.array(lidar_snow_off['band_data'].sel(band=1))[::-1,:]\n",
    "    dem_heights[np.isnan(dem_heights)] = -9999\n",
    "    dem_depths = np.array(lidar_snow_on['band_data'].sel(band=1))[::-1,:]\n",
    "    dem_depths[np.isnan(dem_depths)] = -9999\n",
    "\n",
    "    # Generate interpolator schemes\n",
    "    interp_height = RectBivariateSpline(np.array(y0)[::-1], \n",
    "                                        np.array(x0),\n",
    "                                        dem_heights)\n",
    "    interp_depth = RectBivariateSpline(np.array(ys)[::-1],\n",
    "                                       np.array(xs),\n",
    "                                       dem_depths)\n",
    "\n",
    "    # Use the spline interpolator to align the lidar with ICESat-2\n",
    "    is2_lidar_df = pd.DataFrame()\n",
    "    for beam in np.unique(is2_data['gt']):\n",
    "        # Subset ICESat-2 data by current beam\n",
    "        is2_tmp = is2_data.loc[is2_data['gt']==beam]\n",
    "\n",
    "        # ICESat-2 x/y coordinates\n",
    "        xn, yn = is2_tmp.geometry.x, is2_tmp.geometry.y\n",
    "\n",
    "        # Define indices within x/y bounds of DEM\n",
    "        i1 = (xn>np.min(x0)) & (xn<np.max(x0))\n",
    "        i1 &= (yn>np.min(y0)) & (yn<np.max(y0))\n",
    "\n",
    "        # Estimate lidar elevation and snow depth along ICESat-2 track\n",
    "        lidar_height = interp_height(yn[i1], xn[i1], grid=False)\n",
    "        lidar_depth = interp_depth(yn[i1], xn[i1], grid=False)\n",
    "\n",
    "        # Construct dataframe of ICESat-2 and lidar data\n",
    "        tmp = pd.DataFrame(data={'lat': is2_tmp['lat'][i1],\n",
    "                                 'lon': is2_tmp['lon'][i1],\n",
    "                                 'x': xn[i1],\n",
    "                                 'y': yn[i1],\n",
    "                                 'rgt': is2_tmp['rgt'][i1],\n",
    "                                 'beam': is2_tmp['gt'][i1],\n",
    "                                 'is2_height': is2_tmp['h_mean'][i1],\n",
    "                                 'n_fit_photons': is2_tmp['n_fit_photons'][i1],\n",
    "                                 'h_sigma': is2_tmp['h_sigma'][i1],\n",
    "                                 'dh_fit_dx': is2_tmp['dh_fit_dx'][i1],\n",
    "                                 'lidar_height': lidar_height,\n",
    "                                 'lidar_snow_depth': lidar_depth\n",
    "                                    }\n",
    "                              )\n",
    "        # Concatenate the co-located data into  final DataFrame\n",
    "        is2_lidar_df = pd.concat([is2_lidar_df, tmp])\n",
    "\n",
    "    return is2_lidar_df \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb218f2-3cb0-4de1-a1b9-70861e15fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Use the above function to co-locate the airborne lidar and ICESat-2\n",
    "atl06sr_uaf = colocate_is2(lidar_snow_off, lidar_snow_on, atl06sr)\n",
    "\n",
    "# Estimate the ICESat-2 snow depth\n",
    "atl06sr_uaf['is2_snow_depth'] = atl06sr_uaf['is2_height'] - atl06sr_uaf['lidar_height']\n",
    "\n",
    "# Convert final DataFrame in GeoDataFrame\n",
    "atl06sr_uaf_gdf = gpd.GeoDataFrame(atl06sr_uaf,\n",
    "                                   geometry=gpd.points_from_xy(atl06sr_uaf.lon, atl06sr_uaf.lat),\n",
    "                                   crs='EPSG:4326')\n",
    "\n",
    "atl06sr_uaf_gdf \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba4967d-8967-429d-9976-cad271a52895",
   "metadata": {},
   "source": [
    "An outline of the variables in our current GeoDataFrame:\n",
    "* `lat` and `lon`: The latitude and longitude along the ICESat-2 track.\n",
    "* `x` and `y`: The easting and northing along the ICESat-2 track, in projection EPSG:32606.\n",
    "* `rgt`: The reference ground track number of the ICESat-2 track of interest.\n",
    "* `beam`: The ICESat-2 beam designation (gt1l, gt2r, etc.)\n",
    "* `is2_height`: ICESat-2 height estimate at the given location.\n",
    "* `n_fit_photons`: Number of ICESat-2 photons used to derive `is2_height`.\n",
    "* `h_sigma`: Approximate uncertainty of `is2_height`.\n",
    "* `dh_fit_dx`: A rough measure of surface slope along the ICESat-2 track.\n",
    "* `lidar_height`: Lidar height estimate at the given location.\n",
    "* `lidar_snow_depth`: Lidar snow depth estimate at the given location.\n",
    "* `is2_snow_depth`: ICESat-2 snow depth estimate at the given location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8fc22-9d01-4c33-a361-8eda7b9e25c7",
   "metadata": {},
   "source": [
    "The key variables are `is2_snow_depth` and `lidar_snow_depth` for our comparisons. Several of the other variables, such as `n_fit_photons` and `h_sigma`, can be used to filter or process the data further, if desired.\n",
    "\n",
    "Let's look at a simple comparison between the two depth products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85a430-2601-4d33-93a3-cf82f5476e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Remove sub-zero values\n",
    "atl06sr_uaf_gdf = atl06sr_uaf_gdf[atl06sr_uaf_gdf['is2_snow_depth']>=0]\n",
    "atl06sr_uaf_gdf = atl06sr_uaf_gdf[atl06sr_uaf_gdf['lidar_snow_depth']>=0] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9bca8-0eaa-4037-a2e5-91b3ad5f3c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import matplotlib.pyplot as plt\n",
    "\n",
    "single_beam = atl06sr_uaf_gdf[atl06sr_uaf_gdf['beam']==60]\n",
    "\n",
    "# Line plot of along-track snow depths\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "single_beam.plot(kind='line', ax=ax, x='lat', y='is2_snow_depth',\n",
    "                     linewidth=3, label='ICESat-2')\n",
    "single_beam.plot(kind='line', ax=ax, x='lat', y='lidar_snow_depth',\n",
    "                     linewidth=1.5, color='orange', label='UAF lidar')\n",
    "ax.set_xlabel('Latitude', fontsize=14)\n",
    "ax.set_ylabel('Snow depth [m]', fontsize=14)\n",
    "ax.set_ylim([0, 2])\n",
    "ax.legend() \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ab94b-548e-4a67-8db2-c587ec004e3b",
   "metadata": {},
   "source": [
    "We can also calculate the difference in snow depth between ICESat-2 and UAF, then make a spatial plot using `geopandas.explore()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17703e1c-0503-4f7a-86da-6f9a42940d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Calculate snow depth bias\n",
    "atl06sr_uaf_gdf['snow_depth_residual'] = atl06sr_uaf_gdf['is2_snow_depth'] - atl06sr_uaf_gdf['lidar_snow_depth']\n",
    "\n",
    "# Create a spatial plot of the snow depth bias\n",
    "atl06sr_uaf_gdf.explore(column='snow_depth_residual', \n",
    "                        tiles='Esri.WorldImagery',\n",
    "                        cmap='viridis',\n",
    "                        vmin=-1.5, vmax=1.5) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f51a821-b268-4715-9fb0-8f0c4c324a92",
   "metadata": {},
   "source": [
    "If the data looks good, then we can save the final GeoDataFrame as a geoJSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544bc5e5-94d2-41d1-aa5c-f7f595c61266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Save the GeoDataFrame\n",
    "atl06sr_uaf_gdf.to_file(f'{path}/is2_uaf_snow-depths.geojson',\n",
    "                        driver='GeoJSON') \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cookbook-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
