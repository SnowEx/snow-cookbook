{"version":"1","records":[{"hierarchy":{"lvl1":"Snow Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Snow Cookbook"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Snow Cookbook"},"type":"lvl1","url":"/#snow-cookbook","position":2},{"hierarchy":{"lvl1":"Snow Cookbook"},"content":"\n\n\n\n\n\n\n\nThis Project Pythia Cookbook is a compilation of tutorials and training\nmaterials in support of the NASA snow reserach community. Some tutorials\ncome from the 2020 to 2024 SnowEx Hackweek program hosted at the UW eScience\nInstitute. Other materials are drawn from the NASA Goddard “SnowPit” Science\nTask Group or STG. The purpose of the tutorials is to help people with data\naccess and to demonstrate a variety of disciplinary use cases.","type":"content","url":"/#snow-cookbook","position":3},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":4},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Motivation"},"content":"There are numerous data products and methods for accessing and analyzing\nsnow observations. These include field, airborne, and satellite missions.\nThe goal of these tutorials is to streamline data access, reduce duplication\nof effort and build an open science community around snow research\ndatasets, algorithms and software.","type":"content","url":"/#motivation","position":5},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":6},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Authors"},"content":"Zach Fair\n\n\nAnthony Arendt,\n\n\nMark Welden-Smith\n\nmore to be added","type":"content","url":"/#authors","position":7},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":8},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":9},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":10},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Structure"},"content":"This cookbook is broken up into three main sections: “Data Access”, “Observations”, and “Analysis and Machine Learning”. The current listing of subtopics is currently a work in progress.","type":"content","url":"/#structure","position":11},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Section 1: Data Access","lvl2":"Structure"},"type":"lvl3","url":"/#section-1-data-access","position":12},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Section 1: Data Access","lvl2":"Structure"},"content":"Field Campaigns Overview\n\nSnowExSQL Database","type":"content","url":"/#section-1-data-access","position":13},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Section 2: Observations","lvl2":"Structure"},"type":"lvl3","url":"/#section-2-observations","position":14},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Section 2: Observations","lvl2":"Structure"},"content":"GPR and Lidar\n\nTime-lapse Cameras\n\nUAVSAR\n\nMicrostructure\n\nAVIRIS-NG\n\nTerrestrial Laser Scanning","type":"content","url":"/#section-2-observations","position":15},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Section 3: Analysis and Machine Learning","lvl2":"Structure"},"type":"lvl3","url":"/#section-3-analysis-and-machine-learning","position":16},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Section 3: Analysis and Machine Learning","lvl2":"Structure"},"content":"Neural Networks with PyTorch\n\nSnow Modeling\n\nUCLA Reanalysis\n\nMERRA-2\n\nERA5","type":"content","url":"/#section-3-analysis-and-machine-learning","position":17},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":18},{"hierarchy":{"lvl1":"Snow Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using\n\n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":19},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":20},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of\nhow this works are not important for now. All you need to know is how to launch\na Pythia Cookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.\n\nNote, not all Cookbook chapters are executable. If you do not see\nthe rocket ship icon, such as on this page, you are not viewing an\nexecutable book chapter.","type":"content","url":"/#running-on-binder","position":21},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":22},{"hierarchy":{"lvl1":"Snow Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer,\nyou will need to follow this workflow:\n\nClone the https://github.com/ProjectPythia/snow-cookbook repository: git clone https://github.com/ProjectPythia/snow-cookbook.git\n\nMove into the snow-cookbook directorycd snow-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate snow-cookbook\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":23},{"hierarchy":{"lvl1":"GPR and Lidar"},"type":"lvl1","url":"/notebooks/gpr-lidar-hackweektutorial","position":0},{"hierarchy":{"lvl1":"GPR and Lidar"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial","position":1},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"Author: Randall Bonnell"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#author-randall-bonnell","position":2},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"Author: Randall Bonnell"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#author-randall-bonnell","position":3},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"Outline:"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#outline","position":4},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"Outline:"},"content":"GPR Methods for the Retrieval of Snow Depth and SWE\n\nLidar Methods for Snow Depth Retrieval and SWE Estimation\n\nLeveraging Coincident GPR and Lidar Data Sets to Derive Snow Density\n\nSnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska\n\nDiscussion: Improving Density Estimation\n\nGPR SnowEx Analysis-Ready Datasets\n\nReferences\n\n\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#outline","position":5},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-1-gpr-methods-for-the-retrieval-of-snow-depth-and-swe","position":6},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-1-gpr-methods-for-the-retrieval-of-snow-depth-and-swe","position":7},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Previous GPR tutorial developed by Tate Meehan (CRREL) that may be of interest: https://​snowex​-2021​.hackweek​.io​/tutorials​/gpr​/gpr​.html","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#previous-gpr-tutorial-developed-by-tate-meehan-crrel-that-may-be-of-interest-https-snowex-2021-hackweek-io-tutorials-gpr-gpr-html","position":8},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Previous GPR tutorial developed by Tate Meehan (CRREL) that may be of interest: https://​snowex​-2021​.hackweek​.io​/tutorials​/gpr​/gpr​.html","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#previous-gpr-tutorial-developed-by-tate-meehan-crrel-that-may-be-of-interest-https-snowex-2021-hackweek-io-tutorials-gpr-gpr-html","position":9},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"SnowEx Review","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#snowex-review","position":10},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"SnowEx Review","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"Ground-based, airborne, and satellite radars were operated as part of the NASA SnowEx campaigns.\n\nGround-based radars included ground-penetrating radar (GPR), frequency-modulated continuous-wave radar (FMCW), and tower mounted radars.\n\nWhat airborne and satellite radars were tasked?","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#snowex-review","position":11},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"A Brief Blurb on Radar Physics","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#a-brief-blurb-on-radar-physics","position":12},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"A Brief Blurb on Radar Physics","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"Radar is fully transmissible in dry snow, but there is frequency-dependent interaction between the radar signal and the snowpack.\n\nAt L-band frequencies (1–2 GHz, ~25 cm wavelength) there limited to no interaction with the snowpack.","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#a-brief-blurb-on-radar-physics","position":13},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"What is GPR?","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#what-is-gpr","position":14},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"What is GPR?","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"We use L-band GPR, which was operated during all SnowEx campaigns!\n\nGPR transmits a radar signal into the snowpack, which then reflects off objects/interfaces with contrasting dielectric permittivity. The GPR records the amplitude and two-way travel time (twt) of the reflections.\n\nDielectric permittivity refers to the dielectric properties of the snowpack that define how EM energy transmits through the medium.\n\nUsually, we are interested in the snow-ground interface and we measure the snowpack thickness in twt (nanoseconds).\n\nHowever, in complex vegetation, radargrams are difficult to interpret! Causes increased uncertainty.\n\nSee radargram examples below for the boreal forest GPR surveys (credit Kajsa Holland-Goon).\n\n\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#what-is-gpr","position":15},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Snow Depth, SWE, and Density Calculations","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#snow-depth-swe-and-density-calculations","position":16},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Snow Depth, SWE, and Density Calculations","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"To calculate snow depth (d_s) from twt, we need to estimate the relative permittivity (\\epsilon_s) and radar velocity (v_s) of the snowpack:\n\nv_s = \\frac{c}{\\sqrt{\\epsilon_s}}; --> Where c is the velocity of EM energy in a vacuum.\n\n\\epsilon_s = (1+\\frac{0.845\\rho_s}{1000})^2; --> Kovacs et al. (1995), but more than 19 equations exist for dry snow conditions.\n\nd_s = \\frac{twt}{2}*v_s;\n\nSWE = d_s\\rho_s;--> Where SWE is snow water equivalent.\n\nBut...If we know the snow depth, we can constrain the radar velocity and estimate relative permittivity and density!\n\n\\epsilon_s=(\\frac{c*twt}{2d_s})^2\n\n\\rho_s=(\\sqrt{\\epsilon_s}-1)\\frac{1000}{0.845}\n\nHow can we find the snow depth?","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#snow-depth-swe-and-density-calculations","position":17},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"A Shameless Plug...","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#a-shameless-plug","position":18},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"A Shameless Plug...","lvl2":"1. GPR Methods for the Retrieval of Snow Depth and SWE"},"content":"Most analysis-ready GPR products have twt, snow depth, and snow water equivalent. Some have been updated with derived snow densities. See 6. SnowEx GPR Analysis-Ready Datasets below.\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#a-shameless-plug","position":19},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-2-lidar-methods-for-snow-depth-retrieval-and-swe-estimation","position":20},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-2-lidar-methods-for-snow-depth-retrieval-and-swe-estimation","position":21},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Previous lidar tutorial developed by Naheem Adebisi (ESRI) that may be of interest: https://​snowex​-2022​.hackweek​.io​/tutorials​/lidar​/index​.html","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#previous-lidar-tutorial-developed-by-naheem-adebisi-esri-that-may-be-of-interest-https-snowex-2022-hackweek-io-tutorials-lidar-index-html","position":22},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Previous lidar tutorial developed by Naheem Adebisi (ESRI) that may be of interest: https://​snowex​-2022​.hackweek​.io​/tutorials​/lidar​/index​.html","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#previous-lidar-tutorial-developed-by-naheem-adebisi-esri-that-may-be-of-interest-https-snowex-2022-hackweek-io-tutorials-lidar-index-html","position":23},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"A (Very) General Review of Lidar","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#a-very-general-review-of-lidar","position":24},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"A (Very) General Review of Lidar","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"content":"Lidar emits photons and measures the twt of the returned photons\n\nThese twt are converted to elevation surfaces (e.g., DEM, DTM, DSM).\n\nLidar can be collected from a variety of platforms:\n\nTerrestrial\n\nUAV\n\nAirborne\n\nSatellite\n\nTwo acquisitions are required for snow, a snow-on acquisition and a snow-off acquisition. Snow depth can be calculated in two general ways:\n\nRaster-based approaches (see figure below, credit Airborne Snow Observatories Inc.)\n\nPoint cloud approaches","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#a-very-general-review-of-lidar","position":25},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"How is SWE calculated from lidar snow depths?","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#how-is-swe-calculated-from-lidar-snow-depths","position":26},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"How is SWE calculated from lidar snow depths?","lvl2":"2. Lidar Methods for Snow Depth Retrieval and SWE Estimation"},"content":"At larger scales, SWE is calculated via modeled densities (e.g., M3 Works and ASO).\n\nAt smaller field sites, it may be appropriate to use representative in situ measurements.\n\n\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#how-is-swe-calculated-from-lidar-snow-depths","position":27},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"3. Leveraging Coincident GPR and Lidar Data Sets to Derive Snow Density"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-3-leveraging-coincident-gpr-and-lidar-data-sets-to-derive-snow-density","position":28},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"3. Leveraging Coincident GPR and Lidar Data Sets to Derive Snow Density"},"content":"Density, liquid water content, and relative permittivity are understudied relative to snow depth and/or SWE.\n\nCombined coincident snow depths and twt can yield spatially distributed measurements of relative permittivity.\n\nIn wet snow, relative permittivity can be converted to liquid water content (e.g., Webb et al., 2018, 2020, 2022; Bonnell et al., 2021).\n\nIn dry snow, density can be estimated from the relative permittivity (Yildiz et al., 2021; McGrath et al., 2022; Bonnell et al., 2023; Meehan et al., 2024).\n\nThis technique has provided an unprecedented glimpse into the spatial properties of these parameters!\n\nCritically, studies have noted a large random error in derived products that should be considered (see figure below, credit: Meehan et al., 2024).\n\n\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-3-leveraging-coincident-gpr-and-lidar-data-sets-to-derive-snow-density","position":29},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-4-snowex23-gpr-lidar-derived-permittivities-densities-in-the-boreal-forest-alaska","position":30},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-4-snowex23-gpr-lidar-derived-permittivities-densities-in-the-boreal-forest-alaska","position":31},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Here, we will use this approach to derive densities at Farmer’s Loop Creamer’s Field during the SnowEx23 Alaska Campaign","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#here-we-will-use-this-approach-to-derive-densities-at-farmers-loop-creamers-field-during-the-snowex23-alaska-campaign","position":32},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Here, we will use this approach to derive densities at Farmer’s Loop Creamer’s Field during the SnowEx23 Alaska Campaign","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"Lidar data was collected on 11 March 2023\n\nGPR data was collected on 7, 11, 13, and 16 March 2023\n\n\"\"\" #1.1 Load relevant packages\nimport os\nimport numpy as np \nfrom datetime import date\nfrom scipy.spatial import cKDTree\n\n#packages for figures\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\n\n#geospatial packages\nimport geopandas as gpd #for vector data\nimport xarray as xr\nimport rioxarray #for raster data\nimport pandas as pd\nfrom shapely.geometry import box, Point\nimport rasterio as rio\n\n#Import SnowEx database\nfrom snowexsql.api import PointMeasurements, LayerMeasurements, RasterMeasurements\n \"\"\"\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#here-we-will-use-this-approach-to-derive-densities-at-farmers-loop-creamers-field-during-the-snowex23-alaska-campaign","position":33},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Part 1: Load the GPR data from the SnowEx data base","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#part-1-load-the-gpr-data-from-the-snowex-data-base","position":34},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Part 1: Load the GPR data from the SnowEx data base","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"-Huge thank you to Micah Johnson and Micah Sandusky for their support!\n\nNote that if we used the full GPR/Lidar dataset, we would need to allocate way more memory. This example focuses on a single date of collection in very dense forest.\n\nExamine the headers from the GPR csv --> what are the variables that we are interested in?\n\n\"\"\" # 1.2 Load GPR data\n\n#Note, memory space is fairly limited, will need to pull only one date\n\n#Set a number of dates to pull GPR for\n#dt1 = date(2023, 3, 7)\ndt2 = date(2023, 3, 11)\n#dt3 = date(2023, 3, 13)\n#dt4 = date(2023, 3, 16)\n\n#site1 = LayerMeasurements.from_filter(date=dt1, site_name='Fairbanks', site_id='FLCF', limit=1)\nsite2 = LayerMeasurements.from_filter(date=dt2, site_name='Fairbanks', site_id='FLCF', limit=1)\n#site3 = LayerMeasurements.from_filter(date=dt3, site_name='Fairbanks', site_id='FLCF', limit=1)\n#site4 = LayerMeasurements.from_filter(date=dt4, site_name='Fairbanks', site_id='FLCF', limit=1)\n\n#Use pandas ot read in csv data\n#gpr_df_dt1 = PointMeasurements.from_area(pt=site1.geometry[0], crs=26906, buffer=10000,\n#    type='two_way_travel',\n#    observers='Randall Bonnell',\n#    date=dt1, site_name='farmers-creamers',\n#    limit=29432)#The number of expected measurements\ngpr_df_dt2 = PointMeasurements.from_area(pt=site2.geometry[0], crs=26906, buffer=10000,\n    type='two_way_travel',\n    observers='Randall Bonnell',\n    date=dt2, site_name='farmers-creamers',\n    limit=20213)#The number of expected measurements\n#gpr_df_dt3 = PointMeasurements.from_area(pt=site3.geometry[0], crs=26906, buffer=10000,\n#    type='two_way_travel',\n#    observers='Randall Bonnell',\n#    date=dt3, site_name='farmers-creamers',\n#    limit=19024)\n#gpr_df_dt4 = PointMeasurements.from_area(pt=site4.geometry[0], crs=26906, buffer=10000,\n#    type='two_way_travel',\n#    observers='Randall Bonnell',\n#    date=dt4, site_name='farmers-creamers',\n#    limit=15785)\n\n\n#Compile into one dataframe\n#flcf_gpr_df = pd.concat([gpr_df_dt1,gpr_df_dt2,gpr_df_dt3,gpr_df_dt4],axis=0, join='outer', ignore_index=True, keys=None, levels=None,names=None,verify_integrity=False,sort=False,copy=None)\nflcf_gpr_df = gpr_df_dt2\n#Print out the csv headers and initial entries --> What's important here and what do we need?\nprint(flcf_gpr_df.head()) \"\"\"\n\n\"\"\" # Let's look at the distribution of gpr two-way travel times and estimated snow depths\n#Estimate snow depths from twt by assuming a velocity of 0.25 m/ns --> Is this an appropriate velocity estimate?\nflcf_gpr_df['Depth_estimated'] = (flcf_gpr_df['value']/2)*0.25\n\nax1 = flcf_gpr_df.plot.hist(column=[\"value\"], edgecolor='black', title='two-way travel time (ns)')\nax2 = flcf_gpr_df.plot.hist(column=[\"Depth_estimated\"], edgecolor='black', title='Snow depth (m)') \"\"\"\n\n# #Extract x/y limits from GPR data --> these will be used when loading the lidar snow depths\n# bounds = flcf_gpr_df.total_bounds\n\n# # Create a bounding box\n# gpr_limits = box(*bounds)\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#part-1-load-the-gpr-data-from-the-snowex-data-base","position":35},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Let’s load in the lidar canopy heights and snow depths.","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#lets-load-in-the-lidar-canopy-heights-and-snow-depths","position":36},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Let’s load in the lidar canopy heights and snow depths.","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"We’ll look at the canopy heights to get an idea of what kind of forest the data were collected in.\n\nThen, we’ll look at the lidar snow depths to visualize the snow distribution.","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#lets-load-in-the-lidar-canopy-heights-and-snow-depths","position":37},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Discussion questions:","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#discussion-questions","position":38},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Discussion questions:","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"What type of survey design was implemented for the GPR?\n\nDo the lidar snow depth patterns seem to exhibit any kind of dependence upon the forest cover?\n\n\"\"\" # 1.3 Load Lidar vegetation/canopy heights --> This may take a few minutes\n#Read in the canopy heights raster from Farmer's Loop/Creamer's Field Alaska\nflcf_ch = RasterMeasurements.from_area(shp = gpr_limits, crs=26906,\n    buffer=None, type='canopy_height',\n    site_name='farmers-creamers',\n    observers='chris larsen')\nprint(flcf_ch)\n\n#Plot the datasets\nfig, ax = plt.subplots()\nshow(flcf_ch, ax=ax, cmap='Greens', clim=(0,5), title = 'Canopy Height (m)')\n#Plot the GPR points on top\nflcf_gpr_df.plot(ax=ax, color='blue', markersize = 10) \"\"\"\n\n\"\"\" # 1.4 Load Lidar Snow depths --> This will take a few minutes\n\n#Read in the canopy heights raster from Farmer's Loop/Creamer's Field Alaska\nflcf_ds = RasterMeasurements.from_area(shp = gpr_limits, crs=26906,\n    buffer=None, type='depth',\n    site_name='farmers-creamers',\n    observers='chris larsen')\n\n#Plot the datasets\nfig, ax = plt.subplots()\nshow(flcf_ds, ax=ax, cmap='Blues', clim=(0,1.5), title='Snow Depth (m)')\n#Plot the GPR points on top\nflcf_gpr_df.plot(ax=ax, color='red', markersize = 10) \"\"\"\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#discussion-questions","position":39},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Part 2: Match the GPR data to the lidar grid and derive relative permittivity and density","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#part-2-match-the-gpr-data-to-the-lidar-grid-and-derive-relative-permittivity-and-density","position":40},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Part 2: Match the GPR data to the lidar grid and derive relative permittivity and density","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"There are two conceptual paths forward:\n\nRasterize the GPR data or\n\nVectorize the lidar data\n\nFor simplicity, the following code:\n\nvectorizes the lidar data\n\nperforms a nearest neighbor search between the lidar and GPR coordinate vectors\n\nCalculates the median GPR twt from the nearest neighbors\n\nDerives relative permittivity and density from the lidar snow depths and median twt\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#part-2-match-the-gpr-data-to-the-lidar-grid-and-derive-relative-permittivity-and-density","position":41},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"We need to know the resolutions of the lidar and GPR datasets","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#we-need-to-know-the-resolutions-of-the-lidar-and-gpr-datasets","position":42},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"We need to know the resolutions of the lidar and GPR datasets","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"The GPR dataset consists of points that are spaced ~0.10 m apart.\n\nWhat about the lidar? Run the code block below to answer this question.\n\nHow many GPR points would you expect to have per lidar pixel? Assume linear transects through each pixel.\n\n\"\"\" #2.1 Let's learn a bit about the resolution of the lidar rasters\n\nheight, width = flcf_ds.read(1).shape #Find the height and width of the array\n\n#Use meshgrid to create two arrays matching the height/width of the input raster\n#The GPR dataset consists of vectors --> we will eventually need to vectorize these lidar arrays\ncols, rows = np.meshgrid(np.arange(width), np.arange(height)) \n\n\n#Extract the easting/northing from the raster \nx_lidar, y_lidar = rio.transform.xy(flcf_ds.transform, rows, cols) \n\n#What's the resolution of the lidar dataset?\nprint(\"The x resolution of the snow depth raster is:\",x_lidar[0][1]-x_lidar[0][0])\nprint(\"The y resolution of the snow depth raster is:\",y_lidar[0][0]-y_lidar[1][0])\n \"\"\"\n\n\"\"\" # 2.2 Matching GPR to the lidar grid\n\n#Two conceptual paths forward: rasterize the GPR data, or convert lidar data to points\n\n#Let's vectorize the raster data\nx_lidar_vec = np.array(x_lidar).flatten()\ny_lidar_vec = np.array(y_lidar).flatten()\nflcf_ds_vec = flcf_ds.read().flatten()\n\n#Pull vectors from geo dataframe\ngpr_arr = np.stack([flcf_gpr_df.geometry.x, flcf_gpr_df.geometry.y,flcf_gpr_df['value']], axis=1)\ngpr_x=gpr_arr[:,0]\ngpr_y=gpr_arr[:,1]\ngpr_twt=gpr_arr[:,2].reshape(len(gpr_arr[:,2]),1)\n \"\"\"\n\n\"\"\" #2.3 Create sets of coordinates for the nearest neighbors search\ncoordinates_set1 = np.column_stack((x_lidar_vec,y_lidar_vec))\ncoordinates_set2 = np.column_stack((gpr_x,gpr_y))\n\n# Build KDTree from the second set of coordinates\ntree = cKDTree(coordinates_set2)\n\n# Define the radius (in meters)\nradius = 0.25\n\n# Function to find the median of travel times within a radius --> Credit where credit is due, this function was generated in part by chatgpt\ndef find_median_travel_time_within_radius(point, tree, coordinates_set1, gpr_twt, radius):\n    indices = tree.query_ball_point(point, radius)\n    if indices:\n        # Retrieve travel times for the nearest neighbors\n        neighbor_twt = gpr_twt[indices]\n        median_twt = np.median(neighbor_twt)\n        return median_twt\n    else:\n        return np.nan  # Return NaN if no neighbors are within the radius\n# Find medians for each lidar point\nmedians = np.array([find_median_travel_time_within_radius(point, tree, coordinates_set2, gpr_twt, radius) for point in coordinates_set1])\n \"\"\"\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#we-need-to-know-the-resolutions-of-the-lidar-and-gpr-datasets","position":43},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"The GPR data is not as spatially continuous as the lidar data, so most of the median twt dataset consists of nan’s","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#the-gpr-data-is-not-as-spatially-continuous-as-the-lidar-data-so-most-of-the-median-twt-dataset-consists-of-nans","position":44},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"The GPR data is not as spatially continuous as the lidar data, so most of the median twt dataset consists of nan’s","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"Let’s remove the nan’s to free up memory and reduce processing time.\n\n\"\"\" #At this point, all lidar points should have an associated gpr twt --> most are likely nan's though. But let's check!\nprint(\"The gpr array has size:\",medians.shape)\nprint(\"The lidar array has size:\",flcf_ds_vec.shape)\n \"\"\"\n\n\"\"\" #2.4 Before we get to the math part, let's clear out the nan's from all important vectors:\n#Create mask for gpr medians that are nan's\nmask = np.isnan(medians)\n\n#Remove entries from the lidar snow depth, x, and y vectors that align with the nan twt values\nflcf_ds_vec_clean = flcf_ds_vec[~mask]\ncoordinates_set1_clean=coordinates_set1[~mask]\n\n#Lastly, remove entries from the twt medians\nmedians_clean = medians[~mask]\n\n#Let's check the new size of the twt array\nprint(medians_clean.shape)\n\n \"\"\"\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#the-gpr-data-is-not-as-spatially-continuous-as-the-lidar-data-so-most-of-the-median-twt-dataset-consists-of-nans","position":45},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Discussion questions:","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#discussion-questions-1","position":46},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Discussion questions:","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"Roughly, how many points were removed?\n\nWhen we are done, we will have derived 3788 snow density estimates. In the same area, about four snow pits were dug, resulting in four bulk density measurements. How useful do you think our data will be?\n\nIs more always better?","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#discussion-questions-1","position":47},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Let’s now transition to the relative permittivity and density calculations","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#lets-now-transition-to-the-relative-permittivity-and-density-calculations","position":48},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Let’s now transition to the relative permittivity and density calculations","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"\n\n\"\"\" #2.5 We finally get to the math part!!\n#Let's calculate relative permittivity first...\nc=0.2998#The speed of light in a vacuum\ne_s = ((c * medians_clean) / (2 * flcf_ds_vec_clean)) ** 2\n\n#And then calculate density\nrho_s = ((np.sqrt(e_s) - 1) / 0.845) * 1000 \"\"\"\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#lets-now-transition-to-the-relative-permittivity-and-density-calculations","position":49},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Part 3: Examining the derived densities","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#part-3-examining-the-derived-densities","position":50},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Part 3: Examining the derived densities","lvl2":"4. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska"},"content":"\n\n\"\"\" # 3.1 Finally, let's take a peek at what the derived densities look like...\nplt.figure()\nplt.scatter(coordinates_set1_clean[:,0], coordinates_set1_clean[:,1], s=10, c=rho_s, cmap='viridis', clim=(0, 500), edgecolor=None)\n\n# Add colorbar to show the scale of color values\nplt.colorbar()\nplt.title('Snow Density (kg m-3)')\n\n# Show the plot\nplt.show() \"\"\"\n\n\"\"\" # 3.2 What does the histogram distribution look like??\n# Define bin edges\nbin_edges = np.arange(np.min(rho_s), np.max(rho_s), 25)  # Create bin edges from min(x) to max(x) with step size 25\n\n# Create the histogram\nplt.figure()  # Create a new figure\nplt.hist(rho_s, bins=bin_edges, edgecolor=None)  # Plot histogram with specified bin edges\n\nplt.title('Snow Density Histogram')\n\n# Show the plot\nplt.show() \"\"\"\n\n\"\"\" #Let's zoom in a little...\n# Define bin edges\nbin_edges = np.arange(0, 500, 25)  # Create bin edges from min(x) to max(x) with step size 25\n\n# Create the histogram\nplt.figure()  # Create a new figure\nplt.hist(rho_s, bins=bin_edges, edgecolor='black')  # Plot histogram with specified bin edges\n\nplt.title('Snow Density Histogram')\n\n# Show the plot\nplt.show() \"\"\"\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#part-3-examining-the-derived-densities","position":51},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"5. Discussion: Improving Density Estimation"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-5-discussion-improving-density-estimation","position":52},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"5. Discussion: Improving Density Estimation"},"content":"What do you think? Do the derived densities look usable at this stage?","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-5-discussion-improving-density-estimation","position":53},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"What contributes to the random error?","lvl2":"5. Discussion: Improving Density Estimation"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#what-contributes-to-the-random-error","position":54},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"What contributes to the random error?","lvl2":"5. Discussion: Improving Density Estimation"},"content":"There are three groups of factors that control the random error:\n\nMeasurement accuracy for lidar snow depths and GPR twt. Reduced accuracy for either or both of the techniques will lead to large errors. The boreal forest had a lot of complex vegetation that may have impeded the accuracy of these instruments.\n\nDepth of the snowpack. The accuracy of the lidar is not a function of snow depth. Thus, the random errors reduce as the snow depth increases. The boreal forest snow depths were shallow!\n\nGeolocation alignment. GPR coordinates were post-processed, but accuracy is still likely on the order of ±3 m.\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#what-contributes-to-the-random-error","position":55},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Improving the derived densities","lvl2":"5. Discussion: Improving Density Estimation"},"type":"lvl3","url":"/notebooks/gpr-lidar-hackweektutorial#improving-the-derived-densities","position":56},{"hierarchy":{"lvl1":"GPR and Lidar","lvl3":"Improving the derived densities","lvl2":"5. Discussion: Improving Density Estimation"},"content":"Let’s say we want to learn something about snow density in the boreal forest. The derived densities offer a HUGE increase in the number of available density measurements compared to in situ. But, in situ are much more accurate. How can we improve this dataset?\n\nIncrease the footprint of the derived densities by upsampling the lidar (e.g., to 3 m).\n\nThis will reduce the impact of GPR geolocation accuracy and the lidar/GPR observation uncertainty.\n\nNeed to be careful! The GPR footprint is large, but it may not scale well past 3 m.\n\nRemove erroneous values.\n\nHow does the lidar survey time compare with the GPR survey time? Was the snow disturbed or did more snow accumulate between surveys?\n\nRelative permittivity of snow cannot be less than air (\\epsilon_a = 1.0) or greater than liquid water (\\epsilon_w = 88).\n\nFor dry snow, relative permittivity is usually between 1.0 and 2.0. The removal of values outside a certain number of standard deviations and/or the interquartile range may be warranted.\n\nRun a spatial averaging filter.\n\nOur surveys were primarily spirals --> should pair nicely with such a filter!\n\nExperiment with the window size of the filter. How would a 5 m x 5 m filter compare to a 25 m x 25 m filter?\n\nShould the data be parsed into different forest cover classes before such a filter is run?\n\nBe careful of linear transects! Large windows tend to remove any density variability along such transects.\n\nOnce you reach this point, it is likely that the densities will be analysis ready. You could run a predictive model to fill in the void spaces, use the densities to evaluate models, calculate experimental variograms, etc.\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#improving-the-derived-densities","position":57},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"6. SnowEx GPR Analysis-Ready Datasets"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-6-snowex-gpr-analysis-ready-datasets","position":58},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"6. SnowEx GPR Analysis-Ready Datasets"},"content":"Grand Mesa, Colorado (SnowEx 2017, 2020)\n\nWebb et al. (2019). \n\nWebb et al. (2019)\n\nBonnell et al. (2021). \n\nBonnell et al. (2021)\n\nMeehan (2021). \n\nMeehan (2021)\n\nWebb (2021). \n\nWebb (2021)\n\nMeehan & Hojatimalekshah (2024). \n\nMeehan & Hojatimalekshah (2024)\n\nCameron Pass, Colorado (SnowEx 2020, 2021)\n\nMcGrath et al. (2021). \n\nMcGrath et al. (2021)\n\nBonnell et al. (2022). \n\nBonnell et al. (2022)\n\nBonnell et al. (2024). \n\nBonnell et al. (2024)\n\nJemez Mountains, New Mexico (SnowEx 2020)\n\nWebb (2021). \n\nWebb (2021)\n\nArctic Coastal Plains, Alaska (SnowEx 2023)\n\nWebb (2024). \n\nWebb (2024)\n\n","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-6-snowex-gpr-analysis-ready-datasets","position":59},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"7. References"},"type":"lvl2","url":"/notebooks/gpr-lidar-hackweektutorial#id-7-references","position":60},{"hierarchy":{"lvl1":"GPR and Lidar","lvl2":"7. References"},"content":"Lidar Datasets\n\nLarsen (2024). \n\nLarsen (2024)\n\nRelevant GPR LWC Studies\n\nWebb et al. (2018). \n\nWebb et al. (2018)\n\nWebb et al. (2020). \n\nWebb et al. (2020)\n\nBonnell et al. (2021). \n\nBonnell et al. (2021)\n\nWebb et al. (2022). \n\nWebb et al. (2022)\n\nRelevant GPR Density Studies\n\nYildiz et al. (2021). \n\nYildiz et al. (2021)\n\nMcGrath et al. (2022). \n\nMcGrath et al. (2022)\n\nBonnell et al. (2023). \n\nBonnell et al. (2023)\n\nMeehan et al. (2024). \n\nMeehan et al. (2024)","type":"content","url":"/notebooks/gpr-lidar-hackweektutorial#id-7-references","position":61},{"hierarchy":{"lvl1":"Reanalysis Data Access"},"type":"lvl1","url":"/notebooks/ucla-data-access","position":0},{"hierarchy":{"lvl1":"Reanalysis Data Access"},"content":"This workbook is to access the Western United States snow reanalysis data set, as developed by UCLA. Normally, the data is provided as PNG or netCDF files, and it is not cloud-optimized. So, we will need to download the data into a tmp/ folder, as a direct access will be computationally inefficient.\n\nimport xarray as xr\nimport earthaccess\nimport boto3\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport matplotlib.pyplot as plt\nimport warnings\nfrom IPython.display import display, Markdown\n\nAs implied by the dataset’s name, the UCLA reanalysis is only for the Western United States. For this example, we will look over the Tuolumne River Basin in California.\n\nAs with the MERRA-2 example workbook, we will be using the dataset DOI to quickly access the data, and looking at the 2020-2021 water year.\n\n\"\"\" # Define bounding box over the Tuolumne\nbbox = (-119.85, 37.71, -119.1, 38.25)\n\n# Authenticate using Earthdata Login prerequisite files\nauth = earthaccess.login()\n\n# Search for the granule by DOI\nresults = earthaccess.search_data(\n    doi='10.5067/PP7T2GBI52I2',\n    temporal=(\"2020-10-01\", \"2021-09-30\"),\n    bounding_box = bbox\n) \"\"\"\n\n\"\"\" # Download the files to a tmp folder, and save paths as a list\nfiles = earthaccess.download(results, \"tmp/\")\nfiles \"\"\"\n\nThe four files we downloaded provide reanalysis data for snow water equivalent and snow cover (SWE_SCA_POST), as well as snow depth (SD_POST). We have two files for each to correspond to different latitudes (N37 and N38 in the file names).\n\nWe could load these files individually, but Xarray has functionality to load all of them at once with xarray.open_mfdataset()!\n\n# ds = xr.open_mfdataset(files)\n\n# ds\n\nOur DataArray has four dimensions: Day, Stats, Longitude, and Latitude. Days refers to the number of days after the start of the water year (October 1st), which isn’t very useful on its own. So, let’s change it into a datetime format.\n\n# import re\n# import pandas as pd\n\n# # Find year in file name\n# url = files[0]\n# date_pattern = r'\\d{4}'\n\n# # Convert year to start of water year (pd.datetime format)\n# WY_start_date = pd.to_datetime(f'{re.search(date_pattern, url).group()}-10-01')\n\n# # Define new coordinates that use dates rather than day numbers\n# ds.coords['time'] = (\"Day\", pd.date_range(WY_start_date, periods=ds.sizes['Day']))\n# ds = ds.swap_dims({'Day':'time'})\n\nThe Stats coordinate refers to the statistics that are available for each of the variables, but its inputs are numeric, rather than strings. The stats_dictionary below outlines the statistics associated with each number, with 25pct and 75pct referring to the 25th-percentile and the 75th-percentile, respectively.\n\n# # Make dictionary of statistics\n# stats_dictionary = {'mean':0, \n#                     'std':1, \n#                     'median':2, \n#                     '25pct':3, \n#                     '75pct':4}\n\n# # Choose statistic of interest\n# stat = stats_dictionary['mean']\n\nFor this example, we are grabbing the mean daily SWE (SWE_Post):\n\n# mean_daily_swe = ds['SWE_Post'].sel(Stats=stat)\n\n# mean_daily_swe\n\nLooking at the output, we can see that the data now has “Array” and “Chunk” information. This is because the data was lazy-loaded through the dask, given that there is a lot of data stored in each file.\n\nBefore we plot the data, we will need to reduce it to our time frequency of interest (monthly, in this case). We will then properly load the data into memory.\n\n# # Resample the SWE data to a monthly mean\n# mean_monthly_swe = mean_daily_swe.resample(time=\"1ME\").mean()\n\n# # Load the monthly mean data into memory\n# mean_monthly_swe = mean_monthly_swe.compute()\n\nCaution: The above plotting cell can be a bit time-consuming if you are working with a lot of files at once.\n\nFinally, we will make a figure showing the monthly SWE across an entire water year.\n\n# # Define months as strings, for subplot titles\n# months = ['October', 'November', 'December', 'January',\n#           'February', 'March', 'April', 'May',\n#           'June', 'July', 'August', 'September']\n\n# # Plot the SWE data as monthly means\n# fig = mean_monthly_swe.plot.imshow(\n#     col='time',\n#     col_wrap=4,\n#     cmap=\"Blues\",\n#     vmin=0,\n#     vmax=1,\n# )\n\n# # Set titles to month\n# for ax, title in zip(fig.axs.flatten(), months):\n#     ax.set_title(title, fontsize=12)\n\n# # Change colorbar label and label sizes``\n# fig.cbar.ax.tick_params(labelsize=16)\n# fig.cbar.set_label(label='SWE [m]', size=16, weight='bold')","type":"content","url":"/notebooks/ucla-data-access","position":1},{"hierarchy":{"lvl1":"AVIRIS-NG"},"type":"lvl1","url":"/notebooks/aviris-ng-data","position":0},{"hierarchy":{"lvl1":"AVIRIS-NG"},"content":"Lessons learned working with the NSIDC dataset.Dataset: SnowEx 2021; Senator Beck Basin and Grand MesaTutorial Author: \n\nBrent Wilder\n\nLearning Objectives\n\nUnderstand how this data is structured\n\nUnderstand where to find necessary terrain and illumination data\n\nLearn about the spectral python package and apply it to this dataset\n\n","type":"content","url":"/notebooks/aviris-ng-data","position":1},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl3":"Computing environment"},"type":"lvl3","url":"/notebooks/aviris-ng-data#computing-environment","position":2},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl3":"Computing environment"},"content":"We’ll be using the following open source Python libraries in this notebook:\n\n# from spectral import *\n# import numpy as np\n# import matplotlib.pyplot as plt\n\n","type":"content","url":"/notebooks/aviris-ng-data#computing-environment","position":3},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl3":"SnowEx21 Spectral Reflectance Dataset"},"type":"lvl3","url":"/notebooks/aviris-ng-data#snowex21-spectral-reflectance-dataset","position":4},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl3":"SnowEx21 Spectral Reflectance Dataset"},"content":"The data were collected using an airborne imaging spectrometer, AVIRIS-NG can be downloaded from here, \n\nhttps://​nsidc​.org​/data​/snex21​_ssr​/versions/1.\n\nReflectance is provided at 5 nm spectral resolution with a range of 380-2500 nm\n\nFor this dataset, the pixel resolution is 4 m\n\nData span from 19 March 2021 to 29 April 2021, and were collected in two snow-covered environments in Colorado: Senator Beck Basin and Grand Mesa\n\nEach file will have a “.img” and “.hdr”. You need to have both of these in the same directory to open data.\n\n\n\n","type":"content","url":"/notebooks/aviris-ng-data#snowex21-spectral-reflectance-dataset","position":5},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl3":"Downloading necessary terrain and illumination data"},"type":"lvl3","url":"/notebooks/aviris-ng-data#downloading-necessary-terrain-and-illumination-data","position":6},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl3":"Downloading necessary terrain and illumination data"},"content":"The NSIDC repository does not contain the terrain/illumination information.\n\nHowever, you can obtain it for the matching flightline (by its timestamp) at the following URL, \n\nhttps://​search​.earthdata​.nasa​.gov/ ,\n\nand searching for “AVIRIS-NG L1B Calibrated Radiance, Facility Instrument Collection, V1”\n\nYou only need to download the “obs_ort” files for the flight of interest. Please note these are different than “obs” files (ort means orthorectified).\n\nIn the Granule ID search, you can use wildcars “*” on either end of “obs_ort” to reduce your search.\n\nYou may also want to use this bounding box to reduce your search:\n\nSW: 37.55725,-108.58887\n\nNE: 39.78206,-106.16309\n\n","type":"content","url":"/notebooks/aviris-ng-data#downloading-necessary-terrain-and-illumination-data","position":7},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl3":"Using python package, spectral, to open data"},"type":"lvl3","url":"/notebooks/aviris-ng-data#using-python-package-spectral-to-open-data","position":8},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl3":"Using python package, spectral, to open data"},"content":"Important\n\nUpdate the paths below to your local environment\n\n","type":"content","url":"/notebooks/aviris-ng-data#using-python-package-spectral-to-open-data","position":9},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"Cells below commented out until further development occurs"},"type":"lvl2","url":"/notebooks/aviris-ng-data#cells-below-commented-out-until-further-development-occurs","position":10},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl2":"Cells below commented out until further development occurs"},"content":"\n\n# INSERT YOUR PATHS HERE\n#path_to_aviris = '/data/Albedo/AVIRIS/ang20210429t191025_rfl_v2z1'\n#path_to_aviris_hdr = '/data/Albedo/AVIRIS/ang20210429t191025_rfl_v2z1.hdr'\n#path_to_terrain = '/data/Albedo/AVIRIS/ang20210429t191025_rfl_v2z1_obs_ort'\n#path_to_terrain_hdr = '/data/Albedo/AVIRIS/ang20210429t191025_rfl_v2z1_obs_ort.hdr'\n\n# Open a test image\n#aviris = envi.open(path_to_aviris_hdr)\n\n# Save to an array in memory\n# rfl_array = aviris.open_memmap(writeable=True)\n\n# print shape. You can see here we have 425 spectral bands for a grid of 1848x699 pixels\n# rfl_array.shape\n\n\n# You can create an array of the bands centers like this\n# bands = np.array(aviris.bands.centers)\n# bands\n\n# A simple data visalization by selecting random indices\n\"\"\" i = 900\nj = 300\npixel = rfl_array[i,j,:]\n\nfig, ax = plt.subplots(1, 1, figsize=(10,5))\nplt.rcParams.update({'font.size': 18})\nax.scatter(bands, pixel, color='blue', s=20)\nax.set_xlabel('Wavelength [nm]')\nax.set_ylabel('Reflectance')\nplt.show() \"\"\"\n\n","type":"content","url":"/notebooks/aviris-ng-data#cells-below-commented-out-until-further-development-occurs","position":11},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl3":"Lastly, a very important note!","lvl2":"Cells below commented out until further development occurs"},"type":"lvl3","url":"/notebooks/aviris-ng-data#lastly-a-very-important-note","position":12},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl3":"Lastly, a very important note!","lvl2":"Cells below commented out until further development occurs"},"content":"Please notice that convention for aspect follows -\\pi to \\pi.\n\n# Terrain bands:\n# 0 - Path length (m)\n# 1 - To sensor azimuth\n# 2 - To sensor zenith\n# 3 - To sun azimuth\n# 4 - To sun zenith\n# 5 - Solar phase\n# 6 - Slope\n# 7 - Aspect\n# 8 - cosine(i) (local solar illumination angle)\n# 9 - UTC Time\n# 10 - Earth-sun distance (AU)\n\n\"\"\" # open envi object\nterrain = envi.open(path_to_terrain_hdr)\n\n# Save to an array in memory\nterrain_array = terrain.open_memmap(writeable=True)\n\n# Grab just aspect and flatten (remove nan)\naspects = terrain_array[:,:,7].flatten()\naspects = aspects[aspects>-9999]\n\n\n# Plot a histogram to show aspect range\nfig, ax = plt.subplots(1, 1, figsize=(10,5))\nplt.rcParams.update({'font.size': 18})\nax.hist(aspects, color='black', bins=50)\nax.set_xlabel('Aspect [degrees]')\nax.set_ylabel('Count')\nplt.show() \"\"\"\n\n\n\n\n","type":"content","url":"/notebooks/aviris-ng-data#lastly-a-very-important-note","position":13},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl3":"References","lvl2":"Cells below commented out until further development occurs"},"type":"lvl3","url":"/notebooks/aviris-ng-data#references","position":14},{"hierarchy":{"lvl1":"AVIRIS-NG","lvl3":"References","lvl2":"Cells below commented out until further development occurs"},"content":"To further explore these topics:\n\nhttps://​snowex​-2022​.hackweek​.io​/tutorials​/aviris​-ng​/AVIRIS​-NG​_Tutorial​.html\n\nhttps://​www​.spectralpython​.net​/​#documentation","type":"content","url":"/notebooks/aviris-ng-data#references","position":15},{"hierarchy":{"lvl1":"ERA5"},"type":"lvl1","url":"/notebooks/era5-data-access","position":0},{"hierarchy":{"lvl1":"ERA5"},"content":"This is a script designed to obtain snow data from the ERA5 reanalysis product. We will be using the Copernicus API to get global, daily snow cover and snow depth information.\n\nThis code is adapted from Tasha Snow’s ERA5 downloading script: \n\nERADownload.ipynb\n\nThe Copernicus Climate Data Store (CDS) API is not on CryoCloud by default, so the following cell needs to be run, followed by restarting the kernel.\n\n\n\nTo use the CDS API, the user needs credentials to the Copernicus Climate Data Store (CDS). Upon getting a user ID (uid) and an API key (api-key), they need to run the following cell (skip if you already have ./cdsapirc in the /home/jovyan/ directory).\n\n# !echo url: https://cds.climate.copernicus.eu/api/v2 >> /home/jovyan/.cdsapirc\n# !echo key: {uid}:{api-key} >> /home/jovyan/.cdsapirc\n\nfrom ecmwfapi import ECMWFDataServer # Need a ecmwf user name and password first\nimport cdsapi \n\nThe CDS API can be a bit picky with inputs from ERA5, so first-time users are encouraged to use the online request form (\n\nhttps://​cds​.climate​.copernicus​.eu​/datasets​/reanalysis​-era5​-single​-levels​?tab​=​download) to automatically generate a code for their API request, to ensure that the syntax is correct.\n\nThe below functions retrieve ERA5 snow depth and snow density and download them to a tmp/ folder. Additional parameters to consider:\n\nyearStart and yearEnd: Start and end year.\n\nmonthStart and monthEnd: Start and end month.\n\ndayStart and dayEnd: Start and end day.\n\nThe function currently grabs daily data from March 1, 2020 - April 30, 2020 at 12:00 UTC each day, and downloads as daily netCDF files. Because ERA5 is generated hourly, users can expand the time entry to include more hours per day.\n\n# from pathlib import Path\n\n# # Initialize the CDS API\n# c = cdsapi.Client()\n\n# def retrieve_era5():\n#     \"\"\"      \n#        A function to demonstrate how to iterate efficiently over several years and months etc    \n#        for a particular ERA5 request.\n#     \"\"\"\n#     yearStart = 2020\n#     yearEnd = 2020\n#     monthStart = 3\n#     monthEnd = 3\n#     dayStart = 1\n#     dayEnd = 31\n#     for year in list(range(yearStart, yearEnd + 1)):\n#         for month in list(range(monthStart, monthEnd + 1)):\n#             for day in list(range(dayStart, dayEnd + 1)):\n#                 startDy = '%02d' % (day)\n#                 startMo = '%02d' % (month)\n#                 startYr = '%04d' % (year)\n#                 tmp_dir = Path.cwd() / \"tmp\"\n#                 tmp_dir.mkdir(exist_ok=True)\n#                 target = f\"{tmp_dir}/era5_SWE_{startYr}{startMo}{startDy}.nc\"\n#                 era5_request(startYr, startMo, startDy, target)\n\n# def era5_request(startYr, startMo, startDy, target):\n#     \"\"\"      \n#         Helper function for era5_retrieve. An ERA-5 request for snow\n#         depth and snow cover data for the given years/months/days.\n\n#         Inputs\n#         ------------\n#         startYr: str\n#             Starting year of data query, in YYYY format.\n#         startMo: str\n#             Starting month of data query, in MM format.\n#         startDy: str\n#             Starting day of data query, in DD format.\n#         target: str\n#             Path and name of netCDF file to be saved.\n#     \"\"\"\n#     c.retrieve(\n#     'reanalysis-era5-land',\n#     {\n#         'product_type':['reanalysis'],\n#         'data_format':'netcdf',\n#         'variable':['snow_depth', 'snow_cover'],\n#         'year':[startYr],\n#         'month':[startMo],\n#         'day':[startDy],\n#         'time':['12:00']\n#     },\n#     target)\n        \n# if __name__ == '__main__':\n#     retrieve_era5()\n\nDepending on the number of files downloaded (31 in the case of the above example), it can take a while to download everything.\n\nWhen it finishes, there should now be daily ERA5 data in netCDF format! To efficiently load all of this data, we are going to use Xarray and its open_mfdataset() function.\n\nimport os\nimport re\nimport zipfile\nimport xarray as xr\n\nfrom os import listdir\nfrom os.path import join\n\n# def process_era5_data(tmp_path):\n#     # Find ERA5 Zip files in downloaded directory\n#     era5_files = [join(tmp_path,f) for f in listdir(tmp_path) if \"era5_\" in join(tmp_path, f)]\n    \n#     # Iteratively unzip each file and collect into a list\n#     tmp_files = era5_extract(era5_files, tmp_dir)\n\n#     print('------------')\n#     # Open all ERA5 files into single Xarray\n#     ds = xr.open_mfdataset(tmp_files)\n#     print(\"All data has been lazy-loaded into Xarray.\")\n\n#     # Remove extracted files, for cleanliness\n#     for file in tmp_files:\n#         os.remove(file)\n#     print(\"Extracted ERA-5 files deleted.\")\n\n#     return ds\n\n# def era5_extract(era5_files, tmp_dir):\n#     for file in era5_files:\n#         with zipfile.ZipFile(file, 'r') as zfile:\n#             print(f'Now extracting data for file: {file}')\n#             # Extract all files from current Zip file\n#             zfile.extractall(tmp_dir)\n\n#             # Rename output file to prevent overwriting\n#             outfile = join(tmp_dir, \"data_0.nc\")\n#             date_pattern = re.search(r'\\d{8}', file).group(0)\n#             newfile = join(tmp_dir, f'data_{date_pattern}.nc')\n#             os.rename(outfile, newfile)\n#             print(f'Data extracted and saved to file: data_{date_pattern}.nc')\n#             print(' ')\n\n#     # List of output files\n#     tmp_files = [join(tmp_dir,f) for f in os.listdir(tmp_dir) if \"data_\" in join(tmp_dir, f)]\n\n#     return tmp_files\n\n# tmp_dir = Path.cwd() / \"tmp\"\n# ds = process_era5_data(tmp_dir)\n\n# ds\n\nThanks to the above function, loading all of that data is pretty easy! However, it is important to note that the data is currently “lazy-loaded” - we can easily subset and resample the data for our needs, but we will need to load it into memory if we wish to make figures.\n\nFully loading the data as is can be time-consuming, so let’s reduce the data first, starting with making monthly means of snow depth.\n\n# # Calculate monthly mean snow depth and snow cover\n# era5_monthly = ds.resample(valid_time='1ME').mean()\n\nResampling to monthly means reduces the data volume by quite a bit, so let’s now look at global snow depth from the month of March. We will go ahead and load the result into memory using the compute() function.\n\n# # Load March snow depths into memory\n# era5_sd_march = era5_monthly['snowc'].compute().squeeze()\n\nFinally, we can make a map figure showing global, monthly-averaged snow depth from ERA5.\n\n# import matplotlib.pyplot as plt\n\n# fig, ax = plt.subplots()\n# era5_sd_march.plot.imshow(ax=ax, cmap='Blues')\n# ax.set_xlabel(\"Longitude\", fontsize=12)\n# ax.set_ylabel(\"Latitude\", fontsize=12)\n# ax.set_title(\"ERA5 Snow Cover, March 2020\", fontsize=12)\n# fig.tight_layout()\n\nNow for a different example. Here, we will examine snow depths over Alaska only, and generate a state-wide time series for the month of March.\n\n# # Making bounds for Alaska\n# mask_lon = (ds.longitude >= -168.75+360) & (ds.longitude <= -136.01+360)\n# mask_lat = (ds.latitude >= 52.64) & (ds.latitude <= 71.59)\n\n# # Subset ERA5 data to Alaska lats/lons only\n# era5_alaska = ds.where(mask_lon & mask_lat, drop=True)\n\nAs before, we need to load the Alaska data into memory. Because we are looking over a much smaller spatial domain, compute() will be much faster.\n\n# # Load Alaska data into memory\n# era5_alaska = era5_alaska.compute().squeeze()\n\nAgain, we can make a map figure showing snow depth over the state of Alaska, this time for March 1, 2020:\n\n# # Map plot of Alaska snow depths\n# era5_alaska['snowc'].isel(valid_time=0).plot.imshow(vmin=0, vmax=1, cmap=\"Blues\")\n\nWe can also create a spatially-averaged time series of snow depth over the state of Alaska for the entire time period March 1 - April 30:\n\n# # Calculate spatial average of snow depths over Alaska\n# era5_sd_alaska = era5_alaska['snowc'].mean(('longitude', 'latitude'))\n\n# # Time series plot of Alaska snow depths\n# fig, ax = plt.subplots()\n# era5_sd_alaska.plot(ax=ax)\n# ax.set_xlabel(\"Day\", fontsize=12)\n# ax.set_ylabel(\"Snow depth [m]\", fontsize=12)\n# ax.set_title(\"March 1 - April 30, 2020\", fontsize=12)\n# fig.tight_layout()","type":"content","url":"/notebooks/era5-data-access","position":1},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2"},"type":"lvl1","url":"/notebooks/is2-snow-depth-workflow","position":0},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2"},"content":"This notebook uses a combination of ICESat-2 and airborne lidar to derive snow depth. It uses data from the SnowEx 2023 campaign as an example, but can be applied to other locations if a shapefile or geoJSON is given.\n\nThis notebook is adapted from the 2023 ICESat-2 Hackweek, originally developed by Zachary Fair and Karina Zikan.\n\n","type":"content","url":"/notebooks/is2-snow-depth-workflow","position":1},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"User input"},"type":"lvl2","url":"/notebooks/is2-snow-depth-workflow#user-input","position":2},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"User input"},"content":"Acceptable field site IDs over Alaska are:\n\ncffl: Creamer’s Field/Farmer’s Loop\n\ncpcrw: Caribou/Poker Creek Experimental Watershed\n\nbcef: Bonanza Creek Experimental Forest\n\nacp: Arctic Coastal Plain\n\nutk: Toolik Research Station\n\nAcceptable IDs for Sliderule ATL08 class (use numeric ID):\n\nNo classification: -1\n\natl08_unclassified: 0\n\natl08_noise: 1\n\natl08_canopy: 2\n\natl08_top_of_canopy: 3\n\natl08_ground: 4\n\n# Field site ID\nfield_id = 'data/toolik_lidar_boxes.geojson'\n\n# Snow-on (True) or snow-off (False) analysis\nsnow_on = True\n\n# Use March UAF data ('mar') or October depths ('oct')\nuaf_depths = 'mar'\n\n# Base data path\npath = '/home/jovyan/icesat2-snowex'\n\n# Desired RGT and date range for data queries. Set rgt to \"all\" if\n# all ground tracks are desired\ndate_range = ['2023-03-01', '2023-04-01']\nrgt = '1356'\n\n# SlideRule parameters (optional)\ncnf_surface = 4\natl08_class = 4\nsegment_length = 40\nres = 20\n\nA breakdown of the SlideRule parameters above:\n\ncnf_surface: The confidence level of the ICESat-2 photons.\n\nHigh-confidence photons (recommended for snow): 4\n\nHigh-/medium-confidence photons: 3\n\nHigh-/medium-/low-confidence photons: 2\n\nSignal photons (high/medium/low) and noise: 1\n\nSignal photons, noise, and solar background (not recommended): 0\n\nsegment_length: The along-track length to sample and aggregate photons, in meters. Currently set at 40 m, the resolution of the ATL06 product.\n\nres: The along-track resolution of the returned data product. Currently set at 20 m to match ATL06.\n\n","type":"content","url":"/notebooks/is2-snow-depth-workflow#user-input","position":3},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"Read ICESat-2 data"},"type":"lvl2","url":"/notebooks/is2-snow-depth-workflow#read-icesat-2-data","position":4},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"Read ICESat-2 data"},"content":"To load the ICESat-2 data with minimal effort from the user, we will use SlideRule in the below function.\n\n\"\"\" from sliderule import sliderule, icesat2\n\ndef atl06srq(field_geojson, date_range, rgt, cnf_surface, atl08_class, \n             segment_length, res):\n    # Initiate SlideRule\n    icesat2.init('slideruleearth.io', verbose=False)\n\n    # Load geoJSON for site of interest\n    region = sliderule.toregion(field_geojson)['poly']\n\n    # Convert user-defined ATL08 class ID to string readable by SlideRule\n    atl08_ids = {-1: 'None',\n                 0: 'atl08_unclassified',\n                 1: 'atl08_noise',\n                 2: 'atl08_canopy',\n                 3: 'atl08_top_of_canopy',\n                 4: 'atl08_ground'}\n\n    # Construct dictionary of parameters\n    time_root = 'T00:00:00Z'\n    parms = {\n             \"poly\": region,\n             \"srt\": icesat2.SRT_LAND,\n             \"cnf\": cnf_surface,\n             \"len\": segment_length,\n             \"res\": res,\n             \"t0\": date_range[0]+time_root,\n             \"t1\": date_range[1]+time_root\n            }\n\n    # Check if all RGTs are considered, or only a subset\n    if rgt != \"all\":\n        parms[\"rgt\"] = rgt\n        print(f\"Subsetting to only include ICESat-2 RGT {rgt}.\")\n\n    # Check for ATL08 filter\n    if atl08_ids.get(atl08_class) != \"None\":\n        parms[\"atl08_class\"] = atl08_ids.get(atl08_class)\n        print(\"Subsetting by selected ATL08 filter...\")\n\n    # Query SlideRule\n    atl06sr = icesat2.atl06p(parms)\n\n    return atl06sr \"\"\"\n\n\"\"\" # Generate ICESat-2 data from SlideRule\natl06sr = atl06srq(field_id, date_range, rgt,\n                 cnf_surface=cnf_surface,\n                 atl08_class=atl08_class,\n                 segment_length=segment_length,\n                 res=res) \"\"\"\n\n\"\"\" # Convert ATL06SR to geodataframe in EPSG:32606\natl06sr['lon'], atl06sr['lat'] = atl06sr.geometry.x, atl06sr.geometry.y\natl06sr_gdf = atl06sr.to_crs('epsg:32606') \"\"\"\n\n# atl06sr\n\n","type":"content","url":"/notebooks/is2-snow-depth-workflow#read-icesat-2-data","position":5},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"Read Airborne Lidar Data"},"type":"lvl2","url":"/notebooks/is2-snow-depth-workflow#read-airborne-lidar-data","position":6},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"Read Airborne Lidar Data"},"content":"To derive snow depth with ICESat-2, we need a snow-off digital elevation model (DEM), which commonly originates from airborne lidar. This next step is designed to load and prepare some airborne lidar data from the University of Alaska, Fairbanks for this analysis.\n\nThe method currently presented uses earthaccess to stream the data without any download required. However, this process can be slow on a local machine, and may be memory-intensive on a cloud environment. A more streamlined workflow utilizing the SnowEx Database is in the works, though the given method is most effective for non-SnowEx DEM sources.\n\n\"\"\" import earthaccess\nimport xarray as xr\nearthaccess.login(strategy='interactive', persist=True)\nauth = earthaccess.login() \"\"\"\n\n\"\"\" region = sliderule.toregion(field_id)['poly']\ncoords = [(point[\"lon\"], point[\"lat\"]) for point in region] \"\"\"\n\n\"\"\" # Coordinates for SW/NE corners\nlon_min = min([coord[0] for coord in coords])\nlat_min = min([coord[1] for coord in coords])\nlon_max = max([coord[0] for coord in coords])\nlat_max = max([coord[1] for coord in coords])\n\n# Data query for lidar snow depth over Fairbanks, AK\nresults = earthaccess.search_data(\n    short_name='SNEX23_Lidar',\n    bounding_box = (lon_min, lat_min, lon_max, lat_max),\n    temporal = ('2023-03-10', '2023-03-15')\n)\n\nfiles = earthaccess.open(results)\nlidar_snow_on = xr.open_dataset(files[1], engine='rasterio') \"\"\"\n\n# lidar_snow_on\n\n\"\"\" # Data query for lidar snow-off elevation over Fairbanks, AK\nresults = earthaccess.search_data(\n    short_name='SNEX23_Lidar',\n    bounding_box = (lon_min, lat_min, lon_max, lat_max),\n    temporal = ('2022-05-20', '2022-05-31')\n)\n\n# Open the resulting GeoTiff into Xarray\nfiles = earthaccess.open(results)\nlidar_snow_off = xr.open_dataset(files[1], engine='rasterio') \"\"\"\n\n","type":"content","url":"/notebooks/is2-snow-depth-workflow#read-airborne-lidar-data","position":7},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"Co-Locate ICESat-2 and UAF Lidar"},"type":"lvl2","url":"/notebooks/is2-snow-depth-workflow#co-locate-icesat-2-and-uaf-lidar","position":8},{"hierarchy":{"lvl1":"Snow Depth Estimates with ICESat-2","lvl2":"Co-Locate ICESat-2 and UAF Lidar"},"content":"For this step, we will co-locate ICESat-2 and UAF so that we can directly compare the two datasets. The co-location will use a statistical method called “spline interpolation”, and we will perform this co-location with both the snow-on and snow-off data.\n\nThe below function has the code needed to perform the co-location.\n\n\"\"\" # Packages needed for the below functions\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import RectBivariateSpline\n\ndef colocate_is2(lidar_snow_off, lidar_snow_on, is2_data):\n    # Ensure lidar/ICESat-2 projections match\n    if is2_data.crs!=32606:\n        is2_data.to_crs(\"EPSG:32606\", inplace=True)\n    \n    # Define x/y coordinates from snow-off data\n    x0, y0 = np.array(lidar_snow_off.x), np.array(lidar_snow_off.y)\n\n    # Do the same, but for the snow depth data\n    xs, ys = np.array(lidar_snow_on.x), np.array(lidar_snow_on.y)\n\n    # Remove filler values that would mess up the interpolator\n    dem_heights = np.array(lidar_snow_off['band_data'].sel(band=1))[::-1,:]\n    dem_heights[np.isnan(dem_heights)] = -9999\n    dem_depths = np.array(lidar_snow_on['band_data'].sel(band=1))[::-1,:]\n    dem_depths[np.isnan(dem_depths)] = -9999\n\n    # Generate interpolator schemes\n    interp_height = RectBivariateSpline(np.array(y0)[::-1], \n                                        np.array(x0),\n                                        dem_heights)\n    interp_depth = RectBivariateSpline(np.array(ys)[::-1],\n                                       np.array(xs),\n                                       dem_depths)\n\n    # Use the spline interpolator to align the lidar with ICESat-2\n    is2_lidar_df = pd.DataFrame()\n    for beam in np.unique(is2_data['gt']):\n        # Subset ICESat-2 data by current beam\n        is2_tmp = is2_data.loc[is2_data['gt']==beam]\n\n        # ICESat-2 x/y coordinates\n        xn, yn = is2_tmp.geometry.x, is2_tmp.geometry.y\n\n        # Define indices within x/y bounds of DEM\n        i1 = (xn>np.min(x0)) & (xn<np.max(x0))\n        i1 &= (yn>np.min(y0)) & (yn<np.max(y0))\n\n        # Estimate lidar elevation and snow depth along ICESat-2 track\n        lidar_height = interp_height(yn[i1], xn[i1], grid=False)\n        lidar_depth = interp_depth(yn[i1], xn[i1], grid=False)\n\n        # Construct dataframe of ICESat-2 and lidar data\n        tmp = pd.DataFrame(data={'lat': is2_tmp['lat'][i1],\n                                 'lon': is2_tmp['lon'][i1],\n                                 'x': xn[i1],\n                                 'y': yn[i1],\n                                 'rgt': is2_tmp['rgt'][i1],\n                                 'beam': is2_tmp['gt'][i1],\n                                 'is2_height': is2_tmp['h_mean'][i1],\n                                 'n_fit_photons': is2_tmp['n_fit_photons'][i1],\n                                 'h_sigma': is2_tmp['h_sigma'][i1],\n                                 'dh_fit_dx': is2_tmp['dh_fit_dx'][i1],\n                                 'lidar_height': lidar_height,\n                                 'lidar_snow_depth': lidar_depth\n                                    }\n                              )\n        # Concatenate the co-located data into  final DataFrame\n        is2_lidar_df = pd.concat([is2_lidar_df, tmp])\n\n    return is2_lidar_df \"\"\"\n\n\"\"\" # Use the above function to co-locate the airborne lidar and ICESat-2\natl06sr_uaf = colocate_is2(lidar_snow_off, lidar_snow_on, atl06sr)\n\n# Estimate the ICESat-2 snow depth\natl06sr_uaf['is2_snow_depth'] = atl06sr_uaf['is2_height'] - atl06sr_uaf['lidar_height']\n\n# Convert final DataFrame in GeoDataFrame\natl06sr_uaf_gdf = gpd.GeoDataFrame(atl06sr_uaf,\n                                   geometry=gpd.points_from_xy(atl06sr_uaf.lon, atl06sr_uaf.lat),\n                                   crs='EPSG:4326')\n\natl06sr_uaf_gdf \"\"\"\n\nAn outline of the variables in our current GeoDataFrame:\n\nlat and lon: The latitude and longitude along the ICESat-2 track.\n\nx and y: The easting and northing along the ICESat-2 track, in projection EPSG:32606.\n\nrgt: The reference ground track number of the ICESat-2 track of interest.\n\nbeam: The ICESat-2 beam designation (gt1l, gt2r, etc.)\n\nis2_height: ICESat-2 height estimate at the given location.\n\nn_fit_photons: Number of ICESat-2 photons used to derive is2_height.\n\nh_sigma: Approximate uncertainty of is2_height.\n\ndh_fit_dx: A rough measure of surface slope along the ICESat-2 track.\n\nlidar_height: Lidar height estimate at the given location.\n\nlidar_snow_depth: Lidar snow depth estimate at the given location.\n\nis2_snow_depth: ICESat-2 snow depth estimate at the given location.\n\nThe key variables are is2_snow_depth and lidar_snow_depth for our comparisons. Several of the other variables, such as n_fit_photons and h_sigma, can be used to filter or process the data further, if desired.\n\nLet’s look at a simple comparison between the two depth products.\n\n\"\"\" # Remove sub-zero values\natl06sr_uaf_gdf = atl06sr_uaf_gdf[atl06sr_uaf_gdf['is2_snow_depth']>=0]\natl06sr_uaf_gdf = atl06sr_uaf_gdf[atl06sr_uaf_gdf['lidar_snow_depth']>=0] \"\"\"\n\n\"\"\" import matplotlib.pyplot as plt\n\nsingle_beam = atl06sr_uaf_gdf[atl06sr_uaf_gdf['beam']==60]\n\n# Line plot of along-track snow depths\nfig, ax = plt.subplots(figsize=(9,6))\nsingle_beam.plot(kind='line', ax=ax, x='lat', y='is2_snow_depth',\n                     linewidth=3, label='ICESat-2')\nsingle_beam.plot(kind='line', ax=ax, x='lat', y='lidar_snow_depth',\n                     linewidth=1.5, color='orange', label='UAF lidar')\nax.set_xlabel('Latitude', fontsize=14)\nax.set_ylabel('Snow depth [m]', fontsize=14)\nax.set_ylim([0, 2])\nax.legend() \"\"\"\n\nWe can also calculate the difference in snow depth between ICESat-2 and UAF, then make a spatial plot using geopandas.explore().\n\n\"\"\" # Calculate snow depth bias\natl06sr_uaf_gdf['snow_depth_residual'] = atl06sr_uaf_gdf['is2_snow_depth'] - atl06sr_uaf_gdf['lidar_snow_depth']\n\n# Create a spatial plot of the snow depth bias\natl06sr_uaf_gdf.explore(column='snow_depth_residual', \n                        tiles='Esri.WorldImagery',\n                        cmap='viridis',\n                        vmin=-1.5, vmax=1.5) \"\"\"\n\nIf the data looks good, then we can save the final GeoDataFrame as a geoJSON.\n\n\"\"\" # Save the GeoDataFrame\natl06sr_uaf_gdf.to_file(f'{path}/is2_uaf_snow-depths.geojson',\n                        driver='GeoJSON') \"\"\"","type":"content","url":"/notebooks/is2-snow-depth-workflow#co-locate-icesat-2-and-uaf-lidar","position":9},{"hierarchy":{"lvl1":"MERRA-2"},"type":"lvl1","url":"/notebooks/merra2-data-access","position":0},{"hierarchy":{"lvl1":"MERRA-2"},"content":"This code is designed to access reanalysis data from the Modern-Era Retrospective analysis for Research and Applications, Version 2 (MERRA-2). MERRA-2 is useful for its global data record of various land surface variables, including snow cover and snow depth.\n\nIn this example notebook, we are accessing the snow depth product, which is found in the “1-Hourly, Time-Averaged, Single-Level, Assimilation, Land Surface Diagnostics” product (M2T1NXLND), found here: \n\nhttps://​disc​.gsfc​.nasa​.gov​/datasets​/M2T1NXLND​_5​.12​.4​/summary.\n\nimport xarray as xr\nimport earthaccess\nimport boto3\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport matplotlib.pyplot as plt\nimport warnings\nfrom IPython.display import display, Markdown\n\nBecause MERRA-2 is a reanalysis product by NASA, we can get the data through earthaccess. In addition to using a short_name for the data product of interest, earthaccess also allows one to use the dataset’s DOI for queries. The below query uses the DOI (10.5067/RKPHT8KC1Y1T) for M2T1NXLND.\n\n# # Authenticate using Earthdata Login prerequisite files\n# auth = earthaccess.login()\n\n# # Search for the granule by DOI\n# results = earthaccess.search_data(\n#     doi='10.5067/RKPHT8KC1Y1T',\n#     temporal=(\"2022-03-01\", \"2022-03-02\"),\n# )\n\n# len(results)\n\n","type":"content","url":"/notebooks/merra2-data-access","position":1},{"hierarchy":{"lvl1":"MERRA-2","lvl2":"USING ONLY 2 days for now to get the notebook to run. Need to update tutorial accordingly"},"type":"lvl2","url":"/notebooks/merra2-data-access#using-only-2-days-for-now-to-get-the-notebook-to-run-need-to-update-tutorial-accordingly","position":2},{"hierarchy":{"lvl1":"MERRA-2","lvl2":"USING ONLY 2 days for now to get the notebook to run. Need to update tutorial accordingly"},"content":"The queried MERRA-2 data is organized such that 1 file = 1 day, so we should expect 31 files to be loaded below.\n\n# # Access the MERRA-2 file(s) from the cloud\n# fn = earthaccess.open(results)\n\n# # Open MERRA-2 data in Xarray (may be time-/memory-intensive if multiple files are queried)\n# ds = xr.open_mfdataset(fn)\n\nWith the above lines of code, we now have global land surface diagnostics for 744 time steps, or hourly over the month of March 2022.\n\nSince we are interested in snow depth, we will focus on the SNODP variable, which provides snow depth in meters.\n\nLet’s look at global snow depths from March 1, 2022:\n\n# # Sample global plot of snow depth at a single time\n# ds['SNODP'][0,:,:].plot(vmin=0, vmax=1)\n\nWe can also subset by latitude and longitude to look over a region of interest (Alaska, for this example).\n\n# # Making bounds for Alaska\n# mask_lon = (ds.lon >= -168.75) & (ds.lon <= -136.01)\n# mask_lat = (ds.lat >= 52.64) & (ds.lat <= 71.59)\n\n# # Subset MERRA-2 data to Alaska lats/lons only\n# ds_ak = ds.where(mask_lon & mask_lat, drop=True)\n\n# # Sample Alaska plot of snow depth at a single time\n# ds_ak['SNODP'][0,:,:].plot(vmin=0, vmax=1.25)\n\nFinally, let’s generate a time series of snow depth for the month of March 2022 near Fairbanks, AK.\n\n# # Resample snow depths to daily means\n# ak_daily_mean = ds_ak.resample(time='D').mean()\n\n# # Plot daily mean snow depth over a month near Fairbanks, AK\n# ak_daily_mean['SNODP'][:,23,34].plot() ","type":"content","url":"/notebooks/merra2-data-access#using-only-2-days-for-now-to-get-the-notebook-to-run-need-to-update-tutorial-accordingly","position":3},{"hierarchy":{"lvl1":"SNOTEL Data Access"},"type":"lvl1","url":"/notebooks/snotel-data-access","position":0},{"hierarchy":{"lvl1":"SNOTEL Data Access"},"content":"This notebook allows for easy access to snow depths and SWE from the Snow Telemetry (SNOTEL) network. A simple example is used to show quick access to SNOTEL data over Creamer’s Field, AK using the metloom package.\n\nCredit: M3Works for the metloom package, which can be found here: \n\nhttps://​github​.com​/M3Works​/metloom​/tree​/main\n\nfrom datetime import datetime\nimport pandas as pd\nimport geopandas as gpd\nfrom pathlib import Path\n\nMetloom allows for easy access to several weather station types, including SNOTEL, MesoWest, and NorwayMet. The primary query function for each is SnotelPointData (replace “Snotel” with station of choice), which also allows us to view the locations of weather stations. We’ll start this example doing just that.\n\n# Import the SNOTEL pointdata classes\nfrom metloom.pointdata import SnotelPointData\n\n# Import the SNOTEL variable classes\nfrom metloom.variables import SnotelVariables\n\nWe are going to look for active SNOTEL stations near Creamer’s Field in Fairbanks, AK. The below cells search for SNOTEL stations within 0.5 degrees latitude/longitude of the provided polygon.\n\nWe will also go ahead and define the variables we want from the station, using SnotelPointData.ALLOWED_VARIABLES. Some of the allowed variables include:\n\nSNOWDEPTH: Snow depth, typically in inches.\n\nSWE: Snow water equivalent, typically in inches.\n\nPRECIPITATION: Accumulated precipitation, in inches.\n\nTMP: Air temperature, in degrees Fahrenheit.\n\n\"\"\" # Load FLCF lidar box from SnowEx campaigns\nsf_path = Path(\"/home/jovyan/shared-public/SnowPit/cffl_lidar_box.geojson\").expanduser()\nsf = gpd.read_file(str(sf_path))\nsf[\"name\"] = [\"FLCF\"]\n\n# Load the desired variables for SNOTEL query\nvariables = [SnotelPointData.ALLOWED_VARIABLES.SNOWDEPTH] \"\"\"\n\n\"\"\" # Find SNOTEL stations within polygon with desired variables\npoints = SnotelPointData.points_from_geometry(sf, variables)\n\n# Print nearby SNOTEL stations within 0.5 degrees of polygon\nprint(SnotelPointData.points_from_geometry(sf, variables, buffer=0.5).points) \"\"\"\n\nLooks like we have a SNOTEL station here! Note the printed output: Metloom returns the station ID number (1302), the state it’s in (AK), and the type of weather station (SNTL).\n\nLet’s see where it’s located in Creamer’s Field, relative to the polyon we provided.\n\n\"\"\" # Plot lidar box over ESRI tiles\nm = sf.explore(\n    tooltip=False, color=\"grey\", highlight=False, tiles=\"Esri.WorldImagery\",\n    style_kwds={\"opacity\": 0.2}, popup=[\"name\"]\n)\n# Add plot showing location of SNOTEL station(s)\ndf = points.to_dataframe()\ndf.explore(m=m, tooltip=[\"name\", \"id\"], color=\"red\", marker_kwds={\"radius\":4}) \"\"\"\n\nNow that we know which SNOTEL is in the area, we can query for the data.\n\n\"\"\" # Define SNOTEL station from FLCF\npt = SnotelPointData(\"1302:AK:SNTL\", \"Creamer's Field\") \"\"\"\n\nAs with other API requests, we can subset the data with a date range, given as datetime objects.\n\nNote here that we are requesting for snow depths on a daily basis. If desired, we could also obtain the hourly data instead, using pt.get_hourly_data().\n\n\"\"\" # Start and end date of SNOTEL query\nstart_date = datetime(2022, 3, 1)\nend_date = datetime(2023, 4, 1)\n\n# Query SNOTEL snow depths\ndf = pt.get_daily_data(start_date, end_date, variables) \"\"\"\n\n# df.head()\n\nEasy enough! We now have a data frame containing the basic information of the SNOTEL site, as well as the snow depth in inches.\n\nSince inches aren’t very useful in scientific analysis, and SNOWDEPTH can be a hassle to type out, let’s make a new column that shows the depth in meters.\n\n# Convert snow depth to meters\n# df['snow_depth_meters'] = df['SNOWDEPTH']*0.0254\n\nFrom there, it’s simple to plot the snow depth data as a time series.\n\n\"\"\" import matplotlib.pyplot as plt\n\n# Plot time series of daily SNOTEL data\nfig, ax = plt.subplots()\ndf.reset_index().set_index(\"datetime\")[\"snow_depth_meters\"].plot(ax=ax)\nax.set_xlabel(\"Date\", fontsize=12)\nax.set_ylabel(\"Snow depth [m]\", fontsize=12)\nax.set_title(\"SNOTEL: Creamer's Field (1302)\", fontsize=12)\nfig.tight_layout() \"\"\"","type":"content","url":"/notebooks/snotel-data-access","position":1},{"hierarchy":{"lvl1":"Field Campaigns Overview"},"type":"lvl1","url":"/notebooks/snowex-data-overview","position":0},{"hierarchy":{"lvl1":"Field Campaigns Overview"},"content":"\n\n","type":"content","url":"/notebooks/snowex-data-overview","position":1},{"hierarchy":{"lvl1":"Field Campaigns Overview"},"type":"lvl1","url":"/notebooks/snowex-data-overview#field-campaigns-overview","position":2},{"hierarchy":{"lvl1":"Field Campaigns Overview"},"content":"(5 minutes)\n\nBy: Megan Mason (NASA Goddard / SSAI) \n\nmegan​.a​.mason@nasa​.gov\n\nSupport by:  Carrie Vuyovich (NASA Goddard), Hans-Peter Marshall (Boise State), Svetlana Stuefer (University of Alaska Fairbanks)\n\nLearning Objectives\n\nVisual overview of the NASA SnowEx field campaigns\n\nGet a sense for the extent of data coverage\n\n\n\n","type":"content","url":"/notebooks/snowex-data-overview#field-campaigns-overview","position":3},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl2":"Data Coverage"},"type":"lvl2","url":"/notebooks/snowex-data-overview#data-coverage","position":4},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl2":"Data Coverage"},"content":"Each year we build upon our efforts to further investigate snow remote sensing science gaps identified in the NASA SnowEx Science Plan \n\n(Durand et al., 2016). The summary table lists the focus for each campaign by year and type. There are two different campaign types (IOP vs. TS); both result in the same types of measurements and data products. Depending on the  research application it may not matter at all which you choose to work with, or even combine! The important thing to grasp is the difference in spatial and temporal extent of the campaign periods. If the sampling protocols or data products change over time, it is for the sake of improvement. When possible, we aim to keep things consistent to continue to build a legacy data set.\n\nYear\n\nCampaign Type\n\nMeasurement Focus\n\n2017\n\nIOP\n\nColorado, focused on multiple instruments in a forest gradient.\n\n2020\n\nIOP, TS\n\nWestern U.S focused on Time Series of L-band InSAR, active/passive microwave for SWE and thermal IR for snow surface temp.\n\n2021\n\nTS\n\nWestern U.S, continued Time Series of L-band InSAR, also addressed prairie & snow albedo questions.\n\n2023\n\nIOP\n\nAlaska Tundra & Boreal forest, focused on addressing SWE/snow depth and albedo objectives.\n\n*IOP=Intense Observation Period (~2-3 week, daily observations) *; TS=Time Series (~3-5 month winter, weekly observations)\n\n","type":"content","url":"/notebooks/snowex-data-overview#data-coverage","position":5},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"Where has SnowEx Been?","lvl2":"Data Coverage"},"type":"lvl3","url":"/notebooks/snowex-data-overview#where-has-snowex-been","position":6},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"Where has SnowEx Been?","lvl2":"Data Coverage"},"content":"Campaign efforts are focused on various snow climates in the western United States. SnowEx partnerships and expertise are spread across the U.S and international.\n\n\nFigure 1. Map showing the locations of SnowEx field campaign areas (red dot). Base map shows snow classes defined in \n\nSturm and Liston, 2021. The snow pit images show a representative pit in each of the class types visited by SnowEx.\n\nTable 1. Number of manual depths and snow pits associated with NASA SnowEx measurement periods.\n\nSnowEx\n\nField Campaign Location\n\nTemporal Coverage\n\nManual Depths\n\nSnow Pits\n\nS17\n\nGrand Mesa & Senator Beck Basin, Colorado\n\nFebruary 6-25, 2017\n\n23,432\n\n265\n\nS20\n\nGrand Mesa, ColoradoWestern U.S Time Series (13 sites)\n\nNovember 4-7, 2019January 27-February 12, 2020October 24-May 20, 2020*\n\n16,21237,921TBD\n\n21154454\n\nS21\n\nWestern U.S Time Series (7 sites)\n\nNovember 16-May 27, 2021\n\n12,536\n\n247\n\nS23\n\nTundra & Boreal Forest, Alaska (pre-campaign site visit)Tundra & Boreal Forest, AlaskaTundra & Boreal Forest, AlaskaBoreal Forest, AlaskaTundra & Boreal Forest, Alaska\n\nMarch 7-17, 2022October 22-27, 2022March 7-16, 2023April 5-May 6, 2023October 17-28, 2023\n\n10,7289,04926,750TBD6,350\n\n1818617013127\n\n*The majority of sites in 2020 have a temporal coverage of January-March due to the Covid-19 pandemic.\n\n","type":"content","url":"/notebooks/snowex-data-overview#where-has-snowex-been","position":7},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"Snow Classification Coverage","lvl2":"Data Coverage"},"type":"lvl3","url":"/notebooks/snowex-data-overview#snow-classification-coverage","position":8},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"Snow Classification Coverage","lvl2":"Data Coverage"},"content":" Thanks to Sturm and Liston 2021 (and 1995), we have a global seasonal snow classification system. This is a vital mission planning tool for remote sensing snow studies. Revised from inception, the snow classification system offers improved utility of the climatological snow classes due to improved (much higher) resolution (300 m over North America). This data set can be found at NSIDC and downloaded at multiple resolutions.\n\n[NSIDC Global Seasonal-Snow Classification, Version 1](https://nsidc.org/data/NSIDC-0768/versions/1) \n\nCheck out [Sturm and Liston, 2021](https://journals.ametsoc.org/view/journals/hydr/22/11/JHM-D-21-0070.1.xml) to find out more \n    \n![](./content/01_snow-classes-sturm.png)\n**Figure 3.** Snow Classes across North America at 300 m (Sturm and Liston, 2021) \n\nAs part of the mission statement, SnowEx aims to quantify snow estimation uncertainty across a range of snow classes, terrain and vegetation types. This is important to determine what areas and time periods have high SWE uncertainty across the ensemble of instrument techniques.\n\n\nFigure 2. Map of the in situ field visits for the duration of SnowEx field campaigns (2017-2023). At this scale, points are overlapping, especially in the eastern Rocky Mountain region around Colorado. The total number of unique visits with recorded SWE are listed in the legend. Upper Right Bar chart of snow classes over the four SnowEx field campaign years. Counts represent in situ field visits. A range of sites in Boreal and Montane Forests occurred in open areas such as meadows and clearings. The snow classification colors match those used in \n\nSturm and Liston, 2021. ![](./content/01_snow-classes-barchart.png)\n**Figure 2.** Bar chart of snow classes over the four SnowEx field campagin years. Counts represent in situ field visits. A range of sites in Boreal and Montane Forests occured in open areas such as meadows and clearings. The snow classification colors match those used in [Sturm and Liston, 2021](https://journals.ametsoc.org/view/journals/hydr/22/11/JHM-D-21-0070.1.xml).   ![](./content/01_map-n-barchart.png)\n**Figure 2.** Bar chart of snow classes over the four SnowEx field campagin years. Counts represent in situ field visits. A range of sites in Boreal and Montane Forests occured in open areas such as meadows and clearings. The snow classification colors match those used in [Sturm and Liston, 2021](https://journals.ametsoc.org/view/journals/hydr/22/11/JHM-D-21-0070.1.xml). \n\n","type":"content","url":"/notebooks/snowex-data-overview#snow-classification-coverage","position":9},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"Recap","lvl2":"Data Coverage"},"type":"lvl3","url":"/notebooks/snowex-data-overview#recap","position":10},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"Recap","lvl2":"Data Coverage"},"content":"SnowEx campaigns are structured based on the objectives set out in the SnowEx Science Plan. Some of those objectives are meet by conducting an all hands-on, short and intense observation period (IOP), while others are addressed by studying the evolution of the snowpack over a much longer time series (TS) style campaign.\n\nThe coincident field and airborne campaigns are designed to directly respond to the current knowledge gaps in remote sensing of seasonal snow, thus the participant-driven SnowEx effort targets a range of snow classes, terrain and vegetation types.\n\n","type":"content","url":"/notebooks/snowex-data-overview#recap","position":11},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"References","lvl2":"Data Coverage"},"type":"lvl3","url":"/notebooks/snowex-data-overview#references","position":12},{"hierarchy":{"lvl1":"Field Campaigns Overview","lvl3":"References","lvl2":"Data Coverage"},"content":"SnowEx Experimental Plans: 2017, \n\n2020, \n\n2021, \n\n2023\n\nSnowEx Science Plan\n\nSturm and Liston, 2021","type":"content","url":"/notebooks/snowex-data-overview#references","position":13},{"hierarchy":{"lvl1":"Time-lapse Cameras"},"type":"lvl1","url":"/notebooks/timelapse-camera-tutorial","position":0},{"hierarchy":{"lvl1":"Time-lapse Cameras"},"content":"Learning Objectives\n\nAt the conclusion of this tutorial, you will...:\n\nKnow about all the time-lapse images available from the SnowEx 2017 and 2020 field campaigns\n\nView example time-lapse images from SnowEx 2020 and visualize their locations\n\nAccess snow depth measurements extracted from the SnowEx 2020 time-lapse images\n\nCompare snow depths from different SnowEx 2020 time-lapse cameras\n\n","type":"content","url":"/notebooks/timelapse-camera-tutorial","position":1},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"type":"lvl2","url":"/notebooks/timelapse-camera-tutorial#time-lapse-cameras-on-grand-mesa-during-snowex-field-campaigns","position":2},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"content":"Time-lapse cameras were installed in both the SnowEx 2017 and 2020 field campaigns on Grand Mesa in similar locations.\n\nSnowEx 2017 Time-lapse Cameras\n\n28 Total Time-lapse Cameras\n\nCapturing the entire winter season (September 2016-June 2017)\n\nTaking 4 photos/day at 8AM, 10AM, 12PM, 2PM, 4PM\n\nAn orange pole was installed in front of 15 cameras for snow depth measurements\n\nTime-lapse images have been submitted to the NSIDC by Mark Raleigh with all the required metadata (e.g., locations, naming convention, etc.) for use.\n\nSnowEx 2020 Time-lapse Cameras\n\n29 Total Time-lapse Cameras\n\nCapturing the entire winter season (September 2019-June 2020)\n\nTaking 3 photos/day at 11AM, 12PM, 1PM or 2 photos/day at 11AM and 12PM\n\nA red pole was installed in front of each camera for snow depth measurements.\n\nCameras were installed on the east and west side of the Grand Mesa, across a vegetation scale of 1-9, using the convention XMR:\n\nX = East (E) or West (W) areas of the Mesa\n\nM = number 1-9, representing 1 (least vegetation) to 9 (most vegetation). Within each vegetation class, there were three sub-classes of snow depths derived from 2017 SnowEx lidar measurements.\n\nR = Replicate of vegetation assignment, either A, B, C, D, or E.\n\n","type":"content","url":"/notebooks/timelapse-camera-tutorial#time-lapse-cameras-on-grand-mesa-during-snowex-field-campaigns","position":3},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl3":"An automated way of viewing and mapping time-lapse photos","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"type":"lvl3","url":"/notebooks/timelapse-camera-tutorial#an-automated-way-of-viewing-and-mapping-time-lapse-photos","position":4},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl3":"An automated way of viewing and mapping time-lapse photos","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"content":"\n\nFirst, we will procedurally import the necessary packages to access the data. To access the snow depths at each camera station, we will use the SnowEx database (snowexsql) to access the depths as PointMeasurements.\n\nfrom snowexsql.api import PointMeasurements\n\n# Import information for all point measurement types\nmeasurements = PointMeasurements()\n\n# List unique instruments\nresults = measurements.all_instruments\nprint('\\nAvailable Instruments = {}'.format(', '.join([str(r) for r in results])))\n\n# Packages for data analysis \nimport geopandas as gpd # geopandas library for data analysis and visualization\nimport pandas as pd # pandas as to read csv data and visualize tabular data\nimport numpy as np # numpy for data analysis \n\n# Packages for data visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt # matplotlib.pyplot for plotting images and graphs\n\nplt.rcParams['figure.figsize']  = (10, 4) # figure size\nplt.rcParams['axes.titlesize']  = 14 # title size \nplt.rcParams['axes.labelsize']  = 12 # axes label size \nplt.rcParams['xtick.labelsize'] = 11 # x tick label size \nplt.rcParams['ytick.labelsize'] = 11 # y tick label size \nplt.rcParams['legend.fontsize'] = 11 # legend size \nmpl.rcParams['figure.dpi'] = 100\n\n# Query the database for camera-based snow depths\ncamera_depths = measurements.from_filter(\n    type=\"depth\",\n    site_name=\"Grand Mesa\",\n    instrument=\"camera\",\n    limit = 13371\n)\n\ncamera_depths.head()\n\n","type":"content","url":"/notebooks/timelapse-camera-tutorial#an-automated-way-of-viewing-and-mapping-time-lapse-photos","position":5},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl4":"Plot the camera locations, using snow pit locations for reference.","lvl3":"An automated way of viewing and mapping time-lapse photos","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"type":"lvl4","url":"/notebooks/timelapse-camera-tutorial#plot-the-camera-locations-using-snow-pit-locations-for-reference","position":6},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl4":"Plot the camera locations, using snow pit locations for reference.","lvl3":"An automated way of viewing and mapping time-lapse photos","lvl2":"Time-lapse Cameras on Grand Mesa during SnowEx Field Campaigns"},"content":"\n\ncamera_depths.explore(tooltip=['equipment','date','latitude','longitude','value','type','units'])\n\n","type":"content","url":"/notebooks/timelapse-camera-tutorial#plot-the-camera-locations-using-snow-pit-locations-for-reference","position":7},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl2":"Viewing the time-lapse photos"},"type":"lvl2","url":"/notebooks/timelapse-camera-tutorial#viewing-the-time-lapse-photos","position":8},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl2":"Viewing the time-lapse photos"},"content":"Thanks to the SnowEx database, we were able to easily access snow depths at each site. However, if we wish to examine the camera imagery, we will need to be a bit more creative.\n\nThe images are available through NSIDC, so we will use earthaccess to grab one of the image archives.\n\nimport earthaccess\n\n# Authenticate with Earthdata Login servers\nauth = earthaccess.login(strategy=\"interactive\")\n\n# Search for camera imagery\nresults = earthaccess.search_data(\n    doi = \"10.5067/WYRNU50R9L5R\"\n)\n\n# Load the files into memory\nfiles = earthaccess.open(results)\n\nresults[0].data_links()\n\nLarge Downloads Ahead!\n\nLooking at the above data links, one will notice that the images are saved in .tar.gz format. We can read files through earthaccess in this format, but it will require some more work than simply downloading the data.\n\nUsers may download the files if they wish, but they are on the larger side (900+ Mb). If you wish to avoid large data downloads, then the below code will help with the process. However, be aware that the code can be rather memory intensive. If running this code on CryoCloud, then consider using larger memory allocations (4+ Gb).\n\nHere is how you would read in every file in the large tar.gz from earthaccess into memory:\n\nfile_content = files[0].read()\n\nSimplifying the download for learning purposes\n\nFor the sake of this tutorial we will create a synthetic tar.gz file with just three images we want to show here.\n\nfile_content = './data/sample-data.tar.gz'\n\n\nimport tarfile\nfrom io import BytesIO\nfrom datetime import datetime\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\njpg_files = []\n# Open the tarfile remotely\nwith tarfile.open(file_content, mode=\"r:gz\") as tar:\n    # Identify contents of tarfile\n    members = tar.getmembers()\n\n    # Loop through tarfile contents for images of interest\n    fig, ax = plt.subplots(1,3, figsize=(12,12))\n    ax.flatten()\n    for member in members:\n        if member.name.lower().endswith('.jpg'):\n            jpg_file = tar.extractfile(member).read()\n            \n            # Estimate datetime from image\n            creationTime = member.mtime\n            dt_c = datetime.fromtimestamp(creationTime)\n            formatted_datetime = dt_c.strftime(\"%m/%d/%Y %H:%M\")\n\n            desired_datetimes = ['09/27/2016 15:13',\n                                 '11/08/2016 14:00',\n                                 '12/10/2016 14:00']\n            \n            # Append files with desired datetime\n            for idx,dt in enumerate(desired_datetimes):\n                if formatted_datetime == dt:\n                    image = Image.open(BytesIO(jpg_file))\n                    ax[idx].imshow(image)\n                    ax[idx].set_title(desired_datetimes[idx])\n                    ax[idx].axis('off')\n\n    plt.tight_layout()\n\n","type":"content","url":"/notebooks/timelapse-camera-tutorial#viewing-the-time-lapse-photos","position":9},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl2":"Time-lapse Camera Applications"},"type":"lvl2","url":"/notebooks/timelapse-camera-tutorial#time-lapse-camera-applications","position":10},{"hierarchy":{"lvl1":"Time-lapse Cameras","lvl2":"Time-lapse Camera Applications"},"content":"Installing snow poles in front of time-lapse camera provides low-cost, long-term snow depth timeseries. Snow depths from the 2020 SnowEx time-lapse imagery have been manually processed with estimation of submission to the NSIDC database in summer 2021.\n\nThe snow depth is the difference between the number of pixels in a snow-free image and an image with snow, with a conversion from pixels to centimeters (Figure 1).\n\n\n\nFigure 1: Equation to extract snow depth from camera images. For each image, take the difference in pixels between the length of a snow-free stake and the length of the stake and multiply by length(cm)/pixel. The ratio can be found by dividing the full length of the stake (304.8 cm) by the length of a snow-free stake in pixels.\n\nSnow depth can be obtained in this manner manually, but it is now easier to determine the pixel size of the stakes through machine learning. For the sake of completeness, we will provide a brief example using the camera imagery above. Otherwise, users interested in using the camera imagery with machine learning are encouraged to check out the following resources by Katherine Breen and others:\n\nPublication on methodBreen C. M., W. R. Currier, C. Vuyovich, et al. 2024. “Snow Depth Extraction From Time‐Lapse Imagery Using a Keypoint Deep Learning Model.” Water Resources Research 60 (7): [10.1029/2023wr036682]\n\nGithub page for algorithm\n\nhttps://​github​.com​/catherine​-m​-breen​/snowpoles\n\nIn the example images above, we use the red pole in the fully snow-off and snow-on images for estimation.\n\nFor the snow-off image, the length of the red pole is 136 pixels. If we assume that the pole is 304.8 cm in length, then each pixel is approximately 2.24 cm in length.\n\nFor the snow-on image, the length of the red pole is 72 pixels, much shorter than the snow-off length. So, there is a ~64 pixel difference between the snow-on and snow-off lengths. Using the equation in Figure 1, we can calculate snow depth:\n\nDepth = 2.24 * (136-72) = 143.36 cm\n\nAcknowledgements: Anthony Arendt, Scott Henderson, Micah Johnson, Carrie Vuyovich, Ryan Currier, Megan Mason, Mark Raleigh\n\nAdditional ReferencesDickerson-Lange et al., 2017. Snow disappearance timing is dominated by forest effects on snow accumulation in warm winter climates of the Pacific Northwest, United States. Hydrological Processes. Vol 31, Issue 10. 13 February 2017. \n\nDickerson‐Lange et al. (2017)\n\nRaleigh et al., 2013. Approximating snow surface temperature from standard temperature and humidity data: New possibilities for snow model and remote sensing evaluation. Water Resources Research. Vol 49, Issue 12. 07 November 2013.  \n\nRaleigh et al. (2013)","type":"content","url":"/notebooks/timelapse-camera-tutorial#time-lapse-camera-applications","position":11},{"hierarchy":{"lvl1":"Terrestrial Laser Scanning"},"type":"lvl1","url":"/notebooks/tls-data-access","position":0},{"hierarchy":{"lvl1":"Terrestrial Laser Scanning"},"content":"This notebook is designed to take terrestrial laser scanner (TLS) data from the SnowEx Alaska Campaigns and derive snow depth. The TLS data is provided in both a raw point cloud format and a processed DEM format. For this example, we will be focusing on the TLS DEMs.\n\nThe TLS data is available through the cloud on NSIDC, so we will be using the earthaccess package. The first example will involve a single TLS image for simplicity, then we will have a second example that examines multiple TLS scans from the campaigns.\n\nimport earthaccess\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport rioxarray as rxr\nimport shutil\nimport tempfile\nimport xarray as xr\n\nThe TLS data was gathered in Bonanza Creek near Fairbanks, AK in two months: October 2022 and March 2023. These months correspond to the snow-off and snow-on seasons, respectively. We will start by getting some sample snow-on TLS data from a single day.\n\n# # Authenticate with Earthdata Login servers\n# auth = earthaccess.login(strategy=\"interactive\")\n\n# # Search for snow-on granules\n# results = earthaccess.search_data(\n#     #short_name=\"SNEX23_BCEF_TLS\",\n#     doi = \"10.5067/R466GRXNA61S\",\n#     temporal=('2023-03-15', '2023-03-15'),\n# )\n\nBecause the TLS data is available on-demand through the cloud, we do not need to download it. Instead, we can stream it directly with rioxarray!\n\n# # Load a single TLS scan\n# files = earthaccess.open(results)\n# snow_on = rxr.open_rasterio(files[1])\n\n# snow_on.rio.width\n\n# # Visualize the snow-on data\n# fig, ax = plt.subplots()\n# snow_on.plot(ax=ax, vmin=123, vmax=126,\n#              cbar_kwargs={'label': \"Elevation [m]\"})\n# ax.set_xlabel(\"Easting [m]\")\n# ax.set_ylabel(\"Northing [m]\")\n# ax.set_title(\" \")\n\nTwo things are noticeable from this TLS data:\n\nIt has a very high resolution (0.15 m).\n\nThe signal attenutates after ~60 m, so we have a small field of view.\n\nThis suggests that we will be able to obtain very fine-scale measurements of snow depth, but we will need scans from multiple locations to better characterize snow in Bonanza Creek.\n\nIn any case, let’s grab the snow-off data from the same location, and try to derive snow depth.\n\n# # Now search for snow-off granules\n# results = earthaccess.search_data(\n#     #short_name=\"SNEX23_BCEF_TLS\",\n#     doi = \"10.5067/R466GRXNA61S\",\n#     temporal=('2022-10-25', '2022-10-25'),\n# )\n\n# display(results)\n\n# # Again, load a single snow-off TLS scan\n# files = earthaccess.open(results)\n# snow_off = rxr.open_rasterio(files[1])\n\n# fig, ax = plt.subplots()\n# snow_off.plot(vmin=123, vmax=126,\n#               cbar_kwargs={'label': \"Elevation [m]\"})\n# ax.set_xlabel(\"Easting [m]\")\n# ax.set_ylabel(\"Northing [m]\")\n# ax.set_title(\" \")\n\nAlthough the snow-on/-off data look similar to each other, there are slight differences, meaning that we cannot perform a difference right away. We must first interpolate the data, ensuring that fill values are accounted for, then perform the difference.\n\n# # Interpolate snow-on data onto the x/y grid of snow-off data\n# snow_on_interp = snow_on.interp(\n#     x=snow_off.x,\n#     y=snow_off.y,\n#     kwargs={\"fill_value\": snow_on.attrs.get('_FillValue', np.nan)}\n# )\n\n# # Calculate the difference (snow depth)\n# difference = snow_on_interp - snow_off\n\n# # Define fill values in data\n# fill = snow_off.attrs.get('_FillValue', -9999.0)\n\n# # Include only data that is not equal to the fill value\n# difference = difference.where((snow_off != fill) & (snow_on_interp != fill))\n\n# # Plot snow depth over the TLS scene\n# fig, ax = plt.subplots()\n# difference.plot(vmin=0, vmax=1.5,\n#                 cbar_kwargs={'label': \"Snow depth [m]\"})\n# ax.set_xlabel(\"Easting [m]\")\n# ax.set_ylabel(\"Northing [m]\")\n# ax.set_title(\" \")\n\nAlthough not perfect, this provides a very reasonable snow depth DEM for the TLS data gathered in this location. If we want, we can perform basic statistics on the derived snow depths.\n\n# # Calculate median snow depth over the scene\n# median_depth = difference.where(difference>=0).median()\n\n# # Make histogram plot of snow depth\n# fig, ax = plt.subplots()\n# difference.where(difference>=0).plot.hist(ax=ax, bins=50)\n# ax.axvline(x=median_depth, color='black', linewidth=2, linestyle='--') # Median depth line\n# ax.set_xlim([0, 2.5])\n# ax.set_ylabel(\"Counts\")\n# ax.set_xlabel(\"Snow depth [m]\")\n# ax.set_title(' ')\n# ax.text(1, 8000, f'Median depth = {median_depth:.2f} m', fontsize=12)\n\n","type":"content","url":"/notebooks/tls-data-access","position":1},{"hierarchy":{"lvl1":"Terrestrial Laser Scanning","lvl2":"Multiple Scans Example"},"type":"lvl2","url":"/notebooks/tls-data-access#multiple-scans-example","position":2},{"hierarchy":{"lvl1":"Terrestrial Laser Scanning","lvl2":"Multiple Scans Example"},"content":"Because we can stream the TLS data through the cloud, this example is very similar to the above code. The main exception is that we will generate a list of DataArrays, from which we derive snow depth for three TLS scanning locations.\n\n# # Search for snow-on granules\n# snow_on_results = earthaccess.search_data(\n#     #short_name=\"SNEX23_BCEF_TLS\",\n#     doi = \"10.5067/R466GRXNA61S\",\n#     temporal=('2023-03-01', '2023-03-31'),\n# )\n\n# snow_off_results = earthaccess.search_data(\n#     #short_name=\"SNEX23_BCEF_TLS\",\n#     doi = \"10.5067/R466GRXNA61S\",\n#     temporal=('2022-10-01', '2022-10-31'),\n# )\n\n# # Create list of snow-on DataArrays\n# snow_on_files = earthaccess.open(snow_on_results)\n# snow_on_rasters = [rxr.open_rasterio(f) for f in snow_on_files]\n\n# # Create list of snow-off DataArrays\n# snow_off_files = earthaccess.open(snow_off_results)\n# snow_off_rasters = [rxr.open_rasterio(f) for f in snow_off_files]\n\nTo make the final plot of this example cleaner, we will assign each TLS scan a label based on the site ID at Bonanza Creek.\n\n# snon_site_ids = []\n# snoff_site_ids = []\n# # Get site IDs for each snow-on DataArray\n# for f in snow_on_files:\n#     # Get path from file name\n#     path = f.path\n#     # Use regex to extract the site ID from file path, given pattern _SW_YYYYMMDD_SITEID_V\n#     m = re.search(r'_(SW|N)_\\d{8}_(.*?)_V', path)\n#     if m:\n#         snon_site_ids.append(m.group(2))\n#     else:\n#         snon_site_ids.append(\"unknown\")\n\n# # Get site IDs for each snow-off DataArray\n# for f in snow_off_files:\n#     # Step 1: Extract path\n#     path = f.path\n#     # Step 2: Use regex to extract the site ID\n#     # Pattern: _SW_YYYYMMDD_SITEID_V\n#     m = re.search(r'_(SW|N|NE)_\\d{8}_(.*?)_V', path)\n#     if m:\n#         snoff_site_ids.append(m.group(2))\n#     else:\n#         snoff_site_ids.append(\"unknown\")\n\n# print(snon_site_ids)\n# print(snoff_site_ids)\n\n# # Add site ID to attributes of DataArrays\n# for r, site in zip(snow_on_rasters, snon_site_ids):\n#     r.attrs['site_id'] = site\n\n# for r, site in zip(snow_off_rasters, snoff_site_ids):\n#     r.attrs['site_id'] = site\n\n# # Create dictionaries linking each DataArray to a site ID\n# snow_on_dict = {r.attrs['site_id']: r for r in snow_on_rasters}\n# snow_off_dict = {r.attrs['site_id']: r for r in snow_off_rasters}\n\nNow each TLS scan is linked to a site ID. However, we can see that the snow-on data has many more scans than the snow-off data. Because snow depth data is our priority, we will only consider snow-on scans that share a site ID with the snow-off data.\n\n# # Determine site IDs with recorded data for both snow-off and snow-on season\n# common_site_ids = sorted(set(snow_on_dict).intersection(snow_off_dict))\n# print(\"Common site IDs:\", common_site_ids)\n\n# # Create lists of DataArrays for the common sites only\n# snow_on_paired = [snow_on_dict[sid] for sid in common_site_ids]\n# snow_off_paired = [snow_off_dict[sid] for sid in common_site_ids]\n\nNow that the site IDs are matched, deriving snow depth is the same as the first example, only with looping to make the calculation (and plotting) easier.\n\n# snow_depths = []\n# # Interpolate DataArrays and derive snow depth, as before\n# for so, soff, site in zip(snow_on_paired, snow_off_paired, common_site_ids):\n#     # Interpolate snow-on data onto the x/y grid of snow-off data\n#     tmp_interp = so.interp(\n#         x=soff.x,\n#         y=soff.y,\n#     )\n\n#     tmp_diff = tmp_interp - soff\n#     tmp_diff.attrs['site_id'] = site\n\n#     tmp_diff = tmp_diff.where((tmp_diff[0]>0)&(tmp_diff[0]<=2))\n#     snow_depths.append(tmp_diff)\n\n# # Plot the derived snow depths in a 3x3 figure\n# fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n# axes = axes.flatten()\n\n# for idx, data_array in enumerate(snow_depths):\n#     data_array.plot(ax=axes[idx], vmin=0, vmax=2)\n#     axes[idx].set_title(f\"{snow_depths[idx].attrs['site_id']}\")\n\n# plt.tight_layout()\n# plt.show()\n\nThat’s all there is to it! Some of the coverage is a bit sparse, and the depths over site DEC look rather high, but we otherwise have reasonable snow depths over 9 sites in Bonanza Creek. These could then be compared to other ground based efforts or airborne data to cross-calibrate observation methods.","type":"content","url":"/notebooks/tls-data-access#multiple-scans-example","position":3},{"hierarchy":{"lvl1":"UAVSAR"},"type":"lvl1","url":"/notebooks/uavsar-accessing-imagery-pt1","position":0},{"hierarchy":{"lvl1":"UAVSAR"},"content":"\n\nDevelopers: Jack Tarricone, University of Nevada, Reno Zach Keskinen, Boise State University\n\nOther contributors: Ross Palomaki, Montana State UniversityNaheem Adebisi, Boise State University\n\n","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1","position":1},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"What is UAVSAR?"},"type":"lvl2","url":"/notebooks/uavsar-accessing-imagery-pt1#what-is-uavsar","position":2},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"What is UAVSAR?"},"content":"UAVSAR is a low frequency plane-based synthetic aperture radar. UAVSAR stands for “Uninhabited Aerial Vehicle Synthetic Aperture Radar”. It captures imagery using a L-band radar. This low frequency means it can penetrate into and through clouds, vegetation, and snow.\n\nfrequency (cm)\n\nresolution (rng x azi m)\n\nSwath Width (km)\n\nPolarizations\n\nLaunch date\n\nL-band 23\n\n1.8 x 5.5\n\n16\n\nVV, VH, HV, HH\n\n2007","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#what-is-uavsar","position":3},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"NASA SnowEx 2020 and 2021 UAVSAR Campaigns","lvl2":"What is UAVSAR?"},"type":"lvl3","url":"/notebooks/uavsar-accessing-imagery-pt1#nasa-snowex-2020-and-2021-uavsar-campaigns","position":4},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"NASA SnowEx 2020 and 2021 UAVSAR Campaigns","lvl2":"What is UAVSAR?"},"content":"During the winter of 2020 and 2021, NASA conducted an L-band InSAR timeseries across the Western US with the goal of tracking changes in SWE. Field teams in 13 different locations in 2020, and in 6 locations in 2021, deployed on the date of the flight to perform calibration and validation observations.\n\nThe site locations from the above map along with the \n\nUAVSAR defined campaign name and currently processed pairs of InSAR images for each site. Note that the image pair count may contain multiple versions of the same image and may increase as more pairs of images are processed by JPL. Also note that the Lowman campaign name is the wrong state when searching.\n\nSite Location\n\nCampaign Name\n\nImage Pairs\n\nGrand Mesa\n\nGrand Mesa, CO\n\n13\n\nBoise River Basin\n\nLowman, CO\n\n17\n\nFrazier Experimental Forest\n\nFraser, CO\n\n16\n\nSenator Beck Basin\n\nIronton, CO\n\n9\n\nEast River\n\nPeeler Peak, CO\n\n4\n\nCameron Pass\n\nRocky Mountains NP, CO\n\n15\n\nReynold Creek\n\nSilver City, ID\n\n1\n\nCentral Agricultral Research Center\n\nUtica, MT\n\n2\n\nLittle Cottonwoody Canyon\n\nSalt Lake City, UT\n\n21\n\nJemez River\n\nLos Alamos, NM\n\n3\n\nAmerican River Basin\n\nEldorado National Forest, CA\n\n4\n\nSagehen Creek\n\nDonner Memorial State Park, CA\n\n4\n\nLakes Basin\n\nSierra National Forest, CA\n\n3\n\n","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#nasa-snowex-2020-and-2021-uavsar-campaigns","position":5},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"Why would I use UAVSAR?"},"type":"lvl2","url":"/notebooks/uavsar-accessing-imagery-pt1#why-would-i-use-uavsar","position":6},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"Why would I use UAVSAR?"},"content":"UAVSAR works with low frequency radar waves. These low frequencies (< 3 GHz) can penetrate clouds and maintain coherence (a measure of radar image quality) over long periods. For these reasons, time series was captured over 13 sites as part of the winter of 2019-2020 and 2020-2021 for snow applications. Additionally the UAVSAR is awesome!\n\n","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#why-would-i-use-uavsar","position":7},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"Accessing UAVSAR Images"},"type":"lvl2","url":"/notebooks/uavsar-accessing-imagery-pt1#accessing-uavsar-images","position":8},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"Accessing UAVSAR Images"},"content":"UAVSAR imagery can be downloaded from both the \n\nJPL and \n\nAlaska Satellite Facility. However both provide the imagery in a binary format that is not readily usable or readable by GIS software or python libraries.","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#accessing-uavsar-images","position":9},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"Data Download and Conversion with uavsar_pytools"},"type":"lvl2","url":"/notebooks/uavsar-accessing-imagery-pt1#data-download-and-conversion-with-uavsar-pytools","position":10},{"hierarchy":{"lvl1":"UAVSAR","lvl2":"Data Download and Conversion with uavsar_pytools"},"content":"uavsar_pytools (\n\nGithub) is a Python package developed out of work started at SnowEx Hackweek 2021. It nativiely downloads, formats, and converts this data in analysis ready rasters projected in WSG-84 Lat/Lon (\n\nEPSG:4326. The data traditionally comes in a binary format, which is not injestible by traditional geospatial analysis software (Python, R, QGIS, ArcGIS). It can download and convert either individual images - UavsarScene or entire collections of images - UavsarCollection.","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#data-download-and-conversion-with-uavsar-pytools","position":11},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"Netrc Authorization","lvl2":"Data Download and Conversion with uavsar_pytools"},"type":"lvl3","url":"/notebooks/uavsar-accessing-imagery-pt1#netrc-authorization","position":12},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"Netrc Authorization","lvl2":"Data Download and Conversion with uavsar_pytools"},"content":"In order to download uavsar images you will need a \n\nnetrc file that contains your earthdata username and password. If you need to register for a NASA earthdata account use this \n\nlink. A netrc file is a hidden file, it won’t appear in the your file explorer, that is in your home directory and that programs can access to get the appropriate usernames and passwords. While you’ll have already done this for the Hackweek virtual machines, uavsar_pytools has a tool to create this netrc file on a local computer. You only need to create this file once and then it should be permanently stored on your computer.\n\n# ## Creating .netrc file with Earthdata login information\n# from uavsar_pytools.uavsar_tools import create_netrc\n\n# # This will prompt you for your username and password and save this\n# # information into a .netrc file in your home directory. You only need to run\n# # this command once per computer. Then it will be saved.\n# create_netrc()\n\n","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#netrc-authorization","position":13},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"Downloading and converting a single UAVSAR interferogram scene","lvl2":"Data Download and Conversion with uavsar_pytools"},"type":"lvl3","url":"/notebooks/uavsar-accessing-imagery-pt1#downloading-and-converting-a-single-uavsar-interferogram-scene","position":14},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"Downloading and converting a single UAVSAR interferogram scene","lvl2":"Data Download and Conversion with uavsar_pytools"},"content":"You can find urls for UAVSAR images at the \n\nASF vertex website. Make sure to change the platform to UAVSAR and you may also want to filter to ground projected interferograms.\n\n# try:\n#     from uavsar_pytools import UavsarScene\n# except ModuleNotFoundError:\n#     print('Install uavsar_pytools with `pip install uavsar_pytools`')\n\n# ## This is the directory you want to download and convert the images in.\n# work_dir = '/tmp/uavsar_data'\n\n# ## This is a url you want to download. Can be obtained from vertex\n# url = 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/\\\n# lowman_23205_21009-004_21012-000_0007d_s01_L090_01_int_grd.zip'\n\n# ## clean = True will delete the binary and zip files leaving only the tiffs\n# scene = UavsarScene(url = url, work_dir=work_dir, clean= True)\n\n# ## After running url_to_tiffs() you will download the zip file, unzip the binary \n# ## files, and convert them to geotiffs in the directory with the scene name in\n# ## the work directory. It also generate a .csv pandas dictionary of metadata.\n# # scene.url_to_tiffs()\n\n","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#downloading-and-converting-a-single-uavsar-interferogram-scene","position":15},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"Downloading and converting a full UAVSAR collection","lvl2":"Data Download and Conversion with uavsar_pytools"},"type":"lvl3","url":"/notebooks/uavsar-accessing-imagery-pt1#downloading-and-converting-a-full-uavsar-collection","position":16},{"hierarchy":{"lvl1":"UAVSAR","lvl3":"Downloading and converting a full UAVSAR collection","lvl2":"Data Download and Conversion with uavsar_pytools"},"content":"If you want to download and convert an entire Uavsar collection for a larger analysis you can use UavsarCollection. The collection names for the SnowEx campaign are listed in the table in the introduction. The UavsarCollection can download either InSAR pairs and PolSAR images.\n\n# from uavsar_pytools import UavsarCollection\n# ## Collection name, the SnowEx Collection names are listed above. These are case \n# ## and space sensitive.\n# collection_name = 'Grand Mesa, CO'\n\n# ## Directory to save collection into. This will be filled with directory with \n# ## scene names and tiffs inside of them.\n# out_dir = '/tmp/collection_ex/'\n\n# ## This is optional, but you will generally want to at least limit the date\n# ## range between 2019 and today.\n# date_range = ('2019-11-01', 'today')\n\n# # Keywords: to download incidence angles with each image use `inc = True`\n# # For only certain pols use `pols = ['VV','HV']`\n\n# collection = UavsarCollection(collection = collection_name, work_dir = out_dir, dates = date_range)\n\n# ## You can use this to check how many image pairs have at least one image in\n# ## the date range.\n\n# #collection.find_urls()\n\n# ## When you are ready to download all the images run:\n\n# # collection.collection_to_tiffs()\n\n# ## This will take a long time and a lot of space, ~1-5 gB and 10 minutes per \n# ## image pair depending on which scene, so run it if you have the space and time.","type":"content","url":"/notebooks/uavsar-accessing-imagery-pt1#downloading-and-converting-a-full-uavsar-collection","position":17}]}